#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¿®å¤ç‰ˆé«˜å‡†ç¡®ç‡CE-CSLæ‰‹è¯­è¯†åˆ«è®­ç»ƒå™¨ - ä¿®å¤è¯æ±‡è¡¨é—®é¢˜
"""

import os
import sys
import json
import time
import logging
from pathlib import Path
from dataclasses import dataclass
from typing import List, Dict, Tuple, Optional
import random
import numpy as np

import mindspore as ms
from mindspore import nn, ops, context, Model, load_checkpoint, save_checkpoint
from mindspore.dataset import GeneratorDataset
from mindspore.train.callback import Callback
from mindspore.communication.management import init, get_rank, get_group_size

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class FixedAccuracyConfig:
    """ä¿®å¤ç‰ˆé«˜å‡†ç¡®ç‡è®­ç»ƒé…ç½®"""
    # æ•°æ®é…ç½®
    data_dir: str = "data/CE-CSL"
    vocab_file: str = "backend/models/vocab.json"
    
    # æ¨¡å‹é…ç½® - é’ˆå¯¹å°æ•°æ®é›†ä¼˜åŒ–
    input_size: int = 150528  # 224*224*3
    hidden_size: int = 64  # è¿›ä¸€æ­¥å‡å°
    num_layers: int = 1
    num_classes: int = 10
    dropout_rate: float = 0.3  # é€‚ä¸­çš„dropout
    
    # è®­ç»ƒé…ç½® - æ›´ä¿å®ˆ
    batch_size: int = 1  # æå°æ‰¹æ¬¡
    learning_rate: float = 0.0005  # æ›´å°çš„å­¦ä¹ ç‡
    epochs: int = 100
    weight_decay: float = 0.001
    
    # æ•°æ®å¢å¼º
    augment_factor: int = 10  # é€‚ä¸­çš„å¢å¼º
    
    # æ—©åœå’Œä¿å­˜
    patience: int = 50
    min_epochs: int = 20
    
    # è®¾å¤‡é…ç½®
    device_target: str = "CPU"

class FixedDataAugmentor:
    """ä¿®å¤ç‰ˆæ•°æ®å¢å¼ºå™¨"""
    
    def __init__(self, config: FixedAccuracyConfig):
        self.config = config
        
    def augment_sample(self, frames: np.ndarray, label: int) -> List[Tuple[np.ndarray, int]]:
        """å¯¹å•ä¸ªæ ·æœ¬è¿›è¡Œæ•°æ®å¢å¼º"""
        augmented_samples = [(frames, label)]  # åŸå§‹æ ·æœ¬
        
        for i in range(self.config.augment_factor - 1):
            # ç®€å•çš„å¢å¼ºæ–¹æ³•
            aug_type = random.choice(['noise', 'brightness', 'flip'])
            
            if aug_type == 'noise':
                aug_frames = self._add_noise(frames, noise_factor=0.05)
            elif aug_type == 'brightness':
                aug_frames = self._adjust_brightness(frames, factor=random.uniform(0.8, 1.2))
            else:  # flip
                aug_frames = self._horizontal_flip(frames)
            
            augmented_samples.append((aug_frames, label))
        
        return augmented_samples
    
    def _add_noise(self, frames: np.ndarray, noise_factor: float = 0.05) -> np.ndarray:
        """æ·»åŠ è½»å¾®å™ªå£°"""
        noise = np.random.normal(0, noise_factor * 255, frames.shape).astype(np.float32)
        noisy_frames = frames.astype(np.float32) + noise
        return np.clip(noisy_frames, 0, 255).astype(np.uint8)
    
    def _adjust_brightness(self, frames: np.ndarray, factor: float = 1.1) -> np.ndarray:
        """è°ƒæ•´äº®åº¦"""
        bright_frames = frames.astype(np.float32) * factor
        return np.clip(bright_frames, 0, 255).astype(np.uint8)
    
    def _horizontal_flip(self, frames: np.ndarray) -> np.ndarray:
        """æ°´å¹³ç¿»è½¬"""
        return np.flip(frames, axis=2)

class FixedModel(nn.Cell):
    """ä¿®å¤ç‰ˆç®€åŒ–æ¨¡å‹"""
    
    def __init__(self, config: FixedAccuracyConfig):
        super().__init__()
        self.config = config
        
        # å¤§å¹…ç®€åŒ–çš„ç‰¹å¾æå–
        self.feature_reducer = nn.SequentialCell([
            nn.Dense(config.input_size, 256),
            nn.ReLU(),
            nn.Dropout(p=config.dropout_rate),
            nn.Dense(256, 64),
            nn.ReLU(),
            nn.Dropout(p=config.dropout_rate)
        ])
        
        # ç®€åŒ–çš„LSTM
        self.lstm = nn.LSTM(
            input_size=64,
            hidden_size=config.hidden_size,
            num_layers=config.num_layers,
            batch_first=True,
            dropout=0.0
        )
        
        # ç®€åŒ–çš„åˆ†ç±»å™¨
        self.classifier = nn.SequentialCell([
            nn.Dense(config.hidden_size, config.num_classes)
        ])
        
        # åˆå§‹åŒ–æƒé‡
        self._init_weights()
    
    def _init_weights(self):
        """åˆå§‹åŒ–æƒé‡"""
        for _, cell in self.cells_and_names():
            if isinstance(cell, nn.Dense):
                cell.weight.set_data(ms.common.initializer.initializer(
                    ms.common.initializer.XavierUniform(), cell.weight.shape, cell.weight.dtype))
                if cell.bias is not None:
                    cell.bias.set_data(ms.common.initializer.initializer(
                        'zeros', cell.bias.shape, cell.bias.dtype))
    
    def construct(self, x):
        # x shape: (batch, seq_len, height, width, channels)
        batch_size, seq_len = x.shape[0], x.shape[1]
        
        # å±•å¹³ç©ºé—´ç»´åº¦
        x = x.view(batch_size * seq_len, -1)
        
        # ç‰¹å¾é™ç»´
        x = self.feature_reducer(x)
        
        # é‡å¡‘ä¸ºåºåˆ—
        x = x.view(batch_size, seq_len, -1)
        
        # LSTMå¤„ç†
        lstm_out, _ = self.lstm(x)
        
        # ä½¿ç”¨æœ€åä¸€ä¸ªæ—¶é—´æ­¥
        last_output = lstm_out[:, -1, :]
        
        # åˆ†ç±»
        logits = self.classifier(last_output)
        
        return logits

class FixedDataset:
    """ä¿®å¤ç‰ˆæ•°æ®é›†"""
    
    def __init__(self, data_dir: str, split: str, config: FixedAccuracyConfig, 
                 vocab: Dict[str, int], augmentor: Optional[FixedDataAugmentor] = None):
        self.data_dir = Path(data_dir)
        self.split = split
        self.config = config
        self.vocab = vocab
        self.augmentor = augmentor
        
        # åŠ è½½æ•°æ®
        self.samples = self._load_samples()
        logger.info(f"åŠ è½½ {split} æ•°æ®é›†: {len(self.samples)} ä¸ªæ ·æœ¬")
    
    def _load_samples(self) -> List[Tuple[np.ndarray, int]]:
        """åŠ è½½æ•°æ®æ ·æœ¬"""
        samples = []
        
        # åŠ è½½å…ƒæ•°æ®
        metadata_file = self.data_dir / "processed" / self.split / f"{self.split}_metadata.json"
        if not metadata_file.exists():
            logger.error(f"å…ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {metadata_file}")
            return samples
        
        with open(metadata_file, 'r', encoding='utf-8') as f:
            metadata = json.load(f)
        
        for item in metadata:
            try:
                # åŠ è½½å¸§æ•°æ®
                frames_path = self.data_dir / "processed" / self.split / f"{item['video_id']}_frames.npy"
                if not frames_path.exists():
                    logger.warning(f"å¸§æ–‡ä»¶ä¸å­˜åœ¨: {frames_path}")
                    continue
                
                frames = np.load(frames_path)
                
                # è·å–æ ‡ç­¾ - å…³é”®ä¿®å¤
                gloss = item['gloss_sequence'][0]
                if gloss not in self.vocab:
                    logger.warning(f"è¯æ±‡ '{gloss}' ä¸åœ¨è¯æ±‡è¡¨ä¸­ï¼Œè·³è¿‡")
                    continue
                
                label = self.vocab[gloss]
                
                # æ•°æ®é¢„å¤„ç†
                frames = self._preprocess_frames(frames)
                
                # å¦‚æœæ˜¯è®­ç»ƒé›†ä¸”æœ‰å¢å¼ºå™¨ï¼Œè¿›è¡Œæ•°æ®å¢å¼º
                if self.split == "train" and self.augmentor:
                    augmented = self.augmentor.augment_sample(frames, label)
                    samples.extend(augmented)
                else:
                    samples.append((frames, label))
                
            except Exception as e:
                logger.error(f"åŠ è½½æ ·æœ¬å¤±è´¥ {item.get('video_id', 'unknown')}: {e}")
        
        return samples
    
    def _preprocess_frames(self, frames: np.ndarray) -> np.ndarray:
        """é¢„å¤„ç†å¸§æ•°æ®"""
        # å½’ä¸€åŒ–åˆ°[0,1]
        frames = frames.astype(np.float32) / 255.0
        
        # å›ºå®šåºåˆ—é•¿åº¦ä¸º50
        target_len = 50
        seq_len = frames.shape[0]
        
        if seq_len > target_len:
            # å‡åŒ€é‡‡æ ·
            indices = np.linspace(0, seq_len - 1, target_len).astype(int)
            frames = frames[indices]
        elif seq_len < target_len:
            # é‡å¤æœ€åä¸€å¸§è¿›è¡Œå¡«å……
            padding = np.repeat(frames[-1:], target_len - seq_len, axis=0)
            frames = np.concatenate([frames, padding], axis=0)
        
        return frames
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        frames, label = self.samples[idx]
        return frames, label

class FixedTrainer:
    """ä¿®å¤ç‰ˆè®­ç»ƒå™¨"""
    
    def __init__(self, config: FixedAccuracyConfig):
        self.config = config
        self.setup_environment()
        self.build_vocab()  # å…ˆæ„å»ºè¯æ±‡è¡¨
        self.setup_data()
        self.setup_model()
        
        # è®­ç»ƒå†å²
        self.train_history = {
            'train_loss': [],
            'train_acc': [],
            'val_loss': [],
            'val_acc': [],
            'epoch_times': []
        }
        
        self.best_val_acc = 0.0
        self.patience_counter = 0
    
    def setup_environment(self):
        """è®¾ç½®ç¯å¢ƒ"""
        context.set_context(mode=context.GRAPH_MODE, device_target=self.config.device_target)
        logger.info(f"ä¿®å¤ç‰ˆCE-CSLè®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ - è®¾å¤‡: {self.config.device_target}")
    
    def build_vocab(self):
        """æ„å»ºè¯æ±‡è¡¨ - å…³é”®ä¿®å¤"""
        # å…ˆæ„å»ºåŸºç¡€è¯æ±‡è¡¨
        vocab = {'<PAD>': 0, '<UNK>': 1}
        
        # æ‰«æè®­ç»ƒæ•°æ®æ„å»ºè¯æ±‡è¡¨
        train_metadata_file = Path(self.config.data_dir) / "processed" / "train" / "train_metadata.json"
        if train_metadata_file.exists():
            with open(train_metadata_file, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
            
            for item in metadata:
                for gloss in item.get('gloss_sequence', []):
                    if gloss not in vocab:
                        vocab[gloss] = len(vocab)
        
        # æ‰«æå¼€å‘æ•°æ®
        dev_metadata_file = Path(self.config.data_dir) / "processed" / "dev" / "dev_metadata.json"
        if dev_metadata_file.exists():
            with open(dev_metadata_file, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
            
            for item in metadata:
                for gloss in item.get('gloss_sequence', []):
                    if gloss not in vocab:
                        vocab[gloss] = len(vocab)
        
        self.vocab = vocab
        self.config.num_classes = len(vocab)
        
        logger.info(f"è¯æ±‡è¡¨æ„å»ºå®Œæˆ: {list(self.vocab.keys())}")
        logger.info(f"è¯æ±‡è¡¨å¤§å°: {self.config.num_classes}")
    
    def setup_data(self):
        """è®¾ç½®æ•°æ®"""
        logger.info("ğŸ“Š åŠ è½½æ•°æ®ï¼ˆåŒ…å«æ•°æ®å¢å¼ºï¼‰...")
        
        # åˆ›å»ºæ•°æ®å¢å¼ºå™¨
        augmentor = FixedDataAugmentor(self.config)
        
        # åˆ›å»ºæ•°æ®é›†
        self.train_dataset = FixedDataset(
            self.config.data_dir, "train", self.config, self.vocab, augmentor
        )
        self.val_dataset = FixedDataset(
            self.config.data_dir, "dev", self.config, self.vocab
        )
        
        logger.info(f"è®­ç»ƒé›†: {len(self.train_dataset)} æ ·æœ¬ï¼ˆåŒ…å«å¢å¼ºæ•°æ®ï¼‰")
        logger.info(f"éªŒè¯é›†: {len(self.val_dataset)} æ ·æœ¬")
        
        if len(self.train_dataset) == 0:
            logger.error("è®­ç»ƒæ•°æ®é›†ä¸ºç©ºï¼")
            raise ValueError("è®­ç»ƒæ•°æ®é›†ä¸ºç©º")
    
    def setup_model(self):
        """è®¾ç½®æ¨¡å‹"""
        logger.info("ğŸ§  æ„å»ºä¿®å¤ç‰ˆæ¨¡å‹...")
        
        self.model = FixedModel(self.config)
        
        # è®¡ç®—å‚æ•°é‡
        total_params = sum(p.size for p in self.model.trainable_params())
        logger.info(f"æ¨¡å‹æ„å»ºå®Œæˆ - å‚æ•°é‡: {total_params}")
        
        # è®¾ç½®ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
        self.optimizer = nn.Adam(
            self.model.trainable_params(),
            learning_rate=self.config.learning_rate,
            weight_decay=self.config.weight_decay
        )
        
        self.loss_fn = nn.CrossEntropyLoss()
        logger.info("ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°åˆ›å»ºå®Œæˆ")
    
    def create_dataloader(self, dataset, shuffle=True):
        """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
        def generator():
            indices = list(range(len(dataset)))
            if shuffle:
                random.shuffle(indices)
            
            for idx in indices:
                frames, label = dataset[idx]
                # è¿”å›å•ä¸ªæ ·æœ¬
                yield (frames[np.newaxis, :], np.array([label]))
        
        return generator
    
    def train_epoch(self):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.set_train()
        
        epoch_loss = 0.0
        epoch_correct = 0
        epoch_total = 0
        batch_count = 0
        
        dataloader = self.create_dataloader(self.train_dataset, shuffle=True)
        
        for frames_batch, labels_batch in dataloader():
            batch_count += 1
            
            # è½¬æ¢ä¸ºTensor
            frames_tensor = ms.Tensor(frames_batch, ms.float32)
            labels_tensor = ms.Tensor(labels_batch, ms.int32)
            
            # å‰å‘ä¼ æ’­
            def forward_fn():
                logits = self.model(frames_tensor)
                loss = self.loss_fn(logits, labels_tensor)
                return loss, logits
            
            grad_fn = ms.ops.value_and_grad(forward_fn, None, self.optimizer.parameters, has_aux=True)
            (loss, logits), grads = grad_fn()
            
            # åå‘ä¼ æ’­
            self.optimizer(grads)
            
            # ç»Ÿè®¡
            predictions = ops.ArgMaxWithValue(axis=1)(logits)[0]
            correct = ops.equal(predictions, labels_tensor).sum()
            
            epoch_loss += loss.asnumpy()
            epoch_correct += correct.asnumpy()
            epoch_total += len(labels_batch)
            
            if batch_count % 20 == 0:
                logger.info(f"Batch {batch_count}: Loss = {loss.asnumpy():.4f}")
        
        avg_loss = epoch_loss / batch_count if batch_count > 0 else 0
        accuracy = epoch_correct / epoch_total if epoch_total > 0 else 0
        
        return avg_loss, accuracy
    
    def evaluate(self):
        """è¯„ä¼°æ¨¡å‹"""
        self.model.set_train(False)
        
        total_loss = 0.0
        total_correct = 0
        total_samples = 0
        batch_count = 0
        
        # ç±»åˆ«ç»Ÿè®¡
        class_correct = {i: 0 for i in range(self.config.num_classes)}
        class_total = {i: 0 for i in range(self.config.num_classes)}
        
        dataloader = self.create_dataloader(self.val_dataset, shuffle=False)
        
        for frames_batch, labels_batch in dataloader():
            batch_count += 1
            
            frames_tensor = ms.Tensor(frames_batch, ms.float32)
            labels_tensor = ms.Tensor(labels_batch, ms.int32)
            
            # å‰å‘ä¼ æ’­
            logits = self.model(frames_tensor)
            loss = self.loss_fn(logits, labels_tensor)
            
            # é¢„æµ‹
            predictions = ops.ArgMaxWithValue(axis=1)(logits)[0]
            correct = ops.equal(predictions, labels_tensor)
            
            total_loss += loss.asnumpy()
            total_correct += correct.sum().asnumpy()
            total_samples += len(labels_batch)
            
            # ç»Ÿè®¡å„ç±»åˆ«å‡†ç¡®ç‡
            for i in range(len(labels_batch)):
                true_label = labels_batch[i]
                pred_label = predictions[i].asnumpy()
                
                class_total[true_label] += 1
                if pred_label == true_label:
                    class_correct[true_label] += 1
        
        avg_loss = total_loss / batch_count if batch_count > 0 else 0
        accuracy = total_correct / total_samples if total_samples > 0 else 0
        
        # æ‰“å°å„ç±»åˆ«å‡†ç¡®ç‡
        vocab_items = list(self.vocab.items())
        logger.info("å„ç±»åˆ«å‡†ç¡®ç‡:")
        for class_id, count in class_total.items():
            if count > 0:
                class_name = next((name for name, id in vocab_items if id == class_id), f"Class_{class_id}")
                class_acc = class_correct[class_id] / count
                logger.info(f"  {class_name}: {class_acc:.4f}")
        
        return avg_loss, accuracy
    
    def train(self):
        """å¼€å§‹è®­ç»ƒ"""
        logger.info("ğŸ¯ å¼€å§‹ä¿®å¤ç‰ˆé«˜å‡†ç¡®ç‡è®­ç»ƒ...")
        
        for epoch in range(1, self.config.epochs + 1):
            epoch_start_time = time.time()
            
            logger.info(f"å¼€å§‹ç¬¬ {epoch}/{self.config.epochs} è½®è®­ç»ƒ...")
            
            # è®­ç»ƒ
            train_loss, train_acc = self.train_epoch()
            
            logger.info(f"Epoch {epoch} è®­ç»ƒå®Œæˆ:")
            logger.info(f"  å¹³å‡æŸå¤±: {train_loss:.4f}")
            logger.info(f"  è®­ç»ƒå‡†ç¡®ç‡: {train_acc:.4f}")
            
            # è¯„ä¼°
            logger.info("å¼€å§‹æ¨¡å‹è¯„ä¼°...")
            val_loss, val_acc = self.evaluate()
            
            logger.info(f"è¯„ä¼°å®Œæˆ:")
            logger.info(f"  éªŒè¯æŸå¤±: {val_loss:.4f}")
            logger.info(f"  éªŒè¯å‡†ç¡®ç‡: {val_acc:.4f}")
            
            epoch_time = time.time() - epoch_start_time
            
            # è®°å½•å†å²
            self.train_history['train_loss'].append(train_loss)
            self.train_history['train_acc'].append(train_acc)
            self.train_history['val_loss'].append(val_loss)
            self.train_history['val_acc'].append(val_acc)
            self.train_history['epoch_times'].append(epoch_time)
            
            logger.info(f"Epoch {epoch} æ€»ç»“:")
            logger.info(f"  è®­ç»ƒæŸå¤±: {train_loss:.4f}, è®­ç»ƒå‡†ç¡®ç‡: {train_acc:.4f}")
            logger.info(f"  éªŒè¯æŸå¤±: {val_loss:.4f}, éªŒè¯å‡†ç¡®ç‡: {val_acc:.4f}")
            logger.info(f"  è€—æ—¶: {epoch_time:.2f}ç§’")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_acc > self.best_val_acc:
                self.best_val_acc = val_acc
                self.patience_counter = 0
                save_checkpoint(self.model, "output/fixed_accuracy_best_model.ckpt")
                logger.info(f"æ–°çš„æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {val_acc:.4f}")
                logger.info("æœ€ä½³æ¨¡å‹å·²ä¿å­˜: output/fixed_accuracy_best_model.ckpt")
            else:
                self.patience_counter += 1
                logger.info(f"éªŒè¯å‡†ç¡®ç‡æœªæå‡ï¼Œè€å¿ƒè®¡æ•°: {self.patience_counter}/{self.config.patience}")
            
            # æ—©åœæ£€æŸ¥
            if epoch >= self.config.min_epochs and self.patience_counter >= self.config.patience:
                logger.info(f"æ—©åœè§¦å‘ï¼Œåœ¨ç¬¬ {epoch} è½®åœæ­¢è®­ç»ƒ")
                break
        
        logger.info(f"è®­ç»ƒå®Œæˆ! æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {self.best_val_acc:.4f}")
        return self.best_val_acc
    
    def save_final_model(self):
        """ä¿å­˜æœ€ç»ˆæ¨¡å‹"""
        # ä¿å­˜æ¨¡å‹
        save_checkpoint(self.model, "output/fixed_accuracy_final_model.ckpt")
        
        # ä¿å­˜è¯æ±‡è¡¨
        vocab_data = {
            'vocab': self.vocab,
            'num_classes': self.config.num_classes,
            'label_names': list(self.vocab.keys())
        }
        
        with open("output/fixed_accuracy_vocab.json", 'w', encoding='utf-8') as f:
            json.dump(vocab_data, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜è®­ç»ƒå†å²
        with open("output/fixed_accuracy_training_history.json", 'w', encoding='utf-8') as f:
            json.dump(self.train_history, f, indent=2)
        
        logger.info("âœ… ä¿®å¤ç‰ˆæ¨¡å‹å’Œè¯æ±‡è¡¨ä¿å­˜å®Œæˆ")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ ä¿®å¤ç‰ˆé«˜å‡†ç¡®ç‡CE-CSLæ‰‹è¯­è¯†åˆ«è®­ç»ƒå¯åŠ¨")
    print("ğŸ”§ ä¸»è¦ä¿®å¤:")
    print("  âœ“ ä¿®å¤è¯æ±‡è¡¨æ„å»ºé—®é¢˜")
    print("  âœ“ ç®€åŒ–æ¨¡å‹æ¶æ„é˜²æ­¢è¿‡æ‹Ÿåˆ")
    print("  âœ“ ä¼˜åŒ–æ•°æ®åŠ è½½æµç¨‹")
    print("  âœ“ ä½¿ç”¨æ›´ä¿å®ˆçš„è®­ç»ƒç­–ç•¥")
    
    # åˆ›å»ºé…ç½®
    config = FixedAccuracyConfig()
    
    print("ğŸ“Š è¯¦ç»†é…ç½®:")
    print(f"  - è®­ç»ƒè½®æ•°: {config.epochs}")
    print(f"  - æ‰¹æ¬¡å¤§å°: {config.batch_size}")
    print(f"  - å­¦ä¹ ç‡: {config.learning_rate}")
    print(f"  - æƒé‡è¡°å‡: {config.weight_decay}")
    print(f"  - è®¾å¤‡: {config.device_target}")
    print(f"  - éšè—ç»´åº¦: {config.hidden_size}")
    print(f"  - Dropoutç‡: {config.dropout_rate}")
    print(f"  - æ•°æ®ç›®å½•: {config.data_dir}")
    print(f"  - æ•°æ®å¢å¼ºå€æ•°: {config.augment_factor}")
    print(f"  - æ—©åœè€å¿ƒå€¼: {config.patience}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    Path("output").mkdir(exist_ok=True)
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = FixedTrainer(config)
    
    # å¼€å§‹è®­ç»ƒ
    best_acc = trainer.train()
    
    # ä¿å­˜æ¨¡å‹
    print("ğŸ’¾ ä¿å­˜æœ€ç»ˆæ¨¡å‹...")
    trainer.save_final_model()
    
    print("ğŸ‰ ä¿®å¤ç‰ˆé«˜å‡†ç¡®ç‡è®­ç»ƒå®Œæˆï¼")
    print(f"ğŸ“ æ¨¡å‹å·²ä¿å­˜åˆ°: ./output/fixed_accuracy_final_model.ckpt")
    print(f"ğŸ“Š è®­ç»ƒå†å²å·²ä¿å­˜åˆ°: ./output/fixed_accuracy_training_history.json")
    print(f"ğŸ† æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_acc:.4f}")
    print("âœ¨ ä¸»è¦æ”¹è¿›æ•ˆæœ:")
    print("  âœ“ è¯æ±‡è¡¨æ­£ç¡®æ„å»º")
    print("  âœ“ æ•°æ®æˆåŠŸåŠ è½½å’Œå¢å¼º")
    print("  âœ“ æ¨¡å‹æ¶æ„ç®€åŒ–ä½†æœ‰æ•ˆ")
    print("  âœ“ è®­ç»ƒè¿‡ç¨‹ç¨³å®šå¯æ§")

if __name__ == "__main__":
    main()
