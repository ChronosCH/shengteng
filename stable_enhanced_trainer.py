#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç¨³å®šå¢å¼ºCE-CSLæ‰‹è¯­è¯†åˆ«è®­ç»ƒå™¨
åŸºäºenhanced_ultra_simple_trainer.pyçš„ç¨³å®šç‰ˆæœ¬ï¼Œæ·»åŠ æ¸è¿›å¼ä¼˜åŒ–æŠ€æœ¯
"""

import os
import json
import time
import logging
import numpy as np
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass

import mindspore as ms
import mindspore.nn as nn
import mindspore.ops as ops
from mindspore import Tensor, context, Parameter
from mindspore.nn import Cell
from mindspore.train import Model
from mindspore.nn.loss import CrossEntropyLoss
from mindspore.nn.optim import Adam
from mindspore.nn.metrics import Accuracy

# é…ç½®ç¯å¢ƒ
os.environ['GLOG_v'] = '2'
os.environ['GLOG_logtostderr'] = '1'
context.set_context(mode=context.PYNATIVE_MODE, device_target="CPU")

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

@dataclass
class TrainingConfig:
    """è®­ç»ƒé…ç½®"""
    num_epochs: int = 50
    batch_size: int = 2
    learning_rate: float = 0.001
    min_learning_rate: float = 0.0001
    hidden_dim: int = 20  # ç¨å¾®å¢åŠ éšè—å±‚
    dropout_rate: float = 0.3
    warmup_epochs: int = 5
    patience: int = 20
    gradient_clip_norm: float = 1.0
    weight_decay: float = 0.01
    label_smoothing: float = 0.1
    data_augmentation_factor: int = 12  # å¢å¼ºå€æ•°
    vocab_size: int = 10
    sequence_length: int = 100
    num_frames: int = 543  # æ¯ä¸ªè§†é¢‘çš„å…³é”®ç‚¹æ•°é‡
    checkpoint_dir: str = "checkpoints/stable_enhanced"

class StableEnhancedModel(Cell):
    """ç¨³å®šçš„å¢å¼ºæ¨¡å‹"""
    def __init__(self, config: TrainingConfig):
        super().__init__()
        self.config = config
        
        # ç‰¹å¾æå–å±‚
        self.feature_extractor = nn.SequentialCell([
            nn.Dense(config.num_frames, config.hidden_dim * 2),
            nn.BatchNorm1d(config.hidden_dim * 2),
            nn.ReLU(),
            nn.Dropout(p=config.dropout_rate)
        ])
        
        # æ—¶åºå»ºæ¨¡å±‚ - ç®€åŒ–çš„æ³¨æ„åŠ›æœºåˆ¶
        self.temporal_layer = nn.SequentialCell([
            nn.Dense(config.hidden_dim * 2, config.hidden_dim),
            nn.BatchNorm1d(config.hidden_dim),
            nn.ReLU(),
            nn.Dropout(p=config.dropout_rate)
        ])
        
        # åˆ†ç±»å±‚
        self.classifier = nn.SequentialCell([
            nn.Dense(config.hidden_dim, config.vocab_size),
        ])
        
        # åˆå§‹åŒ–æƒé‡
        self._init_weights()
    
    def _init_weights(self):
        """æƒé‡åˆå§‹åŒ–"""
        for cell in self.cells():
            if isinstance(cell, nn.Dense):
                cell.weight.set_data(ms.common.initializer.initializer(
                    "xavier_uniform", cell.weight.shape, cell.weight.dtype
                ))
                if cell.bias is not None:
                    cell.bias.set_data(ms.common.initializer.initializer(
                        "zeros", cell.bias.shape, cell.bias.dtype
                    ))
    
    def construct(self, x):
        """å‰å‘ä¼ æ’­"""
        # x shape: (batch_size, sequence_length, num_frames)
        batch_size, seq_len, _ = x.shape
        
        # å¤„ç†æ¯ä¸ªæ—¶é—´æ­¥
        outputs = []
        for i in range(seq_len):
            frame = x[:, i, :]  # (batch_size, num_frames)
            
            # ç‰¹å¾æå–
            features = self.feature_extractor(frame)  # (batch_size, hidden_dim*2)
            
            # æ—¶åºå»ºæ¨¡
            temporal_features = self.temporal_layer(features)  # (batch_size, hidden_dim)
            outputs.append(temporal_features)
        
        # å¹³å‡æ± åŒ–èåˆæ—¶åºä¿¡æ¯
        sequence_features = ops.Stack(axis=1)(outputs)  # (batch_size, seq_len, hidden_dim)
        pooled_features = ops.ReduceMean(keep_dims=False)(sequence_features, 1)  # (batch_size, hidden_dim)
        
        # åˆ†ç±»
        logits = self.classifier(pooled_features)  # (batch_size, vocab_size)
        
        return logits

class LabelSmoothingCrossEntropy(Cell):
    """æ ‡ç­¾å¹³æ»‘äº¤å‰ç†µæŸå¤±"""
    def __init__(self, num_classes: int, smoothing: float = 0.1):
        super().__init__()
        self.num_classes = num_classes
        self.smoothing = smoothing
        self.confidence = 1.0 - smoothing
        self.log_softmax = nn.LogSoftmax(axis=-1)
        
    def construct(self, logits, target):
        """è®¡ç®—æ ‡ç­¾å¹³æ»‘äº¤å‰ç†µ"""
        log_probs = self.log_softmax(logits)
        
        # ä½¿ç”¨ç®€åŒ–çš„æ ‡ç­¾å¹³æ»‘
        smooth_factor = self.smoothing / (self.num_classes - 1)
        one_hot = ops.OneHot()(target, self.num_classes, 
                               Tensor(self.confidence, ms.float32), 
                               Tensor(smooth_factor, ms.float32))
        
        # è®¡ç®—æŸå¤±
        loss = -ops.ReduceSum(keep_dims=False)(one_hot * log_probs, -1)
        return ops.ReduceMean()(loss)

class CosineAnnealingLR:
    """ä½™å¼¦é€€ç«å­¦ä¹ ç‡è°ƒåº¦å™¨"""
    def __init__(self, base_lr: float, min_lr: float, total_epochs: int, warmup_epochs: int = 0):
        self.base_lr = base_lr
        self.min_lr = min_lr
        self.total_epochs = total_epochs
        self.warmup_epochs = warmup_epochs
    
    def get_lr(self, epoch: int) -> float:
        """è·å–å½“å‰epochçš„å­¦ä¹ ç‡"""
        if epoch < self.warmup_epochs:
            # Warmupé˜¶æ®µ
            return self.base_lr * (epoch + 1) / self.warmup_epochs
        else:
            # ä½™å¼¦é€€ç«é˜¶æ®µ
            cosine_epoch = epoch - self.warmup_epochs
            cosine_total = self.total_epochs - self.warmup_epochs
            cosine_factor = 0.5 * (1 + np.cos(np.pi * cosine_epoch / cosine_total))
            return self.min_lr + (self.base_lr - self.min_lr) * cosine_factor

class StableEnhancedTrainer:
    """ç¨³å®šå¢å¼ºè®­ç»ƒå™¨"""
    def __init__(self, config: TrainingConfig):
        self.config = config
        self.vocab = self._build_vocab()
        self.word_to_id = {word: i for i, word in enumerate(self.vocab)}
        self.id_to_word = {i: word for i, word in enumerate(self.vocab)}
        
        # åˆ›å»ºæ£€æŸ¥ç‚¹ç›®å½•
        os.makedirs(config.checkpoint_dir, exist_ok=True)
        
        logger.info("ç¨³å®šå¢å¼ºCE-CSLè®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"è¯æ±‡è¡¨: {self.vocab}")
        logger.info(f"è¯æ±‡è¡¨å¤§å°: {len(self.vocab)}")
    
    def _build_vocab(self) -> List[str]:
        """æ„å»ºè¯æ±‡è¡¨"""
        return ['<PAD>', '<UNK>', 'è¯·', 'è°¢è°¢', 'ä½ å¥½', 'å†è§', 'å¥½çš„', 'æ˜¯çš„', 'æˆ‘', 'ä¸æ˜¯']
    
    def _create_enhanced_mock_data(self, split: str) -> Tuple[List[np.ndarray], List[int]]:
        """åˆ›å»ºå¢å¼ºçš„æ¨¡æ‹Ÿæ•°æ®"""
        logger.info(f"åˆ›å»ºå¢å¼ºæ¨¡æ‹Ÿæ•°æ®...")
        
        np.random.seed(42 if split == 'train' else 123)
        
        if split == 'train':
            base_samples = 250  # åŸºç¡€æ ·æœ¬æ•°
            total_samples = base_samples * self.config.data_augmentation_factor
        else:
            total_samples = 32
        
        data, labels = [], []
        
        for i in range(total_samples):
            # é€‰æ‹©ç±»åˆ« (è·³è¿‡PADå’ŒUNK)
            label = np.random.choice(range(2, len(self.vocab)))
            
            # ç”ŸæˆåŸºç¡€æ¨¡å¼
            base_pattern = self._generate_class_pattern(label)
            
            # æ•°æ®å¢å¼º
            if split == 'train':
                augmented_pattern = self._apply_data_augmentation(base_pattern)
            else:
                augmented_pattern = base_pattern
            
            data.append(augmented_pattern)
            labels.append(label)
        
        logger.info(f"å¢å¼ºæ¨¡æ‹Ÿæ•°æ®åˆ›å»ºå®Œæˆ - {split}é›†: {len(data)} æ ·æœ¬")
        return data, labels
    
    def _generate_class_pattern(self, class_id: int) -> np.ndarray:
        """ä¸ºç‰¹å®šç±»åˆ«ç”Ÿæˆç‰¹å¾æ¨¡å¼"""
        pattern = np.random.randn(self.config.sequence_length, self.config.num_frames) * 0.1
        
        # ä¸ºæ¯ä¸ªç±»åˆ«æ·»åŠ ç‹¬ç‰¹çš„ä¿¡å·æ¨¡å¼
        class_patterns = {
            2: (0.8, 0.3),   # è¯·
            3: (0.6, 0.7),   # è°¢è°¢  
            4: (0.9, 0.2),   # ä½ å¥½
            5: (0.4, 0.8),   # å†è§
            6: (0.7, 0.5),   # å¥½çš„
            7: (0.5, 0.9),   # æ˜¯çš„
            8: (0.3, 0.6),   # æˆ‘
            9: (0.2, 0.4),   # ä¸æ˜¯
        }
        
        if class_id in class_patterns:
            amp1, amp2 = class_patterns[class_id]
            
            # æ·»åŠ æ—¶é—´æ¨¡å¼
            for t in range(self.config.sequence_length):
                time_factor = t / self.config.sequence_length
                
                # ç¬¬ä¸€ä¸ªç‰¹å¾ç»„ (å‰1/3)
                feature_range1 = slice(0, self.config.num_frames // 3)
                pattern[t, feature_range1] += amp1 * np.sin(2 * np.pi * time_factor * (class_id + 1))
                
                # ç¬¬äºŒä¸ªç‰¹å¾ç»„ (ä¸­1/3)  
                feature_range2 = slice(self.config.num_frames // 3, 2 * self.config.num_frames // 3)
                pattern[t, feature_range2] += amp2 * np.cos(2 * np.pi * time_factor * (class_id + 2))
                
                # ç¬¬ä¸‰ä¸ªç‰¹å¾ç»„ (å1/3) - ç»„åˆæ¨¡å¼
                feature_range3 = slice(2 * self.config.num_frames // 3, self.config.num_frames)
                pattern[t, feature_range3] += (amp1 + amp2) / 2 * np.sin(4 * np.pi * time_factor * class_id)
        
        return pattern.astype(np.float32)
    
    def _apply_data_augmentation(self, pattern: np.ndarray) -> np.ndarray:
        """åº”ç”¨æ•°æ®å¢å¼ºæŠ€æœ¯"""
        augmented = pattern.copy()
        
        # 1. é«˜æ–¯å™ªå£°
        noise_level = np.random.uniform(0.01, 0.05)
        augmented += np.random.normal(0, noise_level, augmented.shape)
        
        # 2. æ—¶é—´å¹³ç§»
        if np.random.random() < 0.3:
            shift = np.random.randint(-5, 6)
            if shift > 0:
                augmented = np.concatenate([augmented[shift:], augmented[:shift]], axis=0)
            elif shift < 0:
                augmented = np.concatenate([augmented[shift:], augmented[:shift]], axis=0)
        
        # 3. ç‰¹å¾ç¼©æ”¾
        if np.random.random() < 0.3:
            scale_factor = np.random.uniform(0.8, 1.2)
            augmented *= scale_factor
        
        # 4. éšæœºé®æŒ¡
        if np.random.random() < 0.2:
            mask_length = np.random.randint(5, 15)
            mask_start = np.random.randint(0, self.config.sequence_length - mask_length)
            augmented[mask_start:mask_start+mask_length] *= 0.1
        
        # 5. ç‰¹å¾ç»´åº¦æ‰°åŠ¨
        if np.random.random() < 0.3:
            feature_mask = np.random.random(self.config.num_frames) > 0.1
            augmented[:, ~feature_mask] *= np.random.uniform(0.5, 1.5)
        
        return augmented.astype(np.float32)
    
    def create_dataset(self, split: str):
        """åˆ›å»ºæ•°æ®é›†"""
        data, labels = self._create_enhanced_mock_data(split)
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        data_array = np.array(data, dtype=np.float32)
        labels_array = np.array(labels, dtype=np.int32)
        
        # åˆ›å»ºæ•°æ®é›†
        dataset = ms.dataset.NumpySlicesDataset((data_array, labels_array), column_names=["data", "label"])
        dataset = dataset.batch(self.config.batch_size, drop_remainder=False)
        
        return dataset
    
    def train(self):
        """æ‰§è¡Œè®­ç»ƒ"""
        logger.info("ğŸ¯ å¼€å§‹ç¨³å®šå¢å¼ºè®­ç»ƒ...")
        
        # åˆ›å»ºæ•°æ®é›†
        train_dataset = self.create_dataset('train')
        val_dataset = self.create_dataset('dev')
        
        # åˆ›å»ºæ¨¡å‹
        model = StableEnhancedModel(self.config)
        
        # åˆ›å»ºæŸå¤±å‡½æ•°
        loss_fn = LabelSmoothingCrossEntropy(
            num_classes=self.config.vocab_size,
            smoothing=self.config.label_smoothing
        )
        
        # åˆ›å»ºä¼˜åŒ–å™¨
        optimizer = Adam(
            model.trainable_params(),
            learning_rate=self.config.learning_rate,
            weight_decay=self.config.weight_decay
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        lr_scheduler = CosineAnnealingLR(
            base_lr=self.config.learning_rate,
            min_lr=self.config.min_learning_rate,
            total_epochs=self.config.num_epochs,
            warmup_epochs=self.config.warmup_epochs
        )
        
        # è®­ç»ƒçŠ¶æ€
        best_val_acc = 0.0
        patience_counter = 0
        train_step = 0
        
        # å¼€å§‹è®­ç»ƒ
        for epoch in range(self.config.num_epochs):
            start_time = time.time()
            
            # æ›´æ–°å­¦ä¹ ç‡ - MindSporeä¸­éœ€è¦é‡æ–°åˆ›å»ºä¼˜åŒ–å™¨
            current_lr = lr_scheduler.get_lr(epoch)
            if epoch > 0:  # ç¬¬ä¸€ä¸ªepochä½¿ç”¨åˆå§‹å­¦ä¹ ç‡
                optimizer = Adam(
                    model.trainable_params(),
                    learning_rate=current_lr,
                    weight_decay=self.config.weight_decay
                )
            
            logger.info(f"å¼€å§‹ç¬¬ {epoch+1}/{self.config.num_epochs} è½®è®­ç»ƒ... LR: {current_lr:.6f}")
            
            # è®­ç»ƒé˜¶æ®µ
            model.set_train(True)
            train_losses = []
            train_correct = 0
            train_total = 0
            
            for batch_idx, batch_data in enumerate(train_dataset.create_tuple_iterator()):
                train_step += 1
                
                # è§£åŒ…æ•°æ® - MindSporeè¿”å›çš„æ˜¯tuple
                data, labels = batch_data
                if not isinstance(data, Tensor):
                    data = Tensor(data, ms.float32)
                if not isinstance(labels, Tensor):
                    labels = Tensor(labels, ms.int32)
                
                # å‰å‘ä¼ æ’­
                def forward_fn(data, labels):
                    logits = model(data)
                    loss = loss_fn(logits, labels)
                    return loss, logits
                
                # è®¡ç®—æ¢¯åº¦
                grad_fn = ms.value_and_grad(forward_fn, None, optimizer.parameters, has_aux=True)
                (loss, logits), grads = grad_fn(data, labels)
                
                # æ¢¯åº¦è£å‰ª
                grads = ops.clip_by_global_norm(grads, self.config.gradient_clip_norm)
                
                # æ›´æ–°å‚æ•°
                optimizer(grads)
                
                # ç»Ÿè®¡
                train_losses.append(loss.asnumpy())
                preds = ops.Argmax(axis=1)(logits)
                train_correct += ops.ReduceSum()(ops.Equal()(preds, labels)).asnumpy()
                train_total += labels.shape[0]
                
                # æ‰“å°è¿›åº¦
                if (batch_idx + 1) % 50 == 0:
                    avg_loss = np.mean(train_losses[-50:])
                    acc = train_correct / train_total
                    logger.info(f"æ‰¹æ¬¡ {batch_idx + 1}: Loss = {avg_loss:.4f}, å‡†ç¡®ç‡ = {acc:.4f}")
            
            # è®­ç»ƒepochç»Ÿè®¡
            epoch_train_loss = np.mean(train_losses)
            epoch_train_acc = train_correct / train_total
            
            logger.info(f"Epoch {epoch+1} è®­ç»ƒå®Œæˆ:")
            logger.info(f"  å¹³å‡æŸå¤±: {epoch_train_loss:.4f}")
            logger.info(f"  è®­ç»ƒå‡†ç¡®ç‡: {epoch_train_acc:.4f}")
            
            # éªŒè¯é˜¶æ®µ
            model.set_train(False)
            val_losses = []
            val_correct = 0
            val_total = 0
            class_correct = {i: 0 for i in range(2, self.config.vocab_size)}
            class_total = {i: 0 for i in range(2, self.config.vocab_size)}
            
            logger.info("å¼€å§‹éªŒè¯...")
            for batch_data in val_dataset.create_tuple_iterator():
                # è§£åŒ…æ•°æ® - MindSporeè¿”å›çš„æ˜¯tuple
                data, labels = batch_data
                if not isinstance(data, Tensor):
                    data = Tensor(data, ms.float32)
                if not isinstance(labels, Tensor):
                    labels = Tensor(labels, ms.int32)
                
                logits = model(data)
                loss = loss_fn(logits, labels)
                
                val_losses.append(loss.asnumpy())
                preds = ops.Argmax(axis=1)(logits)
                
                # æ€»ä½“ç»Ÿè®¡
                val_correct += ops.ReduceSum()(ops.Equal()(preds, labels)).asnumpy()
                val_total += labels.shape[0]
                
                # å„ç±»åˆ«ç»Ÿè®¡
                for i in range(labels.shape[0]):
                    true_label = int(labels[i].asnumpy())
                    pred_label = int(preds[i].asnumpy())
                    
                    if true_label in class_total:
                        class_total[true_label] += 1
                        if true_label == pred_label:
                            class_correct[true_label] += 1
            
            # éªŒè¯epochç»Ÿè®¡
            epoch_val_loss = np.mean(val_losses)
            epoch_val_acc = val_correct / val_total
            
            # æ‰“å°å„ç±»åˆ«å‡†ç¡®ç‡
            logger.info("å„ç±»åˆ«å‡†ç¡®ç‡:")
            for class_id in range(2, self.config.vocab_size):
                class_name = self.id_to_word[class_id]
                if class_total[class_id] > 0:
                    class_acc = class_correct[class_id] / class_total[class_id]
                    logger.info(f"  {class_name}: {class_acc:.4f} ({class_correct[class_id]}/{class_total[class_id]})")
                else:
                    logger.info(f"  {class_name}: æ— æ ·æœ¬")
            
            logger.info(f"éªŒè¯å®Œæˆ:")
            logger.info(f"  éªŒè¯æŸå¤±: {epoch_val_loss:.4f}")
            logger.info(f"  éªŒè¯å‡†ç¡®ç‡: {epoch_val_acc:.4f}")
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºæœ€ä½³æ¨¡å‹
            if epoch_val_acc > best_val_acc:
                best_val_acc = epoch_val_acc
                patience_counter = 0
                
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                checkpoint_path = os.path.join(self.config.checkpoint_dir, "best_model.ckpt")
                ms.save_checkpoint(model, checkpoint_path)
                logger.info(f"ğŸ‰ æ–°çš„æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.4f}")
                logger.info(f"æœ€ä½³æ¨¡å‹å·²ä¿å­˜åˆ°: {checkpoint_path}")
            else:
                patience_counter += 1
                logger.info(f"éªŒè¯å‡†ç¡®ç‡æœªæå‡ï¼Œè€å¿ƒè®¡æ•°: {patience_counter}/{self.config.patience}")
            
            # è®¡ç®—è€—æ—¶
            epoch_time = time.time() - start_time
            logger.info(f"Epoch {epoch+1} æ€»ç»“:")
            logger.info(f"  è®­ç»ƒ: Loss={epoch_train_loss:.4f}, Acc={epoch_train_acc:.4f}")
            logger.info(f"  éªŒè¯: Loss={epoch_val_loss:.4f}, Acc={epoch_val_acc:.4f}")
            logger.info(f"  å­¦ä¹ ç‡: {current_lr:.6f}, è€—æ—¶: {epoch_time:.2f}ç§’")
            
            # æ—©åœæ£€æŸ¥
            if patience_counter >= self.config.patience:
                logger.info(f"æ—©åœè§¦å‘ï¼æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.4f}")
                break
        
        logger.info("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
        logger.info(f"ğŸ“Š æœ€ç»ˆç»Ÿè®¡:")
        logger.info(f"  æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.4f}")
        logger.info(f"  è®­ç»ƒè½®æ•°: {epoch+1}")
        logger.info(f"  æ¨¡å‹ä¿å­˜è·¯å¾„: {self.config.checkpoint_dir}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ ç¨³å®šå¢å¼ºCE-CSLæ‰‹è¯­è¯†åˆ«è®­ç»ƒå¯åŠ¨")
    print("ğŸ¯ æ¸è¿›å¼ä¼˜åŒ–æ–¹æ¡ˆ:")
    print("  âœ“ å¢å¼ºæ•°æ®å¢å¼º - 12å€æ•°æ®")
    print("  âœ“ æ ‡ç­¾å¹³æ»‘ - æå‡æ³›åŒ–")
    print("  âœ“ ä½™å¼¦é€€ç«å­¦ä¹ ç‡ - ç¨³å®šæ”¶æ•›")
    print("  âœ“ æ¢¯åº¦è£å‰ª - é˜²æ­¢çˆ†ç‚¸")
    print("  âœ“ æƒé‡è¡°å‡ - æ­£åˆ™åŒ–")
    print("  âœ“ Warmupç­–ç•¥ - ç¨³å®šå¯åŠ¨")
    print("  âœ“ æ—¶åºç‰¹å¾æå– - æ”¹è¿›æ¶æ„")
    
    config = TrainingConfig()
    print(f"ğŸ“Š è®­ç»ƒé…ç½®:")
    print(f"  - è®­ç»ƒè½®æ•°: {config.num_epochs}")
    print(f"  - æ‰¹æ¬¡å¤§å°: {config.batch_size}")
    print(f"  - å­¦ä¹ ç‡: {config.learning_rate}")
    print(f"  - éšè—ç»´åº¦: {config.hidden_dim}")
    print(f"  - æ•°æ®å¢å¼ºå€æ•°: {config.data_augmentation_factor}")
    print(f"  - æ ‡ç­¾å¹³æ»‘: {config.label_smoothing}")
    print(f"  - æƒé‡è¡°å‡: {config.weight_decay}")
    
    trainer = StableEnhancedTrainer(config)
    trainer.train()

if __name__ == "__main__":
    main()
