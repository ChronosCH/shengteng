#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é«˜å‡†ç¡®ç‡CE-CSLæ‰‹è¯­è¯†åˆ«è®­ç»ƒå™¨ - ä¸“é—¨é’ˆå¯¹å°æ•°æ®é›†ä¼˜åŒ–
"""

import os
import sys
import json
import time
import logging
from pathlib import Path
from dataclasses import dataclass
from typing import List, Dict, Tuple, Optional
import random
import numpy as np

import mindspore as ms
from mindspore import nn, ops, context, Model, load_checkpoint, save_checkpoint
from mindspore.dataset import GeneratorDataset
from mindspore.train.callback import Callback
from mindspore.communication.management import init, get_rank, get_group_size

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class HighAccuracyConfig:
    """é«˜å‡†ç¡®ç‡è®­ç»ƒé…ç½®"""
    # æ•°æ®é…ç½®
    data_dir: str = "data/CE-CSL"
    vocab_file: str = "backend/models/vocab.json"
    
    # æ¨¡å‹é…ç½® - é’ˆå¯¹å°æ•°æ®é›†ä¼˜åŒ–
    input_size: int = 150528  # 224*224*3
    hidden_size: int = 128  # å‡å°ä»¥é˜²æ­¢è¿‡æ‹Ÿåˆ
    num_layers: int = 1  # ç®€åŒ–æ¨¡å‹
    num_classes: int = 10
    dropout_rate: float = 0.5  # å¢å¼ºæ­£åˆ™åŒ–
    
    # è®­ç»ƒé…ç½® - ä¿å®ˆè®¾ç½®
    batch_size: int = 2  # å°æ‰¹æ¬¡
    learning_rate: float = 0.001  # ç¨é«˜å­¦ä¹ ç‡
    epochs: int = 50
    weight_decay: float = 0.01  # å¼ºæ­£åˆ™åŒ–
    
    # æ•°æ®å¢å¼º
    augment_factor: int = 20  # å¤§å¹…å¢å¼ºæ•°æ®
    
    # æ—©åœå’Œä¿å­˜
    patience: int = 25
    min_epochs: int = 10
    
    # è®¾å¤‡é…ç½®
    device_target: str = "CPU"

class AggressiveDataAugmentor:
    """æ¿€è¿›çš„æ•°æ®å¢å¼ºå™¨ - ä¸“é—¨é’ˆå¯¹å°æ•°æ®é›†"""
    
    def __init__(self, config: HighAccuracyConfig):
        self.config = config
        
    def augment_sample(self, frames: np.ndarray, label: int) -> List[Tuple[np.ndarray, int]]:
        """å¯¹å•ä¸ªæ ·æœ¬è¿›è¡Œæ¿€è¿›çš„æ•°æ®å¢å¼º"""
        augmented_samples = [(frames, label)]  # åŸå§‹æ ·æœ¬
        
        for i in range(self.config.augment_factor - 1):
            # éšæœºé€‰æ‹©å¢å¼ºæ–¹æ³•
            aug_type = random.choice(['time_warp', 'noise', 'brightness', 'contrast', 'flip', 'crop'])
            
            if aug_type == 'time_warp':
                aug_frames = self._time_warp(frames, strength=random.uniform(0.1, 0.3))
            elif aug_type == 'noise':
                aug_frames = self._add_noise(frames, noise_factor=random.uniform(0.05, 0.15))
            elif aug_type == 'brightness':
                aug_frames = self._adjust_brightness(frames, factor=random.uniform(0.7, 1.3))
            elif aug_type == 'contrast':
                aug_frames = self._adjust_contrast(frames, factor=random.uniform(0.8, 1.2))
            elif aug_type == 'flip':
                aug_frames = self._horizontal_flip(frames)
            else:  # crop
                aug_frames = self._random_crop_resize(frames, crop_ratio=random.uniform(0.85, 0.95))
            
            augmented_samples.append((aug_frames, label))
        
        return augmented_samples
    
    def _time_warp(self, frames: np.ndarray, strength: float = 0.2) -> np.ndarray:
        """æ—¶é—´æ‰­æ›² - æ”¹å˜åºåˆ—é•¿åº¦"""
        seq_len = frames.shape[0]
        if seq_len <= 10:
            return frames
        
        # éšæœºé€‰æ‹©æ–°çš„åºåˆ—é•¿åº¦
        new_len = int(seq_len * (1 + random.uniform(-strength, strength)))
        new_len = max(10, min(new_len, seq_len * 2))
        
        # é‡é‡‡æ ·
        indices = np.linspace(0, seq_len - 1, new_len).astype(int)
        return frames[indices]
    
    def _add_noise(self, frames: np.ndarray, noise_factor: float = 0.1) -> np.ndarray:
        """æ·»åŠ é«˜æ–¯å™ªå£°"""
        noise = np.random.normal(0, noise_factor * 255, frames.shape).astype(np.float32)
        noisy_frames = frames.astype(np.float32) + noise
        return np.clip(noisy_frames, 0, 255).astype(np.uint8)
    
    def _adjust_brightness(self, frames: np.ndarray, factor: float = 1.2) -> np.ndarray:
        """è°ƒæ•´äº®åº¦"""
        bright_frames = frames.astype(np.float32) * factor
        return np.clip(bright_frames, 0, 255).astype(np.uint8)
    
    def _adjust_contrast(self, frames: np.ndarray, factor: float = 1.2) -> np.ndarray:
        """è°ƒæ•´å¯¹æ¯”åº¦"""
        mean = np.mean(frames)
        contrast_frames = mean + factor * (frames.astype(np.float32) - mean)
        return np.clip(contrast_frames, 0, 255).astype(np.uint8)
    
    def _horizontal_flip(self, frames: np.ndarray) -> np.ndarray:
        """æ°´å¹³ç¿»è½¬"""
        return np.flip(frames, axis=2)  # ç¿»è½¬å®½åº¦ç»´åº¦
    
    def _random_crop_resize(self, frames: np.ndarray, crop_ratio: float = 0.9) -> np.ndarray:
        """éšæœºè£å‰ªå¹¶è°ƒæ•´å¤§å°"""
        seq_len, h, w, c = frames.shape
        
        # è®¡ç®—è£å‰ªå°ºå¯¸
        crop_h = int(h * crop_ratio)
        crop_w = int(w * crop_ratio)
        
        # éšæœºè£å‰ªä½ç½®
        start_h = random.randint(0, h - crop_h)
        start_w = random.randint(0, w - crop_w)
        
        # è£å‰ª
        cropped = frames[:, start_h:start_h+crop_h, start_w:start_w+crop_w, :]
        
        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œç›´æ¥å¡«å……åˆ°åŸå°ºå¯¸
        resized = np.zeros((seq_len, h, w, c), dtype=frames.dtype)
        resized[:, :crop_h, :crop_w, :] = cropped
        
        return resized

class HighAccuracyModel(nn.Cell):
    """é«˜å‡†ç¡®ç‡æ‰‹è¯­è¯†åˆ«æ¨¡å‹ - é’ˆå¯¹å°æ•°æ®é›†ä¼˜åŒ–"""
    
    def __init__(self, config: HighAccuracyConfig):
        super().__init__()
        self.config = config
        
        # ç‰¹å¾é™ç»´ - å…³é”®ä¼˜åŒ–
        self.feature_reducer = nn.SequentialCell([
            nn.Dense(config.input_size, 512),
            nn.ReLU(),
            nn.Dropout(config.dropout_rate),
            nn.Dense(512, 256),
            nn.ReLU(),
            nn.Dropout(config.dropout_rate),
            nn.Dense(256, 128),
            nn.ReLU(),
            nn.Dropout(config.dropout_rate)
        ])
        
        # æ—¶åºå»ºæ¨¡ - ç®€åŒ–çš„LSTM
        self.lstm = nn.LSTM(
            input_size=128,
            hidden_size=config.hidden_size,
            num_layers=config.num_layers,
            batch_first=True,
            dropout=float(config.dropout_rate) if config.num_layers > 1 else 0.0
        )
        
        # åˆ†ç±»å¤´ - å¢åŠ æ­£åˆ™åŒ–
        self.classifier = nn.SequentialCell([
            nn.Dense(config.hidden_size, 64),
            nn.ReLU(),
            nn.Dropout(0.7),  # é«˜dropout
            nn.Dense(64, config.num_classes)
        ])
        
        # åˆå§‹åŒ–æƒé‡
        self._init_weights()
    
    def _init_weights(self):
        """åˆå§‹åŒ–æƒé‡"""
        for _, cell in self.cells_and_names():
            if isinstance(cell, nn.Dense):
                cell.weight.set_data(ms.common.initializer.initializer(
                    ms.common.initializer.XavierUniform(), cell.weight.shape, cell.weight.dtype))
                if cell.bias is not None:
                    cell.bias.set_data(ms.common.initializer.initializer(
                        'zeros', cell.bias.shape, cell.bias.dtype))
            elif isinstance(cell, nn.LSTM):
                for name, param in cell.parameters_and_names():
                    if 'weight' in name:
                        param.set_data(ms.common.initializer.initializer(
                            ms.common.initializer.XavierUniform(), param.shape, param.dtype))
    
    def construct(self, x):
        # x shape: (batch, seq_len, height, width, channels)
        batch_size, seq_len = x.shape[0], x.shape[1]
        
        # å±•å¹³ç©ºé—´ç»´åº¦
        x = x.view(batch_size * seq_len, -1)  # (batch*seq, features)
        
        # ç‰¹å¾é™ç»´
        x = self.feature_reducer(x)  # (batch*seq, 128)
        
        # é‡å¡‘ä¸ºåºåˆ—
        x = x.view(batch_size, seq_len, -1)  # (batch, seq, 128)
        
        # LSTMå¤„ç†
        lstm_out, _ = self.lstm(x)  # (batch, seq, hidden)
        
        # ä½¿ç”¨æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º
        last_output = lstm_out[:, -1, :]  # (batch, hidden)
        
        # åˆ†ç±»
        logits = self.classifier(last_output)  # (batch, num_classes)
        
        return logits

class HighAccuracyDataset:
    """é«˜å‡†ç¡®ç‡æ•°æ®é›†"""
    
    def __init__(self, data_dir: str, split: str, config: HighAccuracyConfig, 
                 vocab: Dict[str, int], augmentor: Optional[AggressiveDataAugmentor] = None):
        self.data_dir = Path(data_dir)
        self.split = split
        self.config = config
        self.vocab = vocab
        self.augmentor = augmentor
        
        # åŠ è½½æ•°æ®
        self.samples = self._load_samples()
        logger.info(f"åŠ è½½ {split} æ•°æ®é›†: {len(self.samples)} ä¸ªæ ·æœ¬")
    
    def _load_samples(self) -> List[Tuple[np.ndarray, int]]:
        """åŠ è½½æ•°æ®æ ·æœ¬"""
        samples = []
        
        # åŠ è½½å…ƒæ•°æ®
        metadata_file = self.data_dir / "processed" / self.split / f"{self.split}_metadata.json"
        if not metadata_file.exists():
            logger.error(f"å…ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {metadata_file}")
            return samples
        
        with open(metadata_file, 'r', encoding='utf-8') as f:
            metadata = json.load(f)
        
        for item in metadata:
            try:
                # åŠ è½½å¸§æ•°æ®
                frames_path = self.data_dir / "processed" / self.split / f"{item['video_id']}_frames.npy"
                if not frames_path.exists():
                    logger.warning(f"å¸§æ–‡ä»¶ä¸å­˜åœ¨: {frames_path}")
                    continue
                
                frames = np.load(frames_path)
                
                # è·å–æ ‡ç­¾
                gloss = item['gloss_sequence'][0]  # å‡è®¾åªæœ‰ä¸€ä¸ªæ‰‹è¯­è¯
                if gloss not in self.vocab:
                    logger.warning(f"æœªçŸ¥è¯æ±‡: {gloss}")
                    continue
                
                label = self.vocab[gloss]
                
                # æ•°æ®é¢„å¤„ç†
                frames = self._preprocess_frames(frames)
                
                # å¦‚æœæ˜¯è®­ç»ƒé›†ä¸”æœ‰å¢å¼ºå™¨ï¼Œè¿›è¡Œæ•°æ®å¢å¼º
                if self.split == "train" and self.augmentor:
                    augmented = self.augmentor.augment_sample(frames, label)
                    samples.extend(augmented)
                else:
                    samples.append((frames, label))
                
            except Exception as e:
                logger.error(f"åŠ è½½æ ·æœ¬å¤±è´¥ {item.get('video_id', 'unknown')}: {e}")
        
        return samples
    
    def _preprocess_frames(self, frames: np.ndarray) -> np.ndarray:
        """é¢„å¤„ç†å¸§æ•°æ®"""
        # å½’ä¸€åŒ–åˆ°[0,1]
        frames = frames.astype(np.float32) / 255.0
        
        # å›ºå®šåºåˆ—é•¿åº¦
        target_len = 100
        seq_len = frames.shape[0]
        
        if seq_len > target_len:
            # å‡åŒ€é‡‡æ ·
            indices = np.linspace(0, seq_len - 1, target_len).astype(int)
            frames = frames[indices]
        elif seq_len < target_len:
            # é‡å¤æœ€åä¸€å¸§è¿›è¡Œå¡«å……
            padding = np.repeat(frames[-1:], target_len - seq_len, axis=0)
            frames = np.concatenate([frames, padding], axis=0)
        
        return frames
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        frames, label = self.samples[idx]
        return frames, label

class HighAccuracyTrainer:
    """é«˜å‡†ç¡®ç‡è®­ç»ƒå™¨"""
    
    def __init__(self, config: HighAccuracyConfig):
        self.config = config
        self.setup_environment()
        self.load_vocab()
        self.setup_data()
        self.setup_model()
        
        # è®­ç»ƒå†å²
        self.train_history = {
            'train_loss': [],
            'train_acc': [],
            'val_loss': [],
            'val_acc': [],
            'epoch_times': []
        }
        
        self.best_val_acc = 0.0
        self.patience_counter = 0
    
    def setup_environment(self):
        """è®¾ç½®ç¯å¢ƒ"""
        context.set_context(mode=context.GRAPH_MODE, device_target=self.config.device_target)
        logger.info(f"é«˜å‡†ç¡®ç‡CE-CSLè®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ - è®¾å¤‡: {self.config.device_target}")
    
    def load_vocab(self):
        """åŠ è½½è¯æ±‡è¡¨"""
        vocab_path = Path(self.config.vocab_file)
        if vocab_path.exists():
            with open(vocab_path, 'r', encoding='utf-8') as f:
                vocab_data = json.load(f)
            self.vocab = vocab_data.get('vocab', {})
        else:
            # åŸºäºæ•°æ®æ„å»ºè¯æ±‡è¡¨
            self.vocab = self._build_vocab()
        
        # ç¡®ä¿åŒ…å«ç‰¹æ®Štoken
        if '<PAD>' not in self.vocab:
            self.vocab['<PAD>'] = 0
        if '<UNK>' not in self.vocab:
            self.vocab['<UNK>'] = 1
        
        self.config.num_classes = len(self.vocab)
        logger.info(f"è¯æ±‡è¡¨æ„å»ºå®Œæˆ: {list(self.vocab.keys())}")
        logger.info(f"è¯æ±‡è¡¨å¤§å°: {self.config.num_classes}")
    
    def _build_vocab(self) -> Dict[str, int]:
        """æ„å»ºè¯æ±‡è¡¨"""
        vocab = {'<PAD>': 0, '<UNK>': 1}
        
        # æ‰«æè®­ç»ƒæ•°æ®
        train_metadata = self.config.data_dir + "/processed/train/train_metadata.json"
        if Path(train_metadata).exists():
            with open(train_metadata, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
            
            for item in metadata:
                for gloss in item.get('gloss_sequence', []):
                    if gloss not in vocab:
                        vocab[gloss] = len(vocab)
        
        return vocab
    
    def setup_data(self):
        """è®¾ç½®æ•°æ®"""
        logger.info("ğŸ“Š åŠ è½½æ•°æ®ï¼ˆåŒ…å«æ¿€è¿›æ•°æ®å¢å¼ºï¼‰...")
        
        # åˆ›å»ºæ•°æ®å¢å¼ºå™¨
        augmentor = AggressiveDataAugmentor(self.config)
        
        # åˆ›å»ºæ•°æ®é›†
        self.train_dataset = HighAccuracyDataset(
            self.config.data_dir, "train", self.config, self.vocab, augmentor
        )
        self.val_dataset = HighAccuracyDataset(
            self.config.data_dir, "dev", self.config, self.vocab
        )
        
        logger.info(f"è®­ç»ƒé›†: {len(self.train_dataset)} æ ·æœ¬ï¼ˆåŒ…å«å¢å¼ºæ•°æ®ï¼‰")
        logger.info(f"éªŒè¯é›†: {len(self.val_dataset)} æ ·æœ¬")
    
    def setup_model(self):
        """è®¾ç½®æ¨¡å‹"""
        logger.info("ğŸ§  æ„å»ºé«˜å‡†ç¡®ç‡æ¨¡å‹...")
        
        self.model = HighAccuracyModel(self.config)
        
        # è®¡ç®—å‚æ•°é‡
        total_params = sum(p.size for p in self.model.trainable_params())
        logger.info(f"æ¨¡å‹æ„å»ºå®Œæˆ - å‚æ•°é‡: {total_params}")
        
        # è®¾ç½®ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
        self.optimizer = nn.Adam(
            self.model.trainable_params(),
            learning_rate=self.config.learning_rate,
            weight_decay=self.config.weight_decay
        )
        
        self.loss_fn = nn.CrossEntropyLoss()
        logger.info("ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°åˆ›å»ºå®Œæˆ")
    
    def create_dataloader(self, dataset, shuffle=True):
        """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
        def generator():
            indices = list(range(len(dataset)))
            if shuffle:
                random.shuffle(indices)
            
            batch_frames = []
            batch_labels = []
            
            for idx in indices:
                frames, label = dataset[idx]
                batch_frames.append(frames)
                batch_labels.append(label)
                
                if len(batch_frames) == self.config.batch_size:
                    yield (np.stack(batch_frames), np.array(batch_labels))
                    batch_frames = []
                    batch_labels = []
            
            # å¤„ç†æœ€åä¸€ä¸ªä¸å®Œæ•´çš„æ‰¹æ¬¡
            if batch_frames:
                yield (np.stack(batch_frames), np.array(batch_labels))
        
        return generator
    
    def train_epoch(self):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.set_train()
        
        epoch_loss = 0.0
        epoch_correct = 0
        epoch_total = 0
        batch_count = 0
        
        dataloader = self.create_dataloader(self.train_dataset, shuffle=True)
        
        for batch_frames, batch_labels in dataloader():
            batch_count += 1
            
            # è½¬æ¢ä¸ºTensor
            frames_tensor = ms.Tensor(batch_frames, ms.float32)
            labels_tensor = ms.Tensor(batch_labels, ms.int32)
            
            # å‰å‘ä¼ æ’­
            def forward_fn():
                logits = self.model(frames_tensor)
                loss = self.loss_fn(logits, labels_tensor)
                return loss, logits
            
            grad_fn = ms.ops.value_and_grad(forward_fn, None, self.optimizer.parameters, has_aux=True)
            (loss, logits), grads = grad_fn()
            
            # åå‘ä¼ æ’­
            self.optimizer(grads)
            
            # ç»Ÿè®¡
            predictions = ops.argmax(logits, axis=1)
            correct = ops.equal(predictions, labels_tensor).sum()
            
            epoch_loss += loss.asnumpy()
            epoch_correct += correct.asnumpy()
            epoch_total += len(batch_labels)
            
            if batch_count % 5 == 0:
                logger.info(f"Batch {batch_count}: Loss = {loss.asnumpy():.4f}")
        
        avg_loss = epoch_loss / batch_count if batch_count > 0 else 0
        accuracy = epoch_correct / epoch_total if epoch_total > 0 else 0
        
        return avg_loss, accuracy
    
    def evaluate(self):
        """è¯„ä¼°æ¨¡å‹"""
        self.model.set_train(False)
        
        total_loss = 0.0
        total_correct = 0
        total_samples = 0
        batch_count = 0
        
        # ç±»åˆ«ç»Ÿè®¡
        class_correct = {i: 0 for i in range(self.config.num_classes)}
        class_total = {i: 0 for i in range(self.config.num_classes)}
        
        dataloader = self.create_dataloader(self.val_dataset, shuffle=False)
        
        for batch_frames, batch_labels in dataloader():
            batch_count += 1
            
            frames_tensor = ms.Tensor(batch_frames, ms.float32)
            labels_tensor = ms.Tensor(batch_labels, ms.int32)
            
            # å‰å‘ä¼ æ’­
            logits = self.model(frames_tensor)
            loss = self.loss_fn(logits, labels_tensor)
            
            # é¢„æµ‹
            predictions = ops.argmax(logits, axis=1)
            correct = ops.equal(predictions, labels_tensor)
            
            total_loss += loss.asnumpy()
            total_correct += correct.sum().asnumpy()
            total_samples += len(batch_labels)
            
            # ç»Ÿè®¡å„ç±»åˆ«å‡†ç¡®ç‡
            for i in range(len(batch_labels)):
                true_label = batch_labels[i]
                pred_label = predictions[i].asnumpy()
                
                class_total[true_label] += 1
                if pred_label == true_label:
                    class_correct[true_label] += 1
        
        avg_loss = total_loss / batch_count if batch_count > 0 else 0
        accuracy = total_correct / total_samples if total_samples > 0 else 0
        
        # æ‰“å°å„ç±»åˆ«å‡†ç¡®ç‡
        vocab_items = list(self.vocab.items())
        logger.info("å„ç±»åˆ«å‡†ç¡®ç‡:")
        for class_id, count in class_total.items():
            if count > 0:
                class_name = next((name for name, id in vocab_items if id == class_id), f"Class_{class_id}")
                class_acc = class_correct[class_id] / count
                logger.info(f"  {class_name}: {class_acc:.4f}")
        
        return avg_loss, accuracy
    
    def train(self):
        """å¼€å§‹è®­ç»ƒ"""
        logger.info("ğŸ¯ å¼€å§‹é«˜å‡†ç¡®ç‡è®­ç»ƒ...")
        logger.info(f"â±ï¸  é¢„æœŸæ¯è½®è®­ç»ƒæ—¶é—´å°†å¤§å¹…å¢åŠ ï¼ˆæ•°æ®å¢å¼º{self.config.augment_factor}å€ï¼‰")
        
        for epoch in range(1, self.config.epochs + 1):
            epoch_start_time = time.time()
            
            logger.info(f"å¼€å§‹ç¬¬ {epoch}/{self.config.epochs} è½®è®­ç»ƒ...")
            
            # è®­ç»ƒ
            train_loss, train_acc = self.train_epoch()
            
            logger.info(f"Epoch {epoch} è®­ç»ƒå®Œæˆ:")
            logger.info(f"  å¹³å‡æŸå¤±: {train_loss:.4f}")
            logger.info(f"  è®­ç»ƒå‡†ç¡®ç‡: {train_acc:.4f}")
            
            # è¯„ä¼°
            logger.info("å¼€å§‹æ¨¡å‹è¯„ä¼°...")
            val_loss, val_acc = self.evaluate()
            
            logger.info(f"è¯„ä¼°å®Œæˆ:")
            logger.info(f"  éªŒè¯æŸå¤±: {val_loss:.4f}")
            logger.info(f"  éªŒè¯å‡†ç¡®ç‡: {val_acc:.4f}")
            
            epoch_time = time.time() - epoch_start_time
            
            # è®°å½•å†å²
            self.train_history['train_loss'].append(train_loss)
            self.train_history['train_acc'].append(train_acc)
            self.train_history['val_loss'].append(val_loss)
            self.train_history['val_acc'].append(val_acc)
            self.train_history['epoch_times'].append(epoch_time)
            
            logger.info(f"Epoch {epoch} æ€»ç»“:")
            logger.info(f"  è®­ç»ƒæŸå¤±: {train_loss:.4f}, è®­ç»ƒå‡†ç¡®ç‡: {train_acc:.4f}")
            logger.info(f"  éªŒè¯æŸå¤±: {val_loss:.4f}, éªŒè¯å‡†ç¡®ç‡: {val_acc:.4f}")
            logger.info(f"  è€—æ—¶: {epoch_time:.2f}ç§’")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_acc > self.best_val_acc:
                self.best_val_acc = val_acc
                self.patience_counter = 0
                save_checkpoint(self.model, "output/high_accuracy_best_model.ckpt")
                logger.info(f"æ–°çš„æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {val_acc:.4f}")
                logger.info("æœ€ä½³æ¨¡å‹å·²ä¿å­˜: output/high_accuracy_best_model.ckpt")
            else:
                self.patience_counter += 1
                logger.info(f"éªŒè¯å‡†ç¡®ç‡æœªæå‡ï¼Œè€å¿ƒè®¡æ•°: {self.patience_counter}/{self.config.patience}")
            
            # æ—©åœæ£€æŸ¥
            if epoch >= self.config.min_epochs and self.patience_counter >= self.config.patience:
                logger.info(f"æ—©åœè§¦å‘ï¼Œåœ¨ç¬¬ {epoch} è½®åœæ­¢è®­ç»ƒ")
                break
        
        logger.info(f"è®­ç»ƒå®Œæˆ! æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {self.best_val_acc:.4f}")
        return self.best_val_acc
    
    def save_final_model(self):
        """ä¿å­˜æœ€ç»ˆæ¨¡å‹"""
        # ä¿å­˜æ¨¡å‹
        save_checkpoint(self.model, "output/high_accuracy_final_model.ckpt")
        
        # ä¿å­˜è¯æ±‡è¡¨
        vocab_data = {
            'vocab': self.vocab,
            'num_classes': self.config.num_classes,
            'label_names': list(self.vocab.keys())
        }
        
        with open("output/high_accuracy_vocab.json", 'w', encoding='utf-8') as f:
            json.dump(vocab_data, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜è®­ç»ƒå†å²
        with open("output/high_accuracy_training_history.json", 'w', encoding='utf-8') as f:
            json.dump(self.train_history, f, indent=2)
        
        logger.info("âœ… é«˜å‡†ç¡®ç‡æ¨¡å‹å’Œè¯æ±‡è¡¨ä¿å­˜å®Œæˆ")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ é«˜å‡†ç¡®ç‡CE-CSLæ‰‹è¯­è¯†åˆ«è®­ç»ƒå¯åŠ¨")
    print("ğŸ”§ ä¸»è¦ä¼˜åŒ–:")
    print("  âœ“ æ¿€è¿›æ•°æ®å¢å¼º: æ¯ä¸ªæ ·æœ¬å¢å¼º20å€ï¼Œè§£å†³æ•°æ®ä¸¥é‡ä¸è¶³")
    print("  âœ“ æ¨¡å‹ç®€åŒ–: é˜²æ­¢è¿‡æ‹Ÿåˆï¼Œä¸“ä¸ºå°æ•°æ®é›†è®¾è®¡")
    print("  âœ“ å¼ºæ­£åˆ™åŒ–: é«˜dropout + æƒé‡è¡°å‡")
    print("  âœ“ ä¿å®ˆè®­ç»ƒ: å°æ‰¹æ¬¡ + é€‚ä¸­å­¦ä¹ ç‡")
    
    # åˆ›å»ºé…ç½®
    config = HighAccuracyConfig()
    
    print("ğŸ“Š è¯¦ç»†é…ç½®:")
    print(f"  - è®­ç»ƒè½®æ•°: {config.epochs}")
    print(f"  - æ‰¹æ¬¡å¤§å°: {config.batch_size}")
    print(f"  - å­¦ä¹ ç‡: {config.learning_rate}")
    print(f"  - æƒé‡è¡°å‡: {config.weight_decay}")
    print(f"  - è®¾å¤‡: {config.device_target}")
    print(f"  - éšè—ç»´åº¦: {config.hidden_size}")
    print(f"  - Dropoutç‡: {config.dropout_rate}")
    print(f"  - æ•°æ®ç›®å½•: {config.data_dir}")
    print(f"  - æ•°æ®å¢å¼ºå€æ•°: {config.augment_factor}")
    print(f"  - æ—©åœè€å¿ƒå€¼: {config.patience}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    Path("output").mkdir(exist_ok=True)
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = HighAccuracyTrainer(config)
    
    # å¼€å§‹è®­ç»ƒ
    best_acc = trainer.train()
    
    # ä¿å­˜æ¨¡å‹
    print("ğŸ’¾ ä¿å­˜æœ€ç»ˆæ¨¡å‹...")
    trainer.save_final_model()
    
    print("ğŸ‰ é«˜å‡†ç¡®ç‡è®­ç»ƒå®Œæˆï¼")
    print(f"ğŸ“ æ¨¡å‹å·²ä¿å­˜åˆ°: ./output/high_accuracy_final_model.ckpt")
    print(f"ğŸ“Š è®­ç»ƒå†å²å·²ä¿å­˜åˆ°: ./output/high_accuracy_training_history.json")
    print(f"ğŸ† æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_acc:.4f}")
    print("âœ¨ ä¸»è¦æ”¹è¿›æ•ˆæœ:")
    print("  âœ“ æ•°æ®é‡å¤§å¹…å¢åŠ ï¼ˆ20å€å¢å¼ºï¼‰")
    print("  âœ“ æ¨¡å‹ä¸“ä¸ºå°æ•°æ®é›†ä¼˜åŒ–")
    print("  âœ“ å¼ºæ­£åˆ™åŒ–é˜²æ­¢è¿‡æ‹Ÿåˆ")
    print("  âœ“ ä¿å®ˆè®­ç»ƒç­–ç•¥ç¡®ä¿ç¨³å®šæ”¶æ•›")

if __name__ == "__main__":
    main()
