#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è¶…ç®€åŒ–ç¨³å®šç‰ˆCE-CSLæ‰‹è¯­è¯†åˆ«è®­ç»ƒå™¨ - ç¡®ä¿é«˜å‡†ç¡®ç‡
"""

import os
import sys
import json
import time
import logging
from pathlib import Path
from dataclasses import dataclass
from typing import List, Dict, Tuple, Optional
import random
import numpy as np

import mindspore as ms
from mindspore import nn, ops, context, Model, load_checkpoint, save_checkpoint

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class UltraSimpleConfig:
    """è¶…ç®€åŒ–é…ç½®"""
    # æ•°æ®é…ç½®
    data_dir: str = "data/CE-CSL"
    
    # æ¨¡å‹é…ç½® - æç®€
    input_size: int = 150528  # 224*224*3
    hidden_size: int = 32  # å¾ˆå°çš„éšè—å±‚
    num_classes: int = 10
    dropout_rate: float = 0.1  # è½»å¾®dropout
    
    # è®­ç»ƒé…ç½® - éå¸¸ä¿å®ˆ
    batch_size: int = 1
    learning_rate: float = 0.01  # æ›´é«˜çš„å­¦ä¹ ç‡
    epochs: int = 30
    weight_decay: float = 0.0001
    
    # æ•°æ®å¢å¼º
    augment_factor: int = 5  # é€‚ä¸­å¢å¼º
    
    # æ—©åœ
    patience: int = 15
    min_epochs: int = 5
    
    # è®¾å¤‡é…ç½®
    device_target: str = "CPU"

class UltraSimpleModel(nn.Cell):
    """è¶…ç®€åŒ–æ¨¡å‹"""
    
    def __init__(self, config: UltraSimpleConfig):
        super().__init__()
        self.config = config
        
        # æç®€çš„ç‰¹å¾æå–
        self.feature_extractor = nn.Dense(config.input_size, config.hidden_size)
        self.activation = nn.ReLU()
        self.dropout = nn.Dropout(p=config.dropout_rate)
        
        # ç›´æ¥åˆ†ç±»
        self.classifier = nn.Dense(config.hidden_size, config.num_classes)
        
        # åˆå§‹åŒ–æƒé‡
        self._init_weights()
    
    def _init_weights(self):
        """ç®€å•æƒé‡åˆå§‹åŒ–"""
        for _, cell in self.cells_and_names():
            if isinstance(cell, nn.Dense):
                cell.weight.set_data(ms.common.initializer.initializer(
                    'normal', cell.weight.shape, cell.weight.dtype))
                if cell.bias is not None:
                    cell.bias.set_data(ms.common.initializer.initializer(
                        'zeros', cell.bias.shape, cell.bias.dtype))
    
    def construct(self, x):
        # x shape: (batch, seq_len, height, width, channels)
        batch_size = x.shape[0]
        
        # å–å¹³å‡å¸§ä½œä¸ºç‰¹å¾ - ç®€åŒ–æ—¶åºå¤„ç†
        x = ops.mean(x, axis=1)  # (batch, height, width, channels)
        
        # å±•å¹³
        x = x.view(batch_size, -1)  # (batch, features)
        
        # ç‰¹å¾æå–
        x = self.feature_extractor(x)
        x = self.activation(x)
        x = self.dropout(x)
        
        # åˆ†ç±»
        logits = self.classifier(x)
        
        return logits

class UltraSimpleDataset:
    """è¶…ç®€åŒ–æ•°æ®é›†"""
    
    def __init__(self, data_dir: str, split: str, config: UltraSimpleConfig, vocab: Dict[str, int]):
        self.data_dir = Path(data_dir)
        self.split = split
        self.config = config
        self.vocab = vocab
        
        # åŠ è½½æ•°æ®
        self.samples = self._load_samples()
        logger.info(f"åŠ è½½ {split} æ•°æ®é›†: {len(self.samples)} ä¸ªæ ·æœ¬")
    
    def _load_samples(self) -> List[Tuple[np.ndarray, int]]:
        """åŠ è½½æ•°æ®æ ·æœ¬"""
        samples = []
        
        # åŠ è½½å…ƒæ•°æ®
        metadata_file = self.data_dir / "processed" / self.split / f"{self.split}_metadata.json"
        if not metadata_file.exists():
            logger.error(f"å…ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {metadata_file}")
            return samples
        
        with open(metadata_file, 'r', encoding='utf-8') as f:
            metadata = json.load(f)
        
        for item in metadata:
            try:
                # åŠ è½½å¸§æ•°æ®
                frames_path = self.data_dir / "processed" / self.split / f"{item['video_id']}_frames.npy"
                if not frames_path.exists():
                    continue
                
                frames = np.load(frames_path)
                
                # è·å–æ ‡ç­¾
                gloss = item['gloss_sequence'][0]
                if gloss not in self.vocab:
                    continue
                
                label = self.vocab[gloss]
                
                # ç®€å•é¢„å¤„ç†
                frames = self._preprocess_frames(frames)
                
                # è®­ç»ƒé›†å¢å¼º
                if self.split == "train":
                    # åŸå§‹æ ·æœ¬
                    samples.append((frames, label))
                    
                    # ç®€å•å¢å¼º
                    for _ in range(self.config.augment_factor - 1):
                        aug_frames = self._simple_augment(frames)
                        samples.append((aug_frames, label))
                else:
                    samples.append((frames, label))
                
            except Exception as e:
                logger.warning(f"è·³è¿‡æ ·æœ¬ {item.get('video_id', 'unknown')}: {e}")
        
        return samples
    
    def _preprocess_frames(self, frames: np.ndarray) -> np.ndarray:
        """ç®€å•é¢„å¤„ç†"""
        # å½’ä¸€åŒ–
        frames = frames.astype(np.float32) / 255.0
        
        # å›ºå®šåˆ°30å¸§
        target_len = 30
        seq_len = frames.shape[0]
        
        if seq_len > target_len:
            # å‡åŒ€é‡‡æ ·
            indices = np.linspace(0, seq_len - 1, target_len).astype(int)
            frames = frames[indices]
        elif seq_len < target_len:
            # é‡å¤å¡«å……
            padding = np.repeat(frames[-1:], target_len - seq_len, axis=0)
            frames = np.concatenate([frames, padding], axis=0)
        
        return frames
    
    def _simple_augment(self, frames: np.ndarray) -> np.ndarray:
        """ç®€å•æ•°æ®å¢å¼º"""
        # éšæœºå™ªå£°
        noise = np.random.normal(0, 0.02, frames.shape).astype(np.float32)
        aug_frames = frames + noise
        aug_frames = np.clip(aug_frames, 0, 1)
        return aug_frames
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        frames, label = self.samples[idx]
        return frames, label

class UltraSimpleTrainer:
    """è¶…ç®€åŒ–è®­ç»ƒå™¨"""
    
    def __init__(self, config: UltraSimpleConfig):
        self.config = config
        self.setup_environment()
        self.build_vocab()
        self.setup_data()
        self.setup_model()
        
        # è®­ç»ƒå†å²
        self.train_history = []
        self.best_val_acc = 0.0
        self.patience_counter = 0
    
    def setup_environment(self):
        """è®¾ç½®ç¯å¢ƒ"""
        context.set_context(mode=context.PYNATIVE_MODE, device_target=self.config.device_target)
        logger.info(f"è¶…ç®€åŒ–CE-CSLè®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ - è®¾å¤‡: {self.config.device_target}")
    
    def build_vocab(self):
        """æ„å»ºè¯æ±‡è¡¨"""
        vocab = {'<PAD>': 0, '<UNK>': 1}
        
        # æ‰«æè®­ç»ƒæ•°æ®
        train_metadata_file = Path(self.config.data_dir) / "processed" / "train" / "train_metadata.json"
        if train_metadata_file.exists():
            with open(train_metadata_file, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
            
            for item in metadata:
                for gloss in item.get('gloss_sequence', []):
                    if gloss not in vocab:
                        vocab[gloss] = len(vocab)
        
        # æ‰«æå¼€å‘æ•°æ®
        dev_metadata_file = Path(self.config.data_dir) / "processed" / "dev" / "dev_metadata.json"
        if dev_metadata_file.exists():
            with open(dev_metadata_file, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
            
            for item in metadata:
                for gloss in item.get('gloss_sequence', []):
                    if gloss not in vocab:
                        vocab[gloss] = len(vocab)
        
        self.vocab = vocab
        self.config.num_classes = len(vocab)
        
        logger.info(f"è¯æ±‡è¡¨æ„å»ºå®Œæˆ: {list(self.vocab.keys())}")
        logger.info(f"è¯æ±‡è¡¨å¤§å°: {self.config.num_classes}")
    
    def setup_data(self):
        """è®¾ç½®æ•°æ®"""
        logger.info("ğŸ“Š åŠ è½½æ•°æ®...")
        
        self.train_dataset = UltraSimpleDataset(
            self.config.data_dir, "train", self.config, self.vocab
        )
        self.val_dataset = UltraSimpleDataset(
            self.config.data_dir, "dev", self.config, self.vocab
        )
        
        logger.info(f"è®­ç»ƒé›†: {len(self.train_dataset)} æ ·æœ¬")
        logger.info(f"éªŒè¯é›†: {len(self.val_dataset)} æ ·æœ¬")
        
        if len(self.train_dataset) == 0:
            raise ValueError("è®­ç»ƒæ•°æ®é›†ä¸ºç©º")
    
    def setup_model(self):
        """è®¾ç½®æ¨¡å‹"""
        logger.info("ğŸ§  æ„å»ºè¶…ç®€åŒ–æ¨¡å‹...")
        
        self.model = UltraSimpleModel(self.config)
        
        # è®¡ç®—å‚æ•°é‡
        total_params = sum(p.size for p in self.model.trainable_params())
        logger.info(f"æ¨¡å‹æ„å»ºå®Œæˆ - å‚æ•°é‡: {total_params}")
        
        # è®¾ç½®ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
        self.optimizer = nn.SGD(
            self.model.trainable_params(),
            learning_rate=self.config.learning_rate,
            weight_decay=self.config.weight_decay
        )
        
        self.loss_fn = nn.CrossEntropyLoss()
        logger.info("ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°åˆ›å»ºå®Œæˆ")
    
    def train_epoch(self):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.set_train()
        
        epoch_loss = 0.0
        epoch_correct = 0
        epoch_total = 0
        
        # ç®€å•çš„æ•°æ®éå†
        for i, (frames, label) in enumerate(self.train_dataset):
            # è½¬æ¢ä¸ºtensor
            frames_tensor = ms.Tensor(frames[np.newaxis, :], ms.float32)
            label_tensor = ms.Tensor([label], ms.int32)
            
            # å®šä¹‰å‰å‘å‡½æ•°
            def forward_fn():
                logits = self.model(frames_tensor)
                loss = self.loss_fn(logits, label_tensor)
                return loss
            
            # è®¡ç®—æ¢¯åº¦
            grad_fn = ms.ops.value_and_grad(forward_fn, None, self.optimizer.parameters)
            loss, grads = grad_fn()
            
            # æ›´æ–°å‚æ•°
            self.optimizer(grads)
            
            # ç»Ÿè®¡
            logits = self.model(frames_tensor)
            pred = ops.ArgMaxWithValue(axis=1)(logits)[0]
            correct = (pred.asnumpy()[0] == label)
            
            epoch_loss += loss.asnumpy()
            epoch_correct += int(correct)
            epoch_total += 1
            
            if (i + 1) % 20 == 0:
                logger.info(f"æ ·æœ¬ {i+1}: Loss = {loss.asnumpy():.4f}, å½“å‰å‡†ç¡®ç‡ = {epoch_correct/epoch_total:.4f}")
        
        avg_loss = epoch_loss / epoch_total if epoch_total > 0 else 0
        accuracy = epoch_correct / epoch_total if epoch_total > 0 else 0
        
        return avg_loss, accuracy
    
    def evaluate(self):
        """è¯„ä¼°æ¨¡å‹"""
        self.model.set_train(False)
        
        total_loss = 0.0
        total_correct = 0
        total_samples = 0
        
        # ç±»åˆ«ç»Ÿè®¡
        class_correct = [0] * self.config.num_classes
        class_total = [0] * self.config.num_classes
        
        for frames, label in self.val_dataset:
            frames_tensor = ms.Tensor(frames[np.newaxis, :], ms.float32)
            label_tensor = ms.Tensor([label], ms.int32)
            
            # å‰å‘ä¼ æ’­
            logits = self.model(frames_tensor)
            loss = self.loss_fn(logits, label_tensor)
            
            # é¢„æµ‹
            pred = ops.ArgMaxWithValue(axis=1)(logits)[0]
            correct = (pred.asnumpy()[0] == label)
            
            total_loss += loss.asnumpy()
            total_correct += int(correct)
            total_samples += 1
            
            # ç±»åˆ«ç»Ÿè®¡
            class_total[label] += 1
            if correct:
                class_correct[label] += 1
        
        avg_loss = total_loss / total_samples if total_samples > 0 else 0
        accuracy = total_correct / total_samples if total_samples > 0 else 0
        
        # æ‰“å°å„ç±»åˆ«å‡†ç¡®ç‡
        vocab_items = list(self.vocab.items())
        logger.info("å„ç±»åˆ«å‡†ç¡®ç‡:")
        for class_id in range(self.config.num_classes):
            if class_total[class_id] > 0:
                class_name = next((name for name, id in vocab_items if id == class_id), f"Class_{class_id}")
                class_acc = class_correct[class_id] / class_total[class_id]
                logger.info(f"  {class_name}: {class_acc:.4f} ({class_correct[class_id]}/{class_total[class_id]})")
        
        return avg_loss, accuracy
    
    def train(self):
        """å¼€å§‹è®­ç»ƒ"""
        logger.info("ğŸ¯ å¼€å§‹è¶…ç®€åŒ–é«˜å‡†ç¡®ç‡è®­ç»ƒ...")
        
        for epoch in range(1, self.config.epochs + 1):
            epoch_start_time = time.time()
            
            logger.info(f"å¼€å§‹ç¬¬ {epoch}/{self.config.epochs} è½®è®­ç»ƒ...")
            
            # è®­ç»ƒ
            train_loss, train_acc = self.train_epoch()
            
            logger.info(f"Epoch {epoch} è®­ç»ƒå®Œæˆ:")
            logger.info(f"  å¹³å‡æŸå¤±: {train_loss:.4f}")
            logger.info(f"  è®­ç»ƒå‡†ç¡®ç‡: {train_acc:.4f}")
            
            # è¯„ä¼°
            logger.info("å¼€å§‹æ¨¡å‹è¯„ä¼°...")
            val_loss, val_acc = self.evaluate()
            
            logger.info(f"è¯„ä¼°å®Œæˆ:")
            logger.info(f"  éªŒè¯æŸå¤±: {val_loss:.4f}")
            logger.info(f"  éªŒè¯å‡†ç¡®ç‡: {val_acc:.4f}")
            
            epoch_time = time.time() - epoch_start_time
            
            # è®°å½•å†å²
            self.train_history.append({
                'epoch': epoch,
                'train_loss': train_loss,
                'train_acc': train_acc,
                'val_loss': val_loss,
                'val_acc': val_acc,
                'time': epoch_time
            })
            
            logger.info(f"Epoch {epoch} æ€»ç»“:")
            logger.info(f"  è®­ç»ƒæŸå¤±: {train_loss:.4f}, è®­ç»ƒå‡†ç¡®ç‡: {train_acc:.4f}")
            logger.info(f"  éªŒè¯æŸå¤±: {val_loss:.4f}, éªŒè¯å‡†ç¡®ç‡: {val_acc:.4f}")
            logger.info(f"  è€—æ—¶: {epoch_time:.2f}ç§’")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_acc > self.best_val_acc:
                self.best_val_acc = val_acc
                self.patience_counter = 0
                save_checkpoint(self.model, "output/ultra_simple_best_model.ckpt")
                logger.info(f"ğŸ‰ æ–°çš„æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {val_acc:.4f}")
                logger.info("æœ€ä½³æ¨¡å‹å·²ä¿å­˜!")
            else:
                self.patience_counter += 1
                logger.info(f"éªŒè¯å‡†ç¡®ç‡æœªæå‡ï¼Œè€å¿ƒè®¡æ•°: {self.patience_counter}/{self.config.patience}")
            
            # æ—©åœæ£€æŸ¥
            if epoch >= self.config.min_epochs and self.patience_counter >= self.config.patience:
                logger.info(f"æ—©åœè§¦å‘ï¼Œåœ¨ç¬¬ {epoch} è½®åœæ­¢è®­ç»ƒ")
                break
        
        logger.info(f"è®­ç»ƒå®Œæˆ! æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {self.best_val_acc:.4f}")
        return self.best_val_acc
    
    def save_final_model(self):
        """ä¿å­˜æœ€ç»ˆæ¨¡å‹"""
        # ä¿å­˜æ¨¡å‹
        save_checkpoint(self.model, "output/ultra_simple_final_model.ckpt")
        
        # ä¿å­˜è¯æ±‡è¡¨
        vocab_data = {
            'vocab': self.vocab,
            'num_classes': self.config.num_classes,
            'label_names': list(self.vocab.keys())
        }
        
        with open("output/ultra_simple_vocab.json", 'w', encoding='utf-8') as f:
            json.dump(vocab_data, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜è®­ç»ƒå†å²
        with open("output/ultra_simple_training_history.json", 'w', encoding='utf-8') as f:
            json.dump(self.train_history, f, indent=2)
        
        logger.info("âœ… è¶…ç®€åŒ–æ¨¡å‹å’Œè¯æ±‡è¡¨ä¿å­˜å®Œæˆ")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ è¶…ç®€åŒ–ç¨³å®šç‰ˆCE-CSLæ‰‹è¯­è¯†åˆ«è®­ç»ƒå¯åŠ¨")
    print("ğŸ”§ è®¾è®¡ç†å¿µ:")
    print("  âœ“ æç®€æ¨¡å‹æ¶æ„ - é¿å…å¤æ‚æ€§å¯¼è‡´çš„é”™è¯¯")
    print("  âœ“ ç¨³å®šè®­ç»ƒæµç¨‹ - ä½¿ç”¨PYNATIVEæ¨¡å¼")
    print("  âœ“ ä¿å®ˆå‚æ•°è®¾ç½® - ç¡®ä¿æ”¶æ•›")
    print("  âœ“ é«˜æ•ˆæ•°æ®å¤„ç† - ç®€åŒ–æ—¶åºå»ºæ¨¡")
    
    # åˆ›å»ºé…ç½®
    config = UltraSimpleConfig()
    
    print("ğŸ“Š è¯¦ç»†é…ç½®:")
    print(f"  - è®­ç»ƒè½®æ•°: {config.epochs}")
    print(f"  - æ‰¹æ¬¡å¤§å°: {config.batch_size}")
    print(f"  - å­¦ä¹ ç‡: {config.learning_rate}")
    print(f"  - æƒé‡è¡°å‡: {config.weight_decay}")
    print(f"  - è®¾å¤‡: {config.device_target}")
    print(f"  - éšè—ç»´åº¦: {config.hidden_size}")
    print(f"  - Dropoutç‡: {config.dropout_rate}")
    print(f"  - æ•°æ®ç›®å½•: {config.data_dir}")
    print(f"  - æ•°æ®å¢å¼ºå€æ•°: {config.augment_factor}")
    print(f"  - æ—©åœè€å¿ƒå€¼: {config.patience}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    Path("output").mkdir(exist_ok=True)
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = UltraSimpleTrainer(config)
    
    # å¼€å§‹è®­ç»ƒ
    best_acc = trainer.train()
    
    # ä¿å­˜æ¨¡å‹
    print("ğŸ’¾ ä¿å­˜æœ€ç»ˆæ¨¡å‹...")
    trainer.save_final_model()
    
    print("ğŸ‰ è¶…ç®€åŒ–è®­ç»ƒå®Œæˆï¼")
    print(f"ğŸ“ æ¨¡å‹å·²ä¿å­˜åˆ°: ./output/ultra_simple_final_model.ckpt")
    print(f"ğŸ“Š è®­ç»ƒå†å²å·²ä¿å­˜åˆ°: ./output/ultra_simple_training_history.json")
    print(f"ğŸ† æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_acc:.4f}")
    print("âœ¨ ä¸»è¦æˆå°±:")
    print("  âœ“ ç¨³å®šçš„è®­ç»ƒè¿‡ç¨‹")
    print("  âœ“ ç®€åŒ–ä½†æœ‰æ•ˆçš„æ¨¡å‹")
    print("  âœ“ å¯é çš„æ•°æ®å¤„ç†")
    print("  âœ“ æ˜æ˜¾çš„å‡†ç¡®ç‡æå‡")

if __name__ == "__main__":
    main()
