#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å…¨é¢åˆ†æCE-CSLæ•°æ®é›†
æ£€æŸ¥åŸå§‹è§†é¢‘æ•°é‡vsé¢„å¤„ç†æ•°é‡çš„å·®å¼‚
"""

import os
import csv
from pathlib import Path
import json

def analyze_full_dataset():
    """å…¨é¢åˆ†ææ•°æ®é›†"""
    data_root = Path("../data/CE-CSL")
    
    print("ğŸ” CE-CSLæ•°æ®é›†å…¨é¢åˆ†æ")
    print("=" * 60)
    
    # 1. æ£€æŸ¥åŸå§‹è§†é¢‘æ•°é‡
    print("\n1. åŸå§‹è§†é¢‘ç»Ÿè®¡:")
    video_root = data_root / "video"
    
    for split in ["train", "dev", "test"]:
        split_dir = video_root / split
        if not split_dir.exists():
            print(f"  {split}: ç›®å½•ä¸å­˜åœ¨")
            continue
            
        total_videos = 0
        translators = []
        
        for translator_dir in sorted(split_dir.iterdir()):
            if translator_dir.is_dir():
                translators.append(translator_dir.name)
                videos = list(translator_dir.glob("*.mp4"))
                total_videos += len(videos)
                print(f"  {split}/{translator_dir.name}: {len(videos)} ä¸ªè§†é¢‘")
        
        print(f"  {split} æ€»è®¡: {total_videos} ä¸ªè§†é¢‘ï¼Œ{len(translators)} ä¸ªè¯‘å‘˜")
    
    # 2. æ£€æŸ¥é¢„å¤„ç†æ•°æ®æ•°é‡
    print("\n2. é¢„å¤„ç†æ•°æ®ç»Ÿè®¡:")
    processed_root = data_root / "processed"
    
    for split in ["train", "dev", "test"]:
        split_dir = processed_root / split
        if not split_dir.exists():
            print(f"  {split}: é¢„å¤„ç†ç›®å½•ä¸å­˜åœ¨")
            continue
            
        npy_files = list(split_dir.glob("*_frames.npy"))
        metadata_file = split_dir / f"{split}_metadata.json"
        
        print(f"  {split}: {len(npy_files)} ä¸ª.npyæ–‡ä»¶")
        
        if metadata_file.exists():
            with open(metadata_file, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
            print(f"  {split}: å…ƒæ•°æ®è®°å½• {len(metadata)} æ¡")
        else:
            print(f"  {split}: æ— å…ƒæ•°æ®æ–‡ä»¶")
    
    # 3. æ£€æŸ¥corpusæ–‡ä»¶
    print("\n3. Corpusæ–‡ä»¶ç»Ÿè®¡:")
    
    for split in ["train", "dev", "test"]:
        corpus_file = data_root / f"{split}.corpus.csv"
        if corpus_file.exists():
            with open(corpus_file, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                rows = list(reader)
                print(f"  {split}.corpus.csv: {len(rows)} æ¡è®°å½•")
                
                # åˆ†ævideo_idæ¨¡å¼
                if rows:
                    video_ids = [row['video_id'] for row in rows]
                    unique_videos = len(set(video_ids))
                    print(f"    ç‹¬ç«‹è§†é¢‘ID: {unique_videos} ä¸ª")
                    print(f"    IDç¤ºä¾‹: {video_ids[:3]}...")
        else:
            print(f"  {split}.corpus.csv: æ–‡ä»¶ä¸å­˜åœ¨")
    
    # 4. æ£€æŸ¥labelæ–‡ä»¶
    print("\n4. Labelæ–‡ä»¶ç»Ÿè®¡:")
    label_dir = data_root / "label"
    
    if label_dir.exists():
        for split in ["train", "dev", "test"]:
            label_file = label_dir / f"{split}.csv"
            if label_file.exists():
                with open(label_file, 'r', encoding='utf-8') as f:
                    lines = f.readlines()
                    print(f"  {split}.csv: {len(lines)-1} æ¡è®°å½•ï¼ˆå«æ ‡é¢˜è¡Œï¼‰")
            else:
                print(f"  {split}.csv: æ–‡ä»¶ä¸å­˜åœ¨")
    else:
        print("  labelç›®å½•ä¸å­˜åœ¨")
    
    # 5. è¯†åˆ«é—®é¢˜
    print("\nğŸ”§ é—®é¢˜è¯Šæ–­:")
    print("=" * 60)
    
    # æ£€æŸ¥video_idå‘½åæ˜¯å¦ä¸å®é™…æ–‡ä»¶å¯¹åº”
    print("æ£€æŸ¥video_idå‘½åè§„åˆ™...")
    
    for split in ["train", "dev"]:  # é‡ç‚¹æ£€æŸ¥æœ‰æ•°æ®çš„split
        video_dir = video_root / split
        corpus_file = data_root / f"{split}.corpus.csv"
        
        if not video_dir.exists() or not corpus_file.exists():
            continue
            
        # ç»Ÿè®¡å®é™…è§†é¢‘æ–‡ä»¶
        actual_videos = []
        for translator_dir in sorted(video_dir.iterdir()):
            if translator_dir.is_dir():
                videos = sorted(translator_dir.glob("*.mp4"))
                for i, video_file in enumerate(videos):
                    actual_videos.append({
                        'translator': translator_dir.name,
                        'file_name': video_file.name,
                        'full_path': video_file,
                        'expected_id': f"{split}_video_{len(actual_videos):03d}"
                    })
        
        # è¯»å–corpusä¸­çš„video_id
        with open(corpus_file, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            corpus_ids = [row['video_id'] for row in reader]
        
        print(f"\n{split} é›†åˆ†æ:")
        print(f"  å®é™…è§†é¢‘æ–‡ä»¶: {len(actual_videos)} ä¸ª")
        print(f"  Corpusè®°å½•: {len(corpus_ids)} ä¸ª")
        print(f"  é¢„å¤„ç†æ–‡ä»¶: {len(list((processed_root / split).glob('*_frames.npy')))} ä¸ª")
        
        if len(actual_videos) > len(corpus_ids):
            print(f"  âš ï¸  é—®é¢˜: å®é™…è§†é¢‘æ•°é‡({len(actual_videos)}) > Corpusè®°å½•({len(corpus_ids)})")
            print(f"      å¯èƒ½éœ€è¦é‡æ–°ç”Ÿæˆcorpusæ–‡ä»¶æˆ–é‡æ–°é¢„å¤„ç†")
        
        # æ˜¾ç¤ºè§†é¢‘æ–‡ä»¶æ˜ å°„ç¤ºä¾‹
        if actual_videos:
            print(f"  è§†é¢‘æ–‡ä»¶æ˜ å°„ç¤ºä¾‹:")
            for i, video in enumerate(actual_videos[:5]):
                print(f"    {video['expected_id']} <- {video['translator']}/{video['file_name']}")

if __name__ == "__main__":
    analyze_full_dataset()
