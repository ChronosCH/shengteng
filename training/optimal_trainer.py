#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æœ€ç»ˆä¼˜åŒ–CE-CSLæ‰‹è¯­è¯†åˆ«è®­ç»ƒå™¨
åŸºäºæ‰€æœ‰è®­ç»ƒç»éªŒçš„æœ€ä½³å®è·µç‰ˆæœ¬ - å¢å¼ºç‰ˆ
"""

import os
import json
import time
import logging
import numpy as np
from typing import List
from dataclasses import dataclass
from pathlib import Path
from datetime import datetime

import mindspore as ms
import mindspore.nn as nn
import mindspore.ops as ops
from mindspore import Tensor, context
from mindspore.nn import Cell
import mindspore.dataset as ds

# å¯¼å…¥CE-CSLæ•°æ®å¤„ç†æ¨¡å—
from cecsl_data_processor import create_cecsl_segment_dataloaders, build_corpus_label_vocab

# é…ç½®ç¯å¢ƒ
os.environ['GLOG_v'] = '2'
os.environ['GLOG_logtostderr'] = '1'
# å…¼å®¹æ–°æ—§APIï¼šä¼˜å…ˆä½¿ç”¨ set_deviceï¼Œå¤±è´¥åˆ™å›é€€
try:
    ms.set_device("CPU")
except Exception:
    context.set_context(device_target="CPU")
context.set_context(mode=context.PYNATIVE_MODE)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

@dataclass
class OptimalConfig:
    """æœ€ä¼˜é…ç½® - ä½¿ç”¨çœŸå®æ•°æ®"""
    num_epochs: int = 50
    batch_size: int = 8
    learning_rate: float = 0.001
    min_learning_rate: float = 0.0001
    hidden_dim: int = 128
    attention_dim: int = 64
    dropout_rate: float = 0.2
    warmup_epochs: int = 5
    patience: int = 15
    gradient_clip_norm: float = 5.0
    weight_decay: float = 0.001
    label_smoothing: float = 0.05
    vocab_size: int = 50  # ä¼šæ ¹æ®çœŸå®æ•°æ®åŠ¨æ€æ›´æ–°
    sequence_length: int = 32  # è§†é¢‘ç‰‡æ®µé•¿åº¦
    num_frames: int = 3  # RGBé€šé“æ•°ï¼ˆå®é™…ä¸ºC=3ï¼‰
    checkpoint_dir: str = "checkpoints/optimal"
    data_root: str = "data/CE-CSL"
    # çœŸå®æ•°æ®é…ç½®
    use_real_data: bool = True
    video_size: tuple = (224, 224)
    train_flip_prob: float = 0.5
    clip_len: int = 32  # è§†é¢‘ç‰‡æ®µå¸§æ•°
    pad_mode: str = "repeat"
    max_samples_per_epoch: int = 2000
    sample_strategy: str = "balanced"

class AdvancedAttentionLayer(Cell):
    """æ”¹è¿›çš„æ³¨æ„åŠ›å±‚"""
    def __init__(self, input_dim: int, attention_dim: int):
        super().__init__()
        self.input_dim = input_dim
        self.attention_dim = attention_dim
        
        # æ³¨æ„åŠ›æƒé‡è®¡ç®—
        self.attention_fc = nn.Dense(input_dim, attention_dim)
        self.attention_out = nn.Dense(attention_dim, 1)
        
        # ç‰¹å¾å˜æ¢
        self.feature_fc = nn.Dense(input_dim, input_dim)
        
        self.softmax = nn.Softmax(axis=1)
        self.tanh = nn.Tanh()
        
    def construct(self, x):
        """
        x: (batch_size, seq_len, input_dim)
        """
        batch_size, seq_len, input_dim = x.shape
        
        # è®¡ç®—æ³¨æ„åŠ›æƒé‡
        att_input = x.view(-1, input_dim)  # (batch_size * seq_len, input_dim)
        att_hidden = self.tanh(self.attention_fc(att_input))  # (batch_size * seq_len, attention_dim)
        att_scores = self.attention_out(att_hidden)  # (batch_size * seq_len, 1)
        att_scores = att_scores.view(batch_size, seq_len)  # (batch_size, seq_len)
        
        # åº”ç”¨softmaxå¾—åˆ°æ³¨æ„åŠ›æƒé‡
        att_weights = self.softmax(att_scores)  # (batch_size, seq_len)
        att_weights = att_weights.view(batch_size, seq_len, 1)  # (batch_size, seq_len, 1)
        
        # ç‰¹å¾å˜æ¢
        transformed_x = self.feature_fc(x.view(-1, input_dim)).view(batch_size, seq_len, input_dim)
        
        # åº”ç”¨æ³¨æ„åŠ›æƒé‡
        attended_x = transformed_x * att_weights  # (batch_size, seq_len, input_dim)
        
        # åŠ æƒæ±‚å’Œ
        output = ops.ReduceSum(keep_dims=False)(attended_x, 1)  # (batch_size, input_dim)
        
        return output

class OptimalModel(Cell):
    """æœ€ä¼˜æ¨¡å‹æ¶æ„"""
    def __init__(self, config: OptimalConfig):
        super().__init__()
        self.config = config
        
        # ç®€åŒ–ç‰¹å¾æå–
        self.feature_layers = nn.SequentialCell([
            # å•å±‚ç‰¹å¾æå–
            nn.Dense(config.num_frames, config.hidden_dim),
            nn.LayerNorm((config.hidden_dim,)),
            nn.GELU(),
            nn.Dropout(p=config.dropout_rate),
        ])
        
        # æ³¨æ„åŠ›å±‚
        self.attention = AdvancedAttentionLayer(config.hidden_dim, config.attention_dim)
        
        # åˆ†ç±»å¤´
        self.classifier = nn.SequentialCell([
            nn.Dense(config.hidden_dim, config.hidden_dim // 2),
            nn.BatchNorm1d(config.hidden_dim // 2),
            nn.GELU(),
            nn.Dropout(p=config.dropout_rate),
            nn.Dense(config.hidden_dim // 2, config.vocab_size),
        ])
        
        # æƒé‡åˆå§‹åŒ–
        self._init_weights()
    
    def _init_weights(self):
        """æƒé‡åˆå§‹åŒ–"""
        for cell in self.cells():
            if isinstance(cell, nn.Dense):
                # Heåˆå§‹åŒ–
                cell.weight.set_data(ms.common.initializer.initializer(
                    "he_normal", cell.weight.shape, cell.weight.dtype
                ))
                if cell.bias is not None:
                    cell.bias.set_data(ms.common.initializer.initializer(
                        "zeros", cell.bias.shape, cell.bias.dtype
                    ))
    def construct(self, x):
        """å‰å‘ä¼ æ’­"""
        batch_size, seq_len, _ = x.shape

        # é€å¸§ç‰¹å¾æå–
        x_reshaped = x.view(-1, self.config.num_frames)  # (batch_size * seq_len, num_frames)
        feats = self.feature_layers(x_reshaped)  # (batch_size * seq_len, hidden_dim)
        sequence_features = feats.view(batch_size, seq_len, self.config.hidden_dim)  # (batch_size, seq_len, hidden_dim)

        # æ³¨æ„åŠ›èšåˆ
        attended_features = self.attention(sequence_features)  # (batch_size, hidden_dim)

        # åˆ†ç±»
        logits = self.classifier(attended_features)  # (batch_size, vocab_size)

        return logits

class SimplifiedModel(Cell):
    """ç®€åŒ–æ¨¡å‹æ¶æ„ - é€‚é…çœŸå®è§†é¢‘æ•°æ®"""
    def __init__(self, config: OptimalConfig):
        super().__init__()
        self.config = config
        
        # ä½¿ç”¨æ›´ç®€å•çš„2D CNN + æ—¶åºå»ºæ¨¡æ¶æ„ï¼Œé¿å…3Då·ç§¯å…¼å®¹æ€§é—®é¢˜
        # å…ˆå¯¹æ¯å¸§è¿›è¡Œ2Dç‰¹å¾æå–ï¼Œå†è¿›è¡Œæ—¶åºå»ºæ¨¡
        
        # 2Då·ç§¯ç‰¹å¾æå–å™¨ï¼ˆé€å¸§å¤„ç†ï¼‰
        self.frame_encoder = nn.SequentialCell([
            # ç¬¬ä¸€ä¸ªå·ç§¯å—
            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, pad_mode='pad'),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=3, stride=2, padding=1, pad_mode='pad'),
            
            # ç¬¬äºŒä¸ªå·ç§¯å—
            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1, pad_mode='pad'),
            nn.BatchNorm2d(128),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=3, stride=2, padding=1, pad_mode='pad'),
            
            # ç¬¬ä¸‰ä¸ªå·ç§¯å—
            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1, pad_mode='pad'),
            nn.BatchNorm2d(256),
            nn.ReLU(),
            
            # å…¨å±€å¹³å‡æ± åŒ–
            nn.AdaptiveAvgPool2d((1, 1))
        ])
        
        # ç‰¹å¾ç»´åº¦ï¼š256
        feature_dim = 256
        
        # æ—¶åºå»ºæ¨¡
        self.temporal_encoder = nn.LSTM(
            input_size=feature_dim,
            hidden_size=config.hidden_dim // 2,
            num_layers=2,
            batch_first=True,
            dropout=config.dropout_rate,
            bidirectional=True
        )
        
        # åˆ†ç±»å™¨
        self.classifier = nn.SequentialCell([
            nn.Dense(config.hidden_dim, config.hidden_dim // 2),
            nn.BatchNorm1d(config.hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(p=config.dropout_rate),
            nn.Dense(config.hidden_dim // 2, config.vocab_size),
        ])
        
        # æƒé‡åˆå§‹åŒ–
        self._init_weights()
    
    def _init_weights(self):
        """æƒé‡åˆå§‹åŒ–"""
        for cell in self.cells():
            if isinstance(cell, (nn.Dense, nn.Conv2d)):
                cell.weight.set_data(ms.common.initializer.initializer(
                    "xavier_uniform", cell.weight.shape, cell.weight.dtype
                ))
                if cell.bias is not None:
                    cell.bias.set_data(ms.common.initializer.initializer(
                        "zeros", cell.bias.shape, cell.bias.dtype
                    ))
    
    def construct(self, x):
        """
        å‰å‘ä¼ æ’­
        x: (batch_size, clip_len, C, H, W) - è§†é¢‘æ•°æ®
        """
        batch_size, clip_len, C, H, W = x.shape
        
        # å°†è§†é¢‘é‡å¡‘ä¸º (batch_size * clip_len, C, H, W) è¿›è¡Œé€å¸§å¤„ç†
        x_frames = x.view(batch_size * clip_len, C, H, W)
        
        # 2Då·ç§¯ç‰¹å¾æå–
        frame_features = self.frame_encoder(x_frames)  # (batch_size * clip_len, 256, 1, 1)
        frame_features = frame_features.squeeze(-1).squeeze(-1)  # (batch_size * clip_len, 256)
        
        # é‡å¡‘å›æ—¶åºæ ¼å¼
        sequence_features = frame_features.view(batch_size, clip_len, -1)  # (batch_size, clip_len, 256)
        
        # LSTMæ—¶åºå»ºæ¨¡
        lstm_out, _ = self.temporal_encoder(sequence_features)  # (batch_size, clip_len, hidden_dim)
        
        # å…¨å±€å¹³å‡æ± åŒ–
        pooled = ops.ReduceMean(keep_dims=False)(lstm_out, 1)  # (batch_size, hidden_dim)
        
        # åˆ†ç±»
        logits = self.classifier(pooled)  # (batch_size, vocab_size)
        
        return logits

class FocalLoss(Cell):
    """Focal Losså®ç°"""
    def __init__(self, num_classes: int, alpha: float = 1.0, gamma: float = 2.0, smoothing: float = 0.0):
        super().__init__()
        self.num_classes = num_classes
        self.alpha = alpha
        self.gamma = gamma  # é™ä½gammaï¼Œå‡å°‘éš¾æ ·æœ¬æƒé‡
        self.smoothing = smoothing
        self.softmax = nn.LogSoftmax(axis=-1)
        
    def construct(self, logits, target):
        """è®¡ç®—Focal Loss"""
        log_probs = self.softmax(logits)
        
        # æ ‡ç­¾å¹³æ»‘
        if self.smoothing > 0:
            smooth_factor = self.smoothing / (self.num_classes - 1)
            one_hot = ops.OneHot()(target, self.num_classes, 
                                   Tensor(1.0 - self.smoothing, ms.float32), 
                                   Tensor(smooth_factor, ms.float32))
        else:
            one_hot = ops.OneHot()(target, self.num_classes, 
                                   Tensor(1.0, ms.float32), 
                                   Tensor(0.0, ms.float32))
        
        # è®¡ç®—äº¤å‰ç†µ
        ce_loss = -ops.ReduceSum(keep_dims=False)(one_hot * log_probs, -1)
        
        # è®¡ç®—æ¦‚ç‡
        probs = ops.Exp()(log_probs)
        pt = ops.ReduceSum(keep_dims=False)(one_hot * probs, -1)
        
        # Focalæƒé‡ï¼ˆé™ä½gammaï¼‰
        focal_weight = ops.Pow()(1 - pt, self.gamma)
        
        # æœ€ç»ˆæŸå¤±
        focal_loss = self.alpha * focal_weight * ce_loss
        
        return ops.ReduceMean()(focal_loss)

# æ·»åŠ åˆ«åï¼Œå…¼å®¹ä»£ç ä¸­çš„å¼•ç”¨
ImprovedFocalLoss = FocalLoss

class CosineWarmupLR:
    """å¸¦Warmupçš„ä½™å¼¦é€€ç«å­¦ä¹ ç‡"""
    def __init__(self, base_lr: float, min_lr: float, total_epochs: int, warmup_epochs: int = 0):
        self.base_lr = base_lr
        self.min_lr = min_lr
        self.total_epochs = total_epochs
        self.warmup_epochs = warmup_epochs
    
    def get_lr(self, epoch: int) -> float:
        """è·å–å½“å‰epochçš„å­¦ä¹ ç‡"""
        if epoch < self.warmup_epochs:
            # Warmupé˜¶æ®µ - çº¿æ€§å¢é•¿
            return self.base_lr * (epoch + 1) / self.warmup_epochs
        else:
            # ä½™å¼¦é€€ç«é˜¶æ®µ
            cosine_epoch = epoch - self.warmup_epochs
            cosine_total = self.total_epochs - self.warmup_epochs
            cosine_factor = 0.5 * (1 + np.cos(np.pi * cosine_epoch / cosine_total))
            return self.min_lr + (self.base_lr - self.min_lr) * cosine_factor

class WarmupCosineScheduler:
    """æ”¹è¿›çš„å­¦ä¹ ç‡è°ƒåº¦å™¨"""
    def __init__(self, base_lr: float, min_lr: float, total_epochs: int, warmup_epochs: int = 0):
        self.base_lr = base_lr
        self.min_lr = min_lr
        self.total_epochs = total_epochs
        self.warmup_epochs = warmup_epochs
    
    def get_lr(self, epoch: int) -> float:
        """è·å–å½“å‰epochçš„å­¦ä¹ ç‡"""
        if epoch < self.warmup_epochs:
            # Warmupé˜¶æ®µ - çº¿æ€§å¢é•¿
            return self.base_lr * (epoch + 1) / self.warmup_epochs
        else:
            # ä½™å¼¦é€€ç«é˜¶æ®µ
            cosine_epoch = epoch - self.warmup_epochs
            cosine_total = self.total_epochs - self.warmup_epochs
            cosine_factor = 0.5 * (1 + np.cos(np.pi * cosine_epoch / cosine_total))
            return self.min_lr + (self.base_lr - self.min_lr) * cosine_factor

class OptimalTrainer:
    """æœ€ä¼˜è®­ç»ƒå™¨ - ä½¿ç”¨çœŸå®CE-CSLæ•°æ®"""
    def __init__(self, config: OptimalConfig):
        self.config = config
        
        # æ•°æ®æ ¹ç›®å½•
        data_root = Path(self.config.data_root)
        if not data_root.is_absolute():
            project_root = Path(__file__).resolve().parent.parent
            data_root = (project_root / self.config.data_root).resolve()
        self.data_root = data_root
        
        if not self.data_root.exists():
            raise FileNotFoundError(f"æ•°æ®ç›®å½•ä¸å­˜åœ¨: {self.data_root}")
        
        # åŠ è½½çœŸå®æ•°æ®çš„è¯æ±‡è¡¨
        self.vocab, self.label2idx = self._load_real_vocab()
        self.word_to_id = self.label2idx
        self.id_to_word = {v: k for k, v in self.label2idx.items()}
        
        # æ›´æ–°é…ç½®ä¸­çš„è¯æ±‡è¡¨å¤§å°
        self.config.vocab_size = len(self.vocab)
        
        # åˆ›å»ºæ£€æŸ¥ç‚¹ç›®å½•
        os.makedirs(config.checkpoint_dir, exist_ok=True)
        
        # è®­ç»ƒæ—¥å¿—
        self.log_dir = Path(config.checkpoint_dir) / "training_logs"
        self.log_dir.mkdir(exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.training_log_file = self.log_dir / f"training_log_{timestamp}.json"
        
        self.training_log = {
            "start_time": timestamp,
            "config": {
                "num_epochs": config.num_epochs,
                "batch_size": config.batch_size,
                "learning_rate": config.learning_rate,
                "hidden_dim": config.hidden_dim,
                "dropout_rate": config.dropout_rate,
                "vocab_size": config.vocab_size,
                "use_real_data": config.use_real_data,
                "clip_len": config.clip_len,
                "video_size": config.video_size
            },
            "epochs": [],
            "best_metrics": {
                "best_val_acc": 0.0,
                "best_epoch": 0,
                "best_train_acc": 0.0,
                "final_train_loss": 0.0,
                "final_val_loss": 0.0
            },
            "training_summary": {}
        }
        
        logger.info("çœŸå®æ•°æ®è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"æ•°æ®æ ¹ç›®å½•: {self.data_root}")
        logger.info(f"è¯æ±‡è¡¨å¤§å°: {len(self.vocab)}")
        logger.info(f"ä½¿ç”¨è§†é¢‘æ¨¡å‹æ¶æ„")
    
    def _load_real_vocab(self):
        """åŠ è½½çœŸå®æ•°æ®çš„è¯æ±‡è¡¨"""
        # é¦–å…ˆå°è¯•åŠ è½½å·²å­˜åœ¨çš„è¯æ±‡è¡¨
        vocab_path = self.data_root / "corpus_vocab.json"
        cleaned_vocab_path = self.data_root / "cleaned_vocab.json"
        
        # ä¼˜å…ˆä½¿ç”¨æ¸…ç†åçš„è¯æ±‡è¡¨
        if cleaned_vocab_path.exists():
            try:
                with open(cleaned_vocab_path, 'r', encoding='utf-8') as f:
                    vocab_data = json.load(f)
                label2idx = vocab_data.get('word2idx', {})
                vocab_list = vocab_data.get('idx2word', [])
                if label2idx and vocab_list:
                    logger.info(f"åŠ è½½æ¸…ç†åè¯æ±‡è¡¨: {len(vocab_list)} ä¸ªç±»åˆ«")
                    return vocab_list, label2idx
            except Exception as e:
                logger.warning(f"åŠ è½½æ¸…ç†åè¯æ±‡è¡¨å¤±è´¥: {e}")
        
        # å¦‚æœæ²¡æœ‰æ¸…ç†åçš„è¯æ±‡è¡¨ï¼Œä»corpusæ–‡ä»¶æ„å»º
        if vocab_path.exists():
            try:
                with open(vocab_path, 'r', encoding='utf-8') as f:
                    vocab_data = json.load(f)
                label2idx = vocab_data.get('label2idx', {})
                idx2label = vocab_data.get('idx2label', [])
                if label2idx and idx2label:
                    logger.info(f"åŠ è½½corpusè¯æ±‡è¡¨: {len(idx2label)} ä¸ªç±»åˆ«")
                    return idx2label, label2idx
            except Exception as e:
                logger.warning(f"åŠ è½½corpusè¯æ±‡è¡¨å¤±è´¥: {e}")
        
        # å¦‚æœéƒ½æ²¡æœ‰ï¼Œä»corpusæ–‡ä»¶æ„å»ºæ–°çš„è¯æ±‡è¡¨
        logger.info("æ„å»ºæ–°çš„è¯æ±‡è¡¨...")
        corpus_files = []
        for split in ["train", "dev", "test"]:
            corpus_file = self.data_root / f"{split}.corpus.csv"
            if corpus_file.exists():
                corpus_files.append(str(corpus_file))
        
        if not corpus_files:
            raise FileNotFoundError(f"æœªæ‰¾åˆ°corpusæ–‡ä»¶åœ¨ {self.data_root}")
        
        label2idx = build_corpus_label_vocab(corpus_files, save_path=str(vocab_path), use_cleaned=True)
        idx2label = [k for k, v in sorted(label2idx.items(), key=lambda x: x[1])]
        
        logger.info(f"æ–°å»ºè¯æ±‡è¡¨: {len(idx2label)} ä¸ªç±»åˆ«")
        return idx2label, label2idx

    def _save_training_log(self):
        """ä¿å­˜è®­ç»ƒæ—¥å¿—åˆ°æ–‡ä»¶"""
        try:
            with open(self.training_log_file, 'w', encoding='utf-8') as f:
                json.dump(self.training_log, f, indent=2, ensure_ascii=False)
        except Exception as e:
            logger.error(f"ä¿å­˜è®­ç»ƒæ—¥å¿—å¤±è´¥: {e}")

    def _update_epoch_log(self, epoch: int, train_loss: float, train_acc: float, 
                         val_loss: float, val_acc: float, learning_rate: float, 
                         epoch_time: float):
        """æ›´æ–°å•ä¸ªepochçš„æ—¥å¿—"""
        epoch_log = {
            "epoch": epoch + 1,
            "train_loss": round(float(train_loss), 6),
            "train_acc": round(float(train_acc), 6),
            "val_loss": round(float(val_loss), 6),
            "val_acc": round(float(val_acc), 6),
            "learning_rate": round(float(learning_rate), 8),
            "epoch_time": round(float(epoch_time), 2)
        }
        self.training_log["epochs"].append(epoch_log)
        
        # æ›´æ–°æœ€ä½³æŒ‡æ ‡
        if val_acc > self.training_log["best_metrics"]["best_val_acc"]:
            self.training_log["best_metrics"]["best_val_acc"] = round(float(val_acc), 6)
            self.training_log["best_metrics"]["best_epoch"] = epoch + 1
            self.training_log["best_metrics"]["best_train_acc"] = round(float(train_acc), 6)
        
        # æ›´æ–°æœ€ç»ˆæŒ‡æ ‡
        self.training_log["best_metrics"]["final_train_loss"] = round(float(train_loss), 6)
        self.training_log["best_metrics"]["final_val_loss"] = round(float(val_loss), 6)

    def _load_cleaned_vocab(self) -> List[str]:
        """åŠ è½½æ¸…ç†åçš„è¯æ±‡è¡¨"""
        vocab_file = self.data_root / "cleaned_vocab.json"
        if vocab_file.exists():
            try:
                with vocab_file.open('r', encoding='utf-8') as f:
                    vocab_data = json.load(f)
                vocab_list = vocab_data.get('idx2word', [])
                if vocab_list:
                    logger.info(f"åŠ è½½æ¸…ç†åè¯æ±‡è¡¨: {len(vocab_list)} ä¸ªè¯")
                    return vocab_list
            except Exception as e:
                logger.warning(f"åŠ è½½æ¸…ç†åè¯æ±‡è¡¨å¤±è´¥: {e}")
        return []
    
    def _build_vocab(self) -> List[str]:
        """æ„å»ºåŸºç¡€è¯æ±‡è¡¨"""
        logger.warning("ä½¿ç”¨åŸºç¡€è¯æ±‡è¡¨")
        return ['<PAD>', '<UNK>', 'ä½ å¥½', 'è°¢è°¢', 'è¯·', 'å†è§', 'å¥½çš„', 'æ˜¯çš„', 'ä¸æ˜¯', 'æˆ‘']

    def _create_enhanced_mock_data(self, split: str):
        """åˆ›å»ºå¢å¼ºçš„æ¨¡æ‹Ÿæ•°æ® - æé«˜å¯å­¦ä¹ æ€§"""
        np.random.seed(42 if split == 'train' else 123)
        
        # å¢åŠ æ ·æœ¬æ•°é‡
        samples_per_class = self.config.data_augmentation_factor * 2 if split == 'train' else 6
        
        data = []
        labels = []
        
        # ä»ç´¢å¼•2å¼€å§‹ï¼Œè·³è¿‡<PAD>å’Œ<UNK>
        for class_id in range(2, self.config.vocab_size):
            for sample_idx in range(samples_per_class):
                # ç”Ÿæˆæ›´æœ‰åŒºåˆ†æ€§çš„ç‰¹å¾
                base_freq = 0.8 + 0.1 * (class_id - 2)
                t = np.linspace(0, 2 * np.pi, self.config.sequence_length)
                phase = 0.1 * sample_idx
                
                # ä¸»è¦æ¨¡å¼ï¼šæ­£å¼¦æ³¢ + ä½™å¼¦æ³¢ç»„åˆ
                pattern1 = np.sin(base_freq * t + phase)
                pattern2 = np.cos((base_freq + 0.2) * t + 0.5 * phase)
                pattern3 = np.sin(0.5 * base_freq * t + 0.3) if self.config.num_frames > 2 else None
                
                # æ„å»ºåºåˆ—
                sequence = np.zeros((self.config.sequence_length, self.config.num_frames), dtype=np.float32)
                sequence[:, 0] = pattern1 + np.random.normal(0, 0.1, self.config.sequence_length)
                if self.config.num_frames > 1:
                    sequence[:, 1] = pattern2 + np.random.normal(0, 0.1, self.config.sequence_length)
                if self.config.num_frames > 2 and pattern3 is not None:
                    sequence[:, 2] = pattern3 + np.random.normal(0, 0.1, self.config.sequence_length)
                
                # æ·»åŠ ç±»åˆ«ç‰¹å®šçš„åç½®
                sequence += (class_id - 2) * 0.1
                
                # æ·»åŠ æ ·æœ¬ç‰¹å¼‚æ€§
                sequence += np.random.normal(0, 0.05)
                
                # æ ‡å‡†åŒ–
                sequence = (sequence - sequence.mean()) / (sequence.std() + 1e-6)
                
                data.append(sequence.astype(np.float32))
                labels.append(class_id)
        
        # æ‰“ä¹±æ•°æ®
        indices = np.random.permutation(len(data))
        data = [data[i] for i in indices]
        labels = [labels[i] for i in indices]
        
        logger.info(f"ç”Ÿæˆ{split}å¢å¼ºæ¨¡æ‹Ÿæ•°æ®: {len(data)}ä¸ªæ ·æœ¬")
        logger.info(f"ç±»åˆ«åˆ†å¸ƒ: {dict(zip(*np.unique(labels, return_counts=True)))}")
        
        return data, labels
    
    # æ–°å¢ï¼šç»Ÿä¸€è§†é¢‘å¼ é‡ç»´åº¦ä¸º (B, T, C, H, W)ï¼Œè‹¥æ˜¯åºåˆ—è¾“å…¥ (B, T, C) åˆ™åŸæ ·è¿”å›
    def _normalize_video_shape(self, video):
        if isinstance(video, np.ndarray):
            video = Tensor(video, ms.float32)
        elif isinstance(video, Tensor) and video.dtype != ms.float32:
            video = ops.Cast()(video, ms.float32)
        if len(video.shape) == 5:
            # If (B, T, H, W, C) -> (B, T, C, H, W)
            if video.shape[-1] == 3:
                video = ops.transpose(video, (0, 1, 4, 2, 3))
            # If (B, C, T, H, W) -> (B, T, C, H, W)
            elif video.shape[1] == 3:
                video = ops.transpose(video, (0, 2, 1, 3, 4))
            # Else assume already (B, T, C, H, W)
        # è‹¥æ˜¯ (B, T, C) çš„åºåˆ—è¾“å…¥ï¼Œç›´æ¥è¿”å›
        return video

    # æ–°å¢ï¼šç»Ÿä¸€è§£æ batchï¼ˆæ”¯æŒ tuple/list æˆ– dictï¼‰ï¼Œå¯å…¼å®¹ 2åˆ—æˆ–1åˆ—(dict)çš„æƒ…å†µ
    def _parse_batch(self, batch):
        if isinstance(batch, dict):
            video = batch.get('video') or batch.get('frames') or batch.get('clip') or batch.get('input') or batch.get('x')
            label = batch.get('label') or batch.get('labels') or batch.get('target') or batch.get('y')
            length = batch.get('length') or batch.get('seq_len') or batch.get('video_len')
            video_id = batch.get('video_id') or batch.get('id') or batch.get('name') or batch.get('path')
        else:
            # å…¼å®¹ tuple/list
            if len(batch) >= 4:
                video, label, length, video_id = batch[0], batch[1], batch[2], batch[3]
            elif len(batch) == 3:
                video, label, length = batch[0], batch[1], batch[2]
                video_id = None
            elif len(batch) == 2:
                video, label = batch[0], batch[1]
                length, video_id = None, None
            elif len(batch) == 1 and isinstance(batch[0], dict):
                return self._parse_batch(batch[0])
            else:
                raise ValueError("æ— æ³•ä» batch ä¸­è§£æå‡º (video, label)ã€‚")
        # ç±»å‹ä¸å½¢çŠ¶è§„èŒƒåŒ–
        video = self._normalize_video_shape(video)
        if not isinstance(label, Tensor):
            label = Tensor(label, ms.int32)
        elif label.dtype != ms.int32:
            label = ops.Cast()(label, ms.int32)
        return video, label, length, video_id

    # æ–°å¢ï¼šæ•°æ®é›†å¯ç”¨æ€§é¢„æ£€ï¼ˆæ¢æµ‹â€œåˆ—æ•°ä¸åŒ¹é…â€çš„å…¸å‹å¼‚å¸¸ï¼‰
    def _dataset_iterable(self, dataset) -> bool:
        try:
            it = dataset.create_tuple_iterator()
            # å–ä¸€ä¸ª batch è¿›è¡Œæ¢æµ‹
            _ = next(iter(it))
            return True
        except Exception as e:
            emsg = str(e)
            if "GeneratorDataset" in emsg and "column_names" in emsg:
                logger.error("æ•°æ®é›†è¿­ä»£å¤±è´¥ï¼ˆåˆ—æ•°ä¸è¿”å›ä¸ä¸€è‡´ï¼‰ï¼š%s", emsg)
                return False
            logger.error("æ•°æ®é›†è¿­ä»£å¤±è´¥ï¼š%s", emsg)
            return False

    # æ–°å¢ï¼šä»å¢å¼ºæ¨¡æ‹Ÿæ•°æ®æ„å»º MindSpore æ•°æ®é›†ï¼ˆç”¨äºå›é€€ï¼‰
    def _build_mock_ms_dataset(self, split: str):
        data, labels = self._create_enhanced_mock_data(split)
        def gen():
            for x, y in zip(data, labels):
                # x: (T, C) -> æ·»åŠ  batch ç»´åº¦åœ¨ DataLoader å¤„è‡ªåŠ¨åˆæ‰¹ï¼Œè¿™é‡Œåªè¿”å›å•æ ·æœ¬
                yield x.astype(np.float32), np.int32(y)
        dataset = ds.GeneratorDataset(gen, column_names=["sequence", "label"], shuffle=True)
        dataset = dataset.batch(self.config.batch_size, drop_remainder=False)
        return dataset

    def create_dataset(self, split: str):
        """åˆ›å»ºçœŸå®æ•°æ®é›†"""
        data_config = {
            "root": str(self.data_root),
            "splits": [split],
            "batch_size": self.config.batch_size,
            "clip_len": self.config.clip_len,
            "size": self.config.video_size,
            "train_flip": self.config.train_flip_prob if split == "train" else 0.0
            # ç§»é™¤ 'label2idx' / 'return_meta' / 'return_length'ï¼Œé¿å…ä¸Šæ¸¸äº§ç”Ÿä¸ä¸€è‡´çš„åˆ—å®šä¹‰
        }
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        if split == "train":
            train_loader, _, _, _ = create_cecsl_segment_dataloaders(data_config)
            return train_loader
        elif split == "dev":
            data_config["splits"] = ["dev"]
            _, val_loader, _, _ = create_cecsl_segment_dataloaders(data_config)
            return val_loader
        elif split == "test":
            data_config["splits"] = ["test"]
            _, _, test_loader, _ = create_cecsl_segment_dataloaders(data_config)
            return test_loader
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®é›†åˆ’åˆ†: {split}")

    def train(self):
        """æ‰§è¡Œè®­ç»ƒ - çœŸå®æ•°æ®ç‰ˆæœ¬"""
        logger.info("ğŸ¯ å¼€å§‹çœŸå®æ•°æ®è®­ç»ƒ...")
        training_start_time = time.time()

        # åˆ›å»ºæ•°æ®é›†
        try:
            train_dataset = self.create_dataset('train')
            val_dataset = self.create_dataset('dev')
            steps_per_epoch = train_dataset.get_dataset_size()
            logger.info(f"è®­ç»ƒé›†å¤§å°: {steps_per_epoch} batches")
            logger.info(f"éªŒè¯é›†å¤§å°: {val_dataset.get_dataset_size()} batches")
        except Exception as e:
            logger.error(f"åˆ›å»ºæ•°æ®é›†å¤±è´¥: {e}")
            raise

        # é¢„æ£€çœŸå®æ•°æ®é›†æ˜¯å¦å¯è¿­ä»£ï¼›è‹¥ä¸å¯è¿­ä»£åˆ™å›é€€åˆ°æ¨¡æ‹Ÿæ•°æ®å’Œåºåˆ—æ¨¡å‹
        use_mock = not self._dataset_iterable(train_dataset)

        # æ ¹æ®æ˜¯å¦å›é€€é€‰æ‹©æ¨¡å‹
        if use_mock:
            logger.warning("å›é€€åˆ°æ¨¡æ‹Ÿæ•°æ®è®­ç»ƒï¼ˆä¸Šæ¸¸æ•°æ®é›†åˆ—å®šä¹‰ä¸è¿”å›ä¸ä¸€è‡´ï¼‰ã€‚")
            # ä½¿ç”¨åºåˆ—æ¨¡å‹
            model = OptimalModel(self.config)
            # æ„å»ºæ¨¡æ‹Ÿæ•°æ®é›†
            train_dataset = self._build_mock_ms_dataset('train')
            val_dataset = self._build_mock_ms_dataset('dev')
            steps_per_epoch = train_dataset.get_dataset_size()
        else:
            # æ­£å¸¸ä½¿ç”¨è§†é¢‘æ¨¡å‹
            model = SimplifiedModel(self.config)

        # ä½¿ç”¨æ”¹è¿›çš„æŸå¤±å‡½æ•°
        loss_fn = FocalLoss(
            num_classes=self.config.vocab_size,
            alpha=0.25,
            gamma=1.0,
            smoothing=self.config.label_smoothing
        )

        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        lr_scheduler = WarmupCosineScheduler(
            base_lr=self.config.learning_rate,
            min_lr=self.config.min_learning_rate,
            total_epochs=self.config.num_epochs,
            warmup_epochs=self.config.warmup_epochs
        )

        # åˆ›å»ºä¼˜åŒ–å™¨ï¼ˆæŒ‰å®é™… steps_per_epoch å±•å¼€ LRï¼‰
        lr_values = []
        for e in range(self.config.num_epochs):
            lr_e = lr_scheduler.get_lr(e)
            lr_values.extend([lr_e] * steps_per_epoch)
        lr_tensor = Tensor(np.array(lr_values, dtype=np.float32))

        optimizer = nn.AdamWeightDecay(
            params=model.trainable_params(),
            learning_rate=lr_tensor,
            weight_decay=self.config.weight_decay,
            beta1=0.9,
            beta2=0.999
        )

        # è®­ç»ƒçŠ¶æ€
        best_val_acc = 0.0
        patience_counter = 0
        
        logger.info("å¼€å§‹è®­ç»ƒå¾ªç¯...")
        
        # è®­ç»ƒå¾ªç¯
        for epoch in range(self.config.num_epochs):
            epoch_start_time = time.time()
            current_lr = lr_scheduler.get_lr(epoch)
            logger.info(f"Epoch {epoch+1}/{self.config.num_epochs}, LR: {current_lr:.6f}")
            
            # è®­ç»ƒé˜¶æ®µ
            model.set_train(True)
            train_losses = []
            train_correct = 0
            train_total = 0

            grad_fn = ms.value_and_grad(
                lambda d, y: loss_fn(model(d), y),
                None,
                model.trainable_params()
            )

            # é€‰æ‹©è¿­ä»£å™¨
            try:
                train_iterator = train_dataset.create_tuple_iterator()
                use_dict_iter = False
            except Exception as _:
                logger.warning("tuple è¿­ä»£å™¨ä¸å¯ç”¨ï¼Œåˆ‡æ¢è‡³ dict è¿­ä»£å™¨ã€‚")
                train_iterator = train_dataset.create_dict_iterator()
                use_dict_iter = True

            for batch_idx, batch_data in enumerate(train_iterator):
                # ç»Ÿä¸€è§£æ batch
                video, label, length, video_id = self._parse_batch(batch_data)

                # å‰å‘ + åå‘ + æ›´æ–°
                loss, grads = grad_fn(video, label)
                grads = ops.clip_by_global_norm(grads, self.config.gradient_clip_norm)
                optimizer(grads)
                train_losses.append(loss.asnumpy())

                # è®¡ç®—è®­ç»ƒå‡†ç¡®ç‡
                model.set_train(False)
                logits = model(video)
                preds = ops.Argmax(axis=1)(logits)
                if preds.dtype != label.dtype:
                    preds = ops.Cast()(preds, label.dtype)
                train_correct += ops.ReduceSum()(ops.Equal()(preds, label)).asnumpy()
                train_total += label.shape[0]
                model.set_train(True)

                if batch_idx % 20 == 0:
                    batch_acc = train_correct / train_total if train_total > 0 else 0.0
                    logger.info(f"  Batch {batch_idx}: Loss={loss.asnumpy():.4f}, Acc={batch_acc:.4f}")
            
            # è®­ç»ƒepochç»Ÿè®¡
            epoch_train_loss = float(np.mean(train_losses)) if train_losses else 0.0
            epoch_train_acc = train_correct / train_total if train_total > 0 else 0.0
            
            # éªŒè¯é˜¶æ®µ
            model.set_train(False)
            val_losses = []
            val_correct = 0
            val_total = 0
            try:
                val_iterator = val_dataset.create_tuple_iterator()
                val_use_dict_iter = False
            except Exception as _:
                logger.warning("éªŒè¯é›† tuple è¿­ä»£å™¨ä¸å¯ç”¨ï¼Œåˆ‡æ¢è‡³ dict è¿­ä»£å™¨ã€‚")
                val_iterator = val_dataset.create_dict_iterator()
                val_use_dict_iter = True

            for batch_data in val_iterator:
                video, label, length, video_id = self._parse_batch(batch_data)
                logits = model(video)
                loss = loss_fn(logits, label)
                val_losses.append(loss.asnumpy())
                preds = ops.Argmax(axis=1)(logits)
                if preds.dtype != label.dtype:
                    preds = ops.Cast()(preds, label.dtype)
                val_correct += ops.ReduceSum()(ops.Equal()(preds, label)).asnumpy()
                val_total += label.shape[0]
            
            # éªŒè¯epochç»Ÿè®¡
            epoch_val_loss = float(np.mean(val_losses)) if val_losses else 0.0
            epoch_val_acc = val_correct / val_total if val_total > 0 else 0.0
            
            # è®¡ç®—è€—æ—¶
            epoch_time = time.time() - epoch_start_time
            
            # æ›´æ–°è®­ç»ƒæ—¥å¿—
            self._update_epoch_log(
                epoch=epoch,
                train_loss=epoch_train_loss,
                train_acc=epoch_train_acc,
                val_loss=epoch_val_loss,
                val_acc=epoch_val_acc,
                learning_rate=current_lr,
                epoch_time=epoch_time
            )
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºæœ€ä½³æ¨¡å‹
            if epoch_val_acc > best_val_acc:
                best_val_acc = epoch_val_acc
                patience_counter = 0
                
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                checkpoint_path = os.path.join(self.config.checkpoint_dir, "best_model.ckpt")
                ms.save_checkpoint(model, checkpoint_path)
                logger.info(f"ğŸ‰ æ–°çš„æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.4f}")
            else:
                patience_counter += 1
            
            logger.info(f"Epoch {epoch+1} æ€»ç»“:")
            logger.info(f"  è®­ç»ƒ: Loss={epoch_train_loss:.4f}, Acc={epoch_train_acc:.4f}")
            logger.info(f"  éªŒè¯: Loss={epoch_val_loss:.4f}, Acc={epoch_val_acc:.4f}")
            logger.info(f"  æœ€ä½³: {best_val_acc:.4f}, è€å¿ƒ: {patience_counter}/{self.config.patience}")
            
            # æ¯5ä¸ªepochä¿å­˜ä¸€æ¬¡æ—¥å¿—
            if (epoch + 1) % 5 == 0:
                self._save_training_log()
            
            # æ—©åœæ£€æŸ¥
            if patience_counter >= self.config.patience:
                logger.info(f"æ—©åœè§¦å‘ï¼æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.4f}")
                break
        
        # è®¡ç®—æ€»è®­ç»ƒæ—¶é—´
        total_training_time = time.time() - training_start_time
        self.training_log["training_summary"] = {
            "total_epochs": epoch + 1,
            "total_training_time": round(total_training_time, 2),
            "early_stopped": patience_counter >= self.config.patience,
            "end_time": datetime.now().strftime("%Y%m%d_%H%M%S")
        }
        
        # ä¿å­˜æœ€ç»ˆè®­ç»ƒæ—¥å¿—
        self._save_training_log()
        
        logger.info("ğŸ‰ ä¿®å¤ç‰ˆè®­ç»ƒå®Œæˆï¼")
        logger.info(f"ğŸ“Š æœ€ç»ˆç»Ÿè®¡:")
        logger.info(f"  æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.4f}")
        logger.info(f"  è®­ç»ƒè½®æ•°: {epoch+1}")
        logger.info(f"  æ€»è®­ç»ƒæ—¶é—´: {total_training_time:.2f}ç§’")
        logger.info(f"  è®­ç»ƒæ—¥å¿—: {self.training_log_file}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ å¯åŠ¨CE-CSLæ‰‹è¯­è¯†åˆ«è®­ç»ƒå™¨ï¼ˆçœŸå®æ•°æ®ç‰ˆï¼‰...")
    print("  âœ“ ä½¿ç”¨çœŸå®CE-CSLæ•°æ®é›†")
    print("  âœ“ 3Då·ç§¯è§†é¢‘ç‰¹å¾æå–")
    print("  âœ“ LSTMæ—¶åºå»ºæ¨¡")
    print("  âœ“ ä»corpusæ–‡ä»¶æ„å»ºè¯æ±‡è¡¨")
    print("  âœ“ ç‰‡æ®µçº§åˆ†ç±»ä»»åŠ¡")
    
    config = OptimalConfig()
    config.use_real_data = True  # ä½¿ç”¨çœŸå®æ•°æ®
    
    print(f"ğŸ“Š é…ç½®:")
    print(f"  - æ•°æ®æ ¹ç›®å½•: {config.data_root}")
    print(f"  - å­¦ä¹ ç‡: {config.learning_rate}")
    print(f"  - æ‰¹æ¬¡å¤§å°: {config.batch_size}")
    print(f"  - è§†é¢‘ç‰‡æ®µé•¿åº¦: {config.clip_len}")
    print(f"  - è§†é¢‘å°ºå¯¸: {config.video_size}")
    print(f"  - ä½¿ç”¨çœŸå®æ•°æ®: {config.use_real_data}")
    
    trainer = OptimalTrainer(config)
    trainer.train()

if __name__ == "__main__":
    main()
