#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŸºäºçœŸå®CE-CSLæ•°æ®çš„æ‰‹è¯­è¯†åˆ«è®­ç»ƒå™¨
ä½¿ç”¨é¢„å¤„ç†å¥½çš„è§†é¢‘å¸§æ•°æ®è¿›è¡Œè®­ç»ƒ
"""

import os
import sys
import json
import logging
import numpy as np
import mindspore as ms
from mindspore import nn, ops, Tensor, save_checkpoint
from mindspore.dataset import GeneratorDataset
from dataclasses import dataclass
from typing import Dict, List, Tuple, Optional
import time
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
current_dir = Path(__file__).parent
project_root = current_dir.parent
sys.path.append(str(project_root))
sys.path.append(str(current_dir))

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(levelname)s:%(name)s:%(message)s')
logger = logging.getLogger(__name__)

@dataclass
class CECSLTrainingConfig:
    """CE-CSLè®­ç»ƒé…ç½®"""
    # æ¨¡å‹é…ç½®
    vocab_size: int = 1000
    d_model: int = 256
    n_heads: int = 8
    n_layers: int = 4
    dropout: float = 0.1
    
    # è®­ç»ƒé…ç½®
    batch_size: int = 8
    learning_rate: float = 1e-3
    weight_decay: float = 1e-4
    epochs: int = 50
    
    # æ•°æ®é…ç½®
    data_root: str = "../data/CE-CSL"
    max_sequence_length: int = 150  # æœ€å¤§å¸§æ•°
    image_size: Tuple[int, int] = (224, 224)
    
    # è®¾å¤‡é…ç½®
    device_target: str = "CPU"

class CECSLDataset:
    """CE-CSLæ•°æ®é›†åŠ è½½å™¨"""
    
    def __init__(self, config: CECSLTrainingConfig, split: str = 'train'):
        self.config = config
        self.split = split
        self.data_root = Path(config.data_root)
        
        # åŠ è½½è¯æ±‡è¡¨
        self.word2idx = {}
        self.idx2word = []
        self._build_vocabulary()
        
        # åŠ è½½æ•°æ®
        self.samples = []
        self._load_data()
        
        logger.info(f"åŠ è½½ {split} æ•°æ®é›†: {len(self.samples)} ä¸ªæ ·æœ¬")
        logger.info(f"è¯æ±‡è¡¨å¤§å°: {len(self.word2idx)}")
    
    def _build_vocabulary(self):
        """æ„å»ºè¯æ±‡è¡¨"""
        # ä»æ‰€æœ‰CSVæ–‡ä»¶ä¸­æå–æ ‡ç­¾
        all_labels = set()
        
        for split in ['train', 'dev', 'test']:
            csv_file = self.data_root / f"{split}.corpus.csv"
            if csv_file.exists():
                import pandas as pd
                df = pd.read_csv(csv_file)
                all_labels.update(df['label'].unique())
        
        # æ·»åŠ ç‰¹æ®Šæ ‡è®°
        self.word2idx['<PAD>'] = 0
        self.word2idx['<UNK>'] = 1
        self.idx2word = ['<PAD>', '<UNK>']
        
        # æ·»åŠ æ‰€æœ‰æ ‡ç­¾
        for label in sorted(all_labels):
            if label not in self.word2idx:
                self.word2idx[label] = len(self.idx2word)
                self.idx2word.append(label)
        
        logger.info(f"è¯æ±‡è¡¨æ„å»ºå®Œæˆ: {sorted(all_labels)}")
    
    def _load_data(self):
        """åŠ è½½é¢„å¤„ç†æ•°æ®"""
        # åŠ è½½å…ƒæ•°æ®
        metadata_file = self.data_root / "processed" / self.split / f"{self.split}_metadata.json"
        
        if not metadata_file.exists():
            logger.warning(f"å…ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {metadata_file}")
            return
        
        with open(metadata_file, 'r', encoding='utf-8') as f:
            metadata = json.load(f)
        
        for item in metadata:
            video_id = item['video_id']
            label = item['text']  # ä½¿ç”¨textå­—æ®µä½œä¸ºæ ‡ç­¾
            
            # æ£€æŸ¥å¯¹åº”çš„å¸§æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            frames_file = self.data_root / "processed" / self.split / f"{video_id}_frames.npy"
            
            if frames_file.exists():
                label_idx = self.word2idx.get(label, self.word2idx['<UNK>'])
                self.samples.append({
                    'video_id': video_id,
                    'frames_file': str(frames_file),
                    'label': label,
                    'label_idx': label_idx,
                    'metadata': item
                })
    
    def _load_frames(self, frames_file: str) -> np.ndarray:
        """åŠ è½½è§†é¢‘å¸§æ•°æ®"""
        try:
            frames = np.load(frames_file)  # shape: (num_frames, height, width, channels)
            
            # ç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡®
            if frames.dtype != np.float32:
                frames = frames.astype(np.float32)
            
            # å½’ä¸€åŒ–åˆ°[0,1]
            if frames.max() > 1.0:
                frames = frames / 255.0
            
            # è°ƒæ•´åºåˆ—é•¿åº¦
            if len(frames) > self.config.max_sequence_length:
                # å‡åŒ€é‡‡æ ·
                indices = np.linspace(0, len(frames) - 1, self.config.max_sequence_length, dtype=int)
                frames = frames[indices]
            elif len(frames) < self.config.max_sequence_length:
                # å¡«å……
                pad_length = self.config.max_sequence_length - len(frames)
                pad_frames = np.zeros((pad_length,) + frames.shape[1:], dtype=frames.dtype)
                frames = np.concatenate([frames, pad_frames], axis=0)
            
            # è½¬æ¢ä¸º (seq_len, channels, height, width) å¦‚æœéœ€è¦
            if len(frames.shape) == 4 and frames.shape[-1] in [1, 3]:  # (seq, h, w, c)
                frames = np.transpose(frames, (0, 3, 1, 2))  # (seq, c, h, w)
            
            return frames
            
        except Exception as e:
            logger.error(f"åŠ è½½å¸§æ•°æ®å¤±è´¥ {frames_file}: {e}")
            # è¿”å›é›¶å¡«å……çš„æ•°æ®
            return np.zeros((self.config.max_sequence_length, 3, *self.config.image_size), dtype=np.float32)
    
    def __getitem__(self, index):
        """è·å–å•ä¸ªæ ·æœ¬"""
        sample = self.samples[index]
        
        # åŠ è½½å¸§æ•°æ®
        frames = self._load_frames(sample['frames_file'])
        
        # å±•å¹³å¸§æ•°æ®ç”¨äºç®€å•çš„LSTMæ¨¡å‹
        # frames shape: (seq_len, channels, height, width) -> (seq_len, features)
        seq_len = frames.shape[0]
        features = np.prod(frames.shape[1:])  # channels * height * width
        frames_flat = frames.reshape(seq_len, features)
        
        return frames_flat.astype(np.float32), np.array(sample['label_idx'], dtype=np.int32)
    
    def __len__(self):
        return len(self.samples)

class CECSLModel(nn.Cell):
    """CE-CSLæ‰‹è¯­è¯†åˆ«æ¨¡å‹"""
    
    def __init__(self, config: CECSLTrainingConfig, vocab_size: int):
        super().__init__()
        self.config = config
        
        # è®¡ç®—è¾“å…¥ç‰¹å¾ç»´åº¦
        input_size = 3 * config.image_size[0] * config.image_size[1]  # channels * height * width
        
        # ç‰¹å¾æå–å±‚
        self.feature_extractor = nn.SequentialCell([
            nn.Dense(input_size, config.d_model),
            nn.ReLU(),
            nn.Dropout(p=config.dropout),
            nn.Dense(config.d_model, config.d_model),
            nn.ReLU(),
            nn.Dropout(p=config.dropout)
        ])
        
        # æ—¶åºå»ºæ¨¡å±‚
        self.temporal_model = nn.LSTM(
            input_size=config.d_model,
            hidden_size=config.d_model,
            num_layers=config.n_layers,
            batch_first=True,
            dropout=config.dropout if config.n_layers > 1 else 0.0
        )
        
        # åˆ†ç±»å±‚
        self.classifier = nn.SequentialCell([
            nn.Dense(config.d_model, config.d_model // 2),
            nn.ReLU(),
            nn.Dropout(p=config.dropout),
            nn.Dense(config.d_model // 2, vocab_size)
        ])
        
        # æŸå¤±å‡½æ•°
        self.loss_fn = nn.CrossEntropyLoss()
    
    def construct(self, x, labels=None):
        batch_size, seq_len, input_size = x.shape
        
        # ç‰¹å¾æå–
        x_reshaped = x.view(batch_size * seq_len, input_size)
        features = self.feature_extractor(x_reshaped)
        features = features.view(batch_size, seq_len, self.config.d_model)
        
        # æ—¶åºå»ºæ¨¡
        output, _ = self.temporal_model(features)
        
        # å…¨å±€å¹³å‡æ± åŒ–æˆ–å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥
        # è¿™é‡Œä½¿ç”¨å¹³å‡æ± åŒ–æ¥èšåˆæ‰€æœ‰æ—¶é—´æ­¥çš„ä¿¡æ¯
        pooled_output = ops.ReduceMean()(output, axis=1)  # (batch_size, d_model)
        
        # åˆ†ç±»
        logits = self.classifier(pooled_output)
        
        if labels is not None:
            loss = self.loss_fn(logits, labels)
            return loss, logits
        
        return logits

class CECSLTrainer:
    """CE-CSLè®­ç»ƒå™¨"""
    
    def __init__(self, config: CECSLTrainingConfig):
        self.config = config
        self.model = None
        self.optimizer = None
        self.train_dataset = None
        self.val_dataset = None
        
        # è®¾ç½®è®¾å¤‡
        ms.set_context(mode=ms.GRAPH_MODE, device_target=config.device_target)
        
        logger.info(f"CE-CSLè®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ - è®¾å¤‡: {config.device_target}")
    
    def load_data(self):
        """åŠ è½½æ•°æ®"""
        logger.info("åŠ è½½CE-CSLæ•°æ®é›†...")
        
        # åˆ›å»ºæ•°æ®é›†
        train_data = CECSLDataset(self.config, 'train')
        val_data = CECSLDataset(self.config, 'dev')
        
        if len(train_data) == 0:
            raise ValueError("è®­ç»ƒæ•°æ®é›†ä¸ºç©ºï¼Œè¯·æ£€æŸ¥æ•°æ®è·¯å¾„å’Œé¢„å¤„ç†æ•°æ®")
        
        # åˆ›å»ºMindSporeæ•°æ®é›†
        self.train_dataset = GeneratorDataset(
            train_data, 
            column_names=["sequence", "label"],
            shuffle=True
        ).batch(self.config.batch_size)
        
        self.val_dataset = GeneratorDataset(
            val_data, 
            column_names=["sequence", "label"],
            shuffle=False
        ).batch(self.config.batch_size)
        
        # ä¿å­˜è¯æ±‡è¡¨ä¿¡æ¯
        self.vocab_size = len(train_data.word2idx)
        self.word2idx = train_data.word2idx
        self.idx2word = train_data.idx2word
        
        logger.info(f"è®­ç»ƒé›†: {len(train_data)} æ ·æœ¬")
        logger.info(f"éªŒè¯é›†: {len(val_data)} æ ·æœ¬")
        logger.info(f"è¯æ±‡è¡¨å¤§å°: {self.vocab_size}")
        logger.info(f"æ ‡ç­¾ç±»åˆ«: {list(self.word2idx.keys())}")
    
    def build_model(self):
        """æ„å»ºæ¨¡å‹"""
        logger.info("æ„å»ºCE-CSLæ¨¡å‹...")
        
        if not hasattr(self, 'vocab_size'):
            raise ValueError("è¯·å…ˆè°ƒç”¨load_data()åŠ è½½æ•°æ®")
        
        self.model = CECSLModel(self.config, self.vocab_size)
        
        # è®¡ç®—å‚æ•°é‡
        total_params = sum(param.size for param in self.model.get_parameters())
        logger.info(f"æ¨¡å‹æ„å»ºå®Œæˆ - å‚æ•°é‡: {total_params}")
        
        # åˆ›å»ºä¼˜åŒ–å™¨
        self.optimizer = nn.Adam(
            self.model.trainable_params(),
            learning_rate=self.config.learning_rate,
            weight_decay=self.config.weight_decay
        )
        
        logger.info("ä¼˜åŒ–å™¨åˆ›å»ºå®Œæˆ")
        
        # åˆ›å»ºè®­ç»ƒæ­¥éª¤å‡½æ•°ï¼Œé¿å…é‡å¤ç¼–è¯‘
        self._setup_training_functions()
    
    def _setup_training_functions(self):
        """è®¾ç½®è®­ç»ƒå‡½æ•°ï¼Œé¿å…é‡å¤JITç¼–è¯‘"""
        def forward_fn(data, labels):
            loss, logits = self.model(data, labels)
            return loss, logits
        
        # åˆ›å»ºæ¢¯åº¦è®¡ç®—å‡½æ•°ï¼Œåªç¼–è¯‘ä¸€æ¬¡
        self.grad_fn = ms.value_and_grad(forward_fn, None, self.optimizer.parameters, has_aux=True)
        
        # åˆ›å»ºè®­ç»ƒæ­¥éª¤å‡½æ•°
        @ms.jit
        def train_step_fn(data, labels):
            (loss, logits), grads = self.grad_fn(data, labels)
            self.optimizer(grads)
            return loss, logits
        
        self.train_step_fn = train_step_fn
        logger.info("è®­ç»ƒå‡½æ•°è®¾ç½®å®Œæˆï¼Œé¿å…é‡å¤JITç¼–è¯‘")
        
        # åˆ›å»ºè¯„ä¼°æ­¥éª¤å‡½æ•°
        @ms.jit
        def eval_step_fn(data, labels):
            loss, logits = self.model(data, labels)
            predicted = ops.ArgMaxWithValue(axis=1)(logits)[0]
            return loss, logits, predicted
        
        self.eval_step_fn = eval_step_fn
    
    def train_step(self, data, labels):
        """å•æ­¥è®­ç»ƒ"""
        return self.train_step_fn(data, labels)
    
    def train_epoch(self, epoch):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.set_train(True)
        total_loss = 0.0
        correct = 0
        total = 0
        num_batches = 0
        
        logger.info(f"å¼€å§‹ç¬¬ {epoch+1}/{self.config.epochs} è½®è®­ç»ƒ...")
        
        for batch_idx, (data, labels) in enumerate(self.train_dataset.create_tuple_iterator()):
            loss, logits = self.train_step(data, labels)
            
            # è®¡ç®—å‡†ç¡®ç‡
            predicted = ops.ArgMaxWithValue(axis=1)(logits)[0]
            correct += ops.ReduceSum()(ops.Cast()(predicted == labels, ms.float32)).asnumpy()
            total += labels.shape[0]
            
            total_loss += loss.asnumpy()
            num_batches += 1
            
            if batch_idx % 5 == 0:
                logger.info(f"Batch {batch_idx}: Loss = {loss.asnumpy():.4f}")
        
        avg_loss = total_loss / num_batches if num_batches > 0 else 0
        accuracy = correct / total if total > 0 else 0
        
        logger.info(f"Epoch {epoch+1} è®­ç»ƒå®Œæˆ:")
        logger.info(f"  å¹³å‡æŸå¤±: {avg_loss:.4f}")
        logger.info(f"  è®­ç»ƒå‡†ç¡®ç‡: {accuracy:.4f}")
        
        return avg_loss, accuracy
    
    def evaluate(self):
        """è¯„ä¼°æ¨¡å‹"""
        logger.info("å¼€å§‹æ¨¡å‹è¯„ä¼°...")
        
        self.model.set_train(False)
        total_loss = 0.0
        correct = 0
        total = 0
        num_batches = 0
        
        # ç»Ÿè®¡æ¯ä¸ªç±»åˆ«çš„é¢„æµ‹ç»“æœ
        class_correct = {}
        class_total = {}
        
        for data, labels in self.val_dataset.create_tuple_iterator():
            loss, logits, predicted = self.eval_step_fn(data, labels)
            
            # è®¡ç®—å‡†ç¡®ç‡
            correct += ops.ReduceSum()(ops.Cast()(predicted == labels, ms.float32)).asnumpy()
            total += labels.shape[0]
            
            # ç»Ÿè®¡æ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡
            for i in range(labels.shape[0]):
                true_label = labels[i].asnumpy().item()
                pred_label = predicted[i].asnumpy().item()
                
                if true_label not in class_total:
                    class_total[true_label] = 0
                    class_correct[true_label] = 0
                
                class_total[true_label] += 1
                if true_label == pred_label:
                    class_correct[true_label] += 1
            
            total_loss += loss.asnumpy()
            num_batches += 1
        
        avg_loss = total_loss / num_batches if num_batches > 0 else 0
        accuracy = correct / total if total > 0 else 0
        
        logger.info(f"è¯„ä¼°å®Œæˆ:")
        logger.info(f"  éªŒè¯æŸå¤±: {avg_loss:.4f}")
        logger.info(f"  éªŒè¯å‡†ç¡®ç‡: {accuracy:.4f}")
        
        # æ‰“å°æ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡
        logger.info("å„ç±»åˆ«å‡†ç¡®ç‡:")
        for label_idx in sorted(class_total.keys()):
            if label_idx < len(self.idx2word):
                label_name = self.idx2word[label_idx]
                acc = class_correct[label_idx] / class_total[label_idx]
                logger.info(f"  {label_name}: {acc:.4f} ({class_correct[label_idx]}/{class_total[label_idx]})")
        
        return {"accuracy": accuracy, "loss": avg_loss}
    
    def train(self):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        logger.info("å¼€å§‹CE-CSLçœŸå®æ•°æ®è®­ç»ƒ...")
        
        best_accuracy = 0.0
        
        for epoch in range(self.config.epochs):
            start_time = time.time()
            
            # è®­ç»ƒä¸€ä¸ªepoch
            train_loss, train_acc = self.train_epoch(epoch)
            
            # è¯„ä¼°
            eval_results = self.evaluate()
            val_loss = eval_results["loss"]
            val_acc = eval_results["accuracy"]
            
            epoch_time = time.time() - start_time
            
            logger.info(f"Epoch {epoch+1} æ€»ç»“:")
            logger.info(f"  è®­ç»ƒæŸå¤±: {train_loss:.4f}, è®­ç»ƒå‡†ç¡®ç‡: {train_acc:.4f}")
            logger.info(f"  éªŒè¯æŸå¤±: {val_loss:.4f}, éªŒè¯å‡†ç¡®ç‡: {val_acc:.4f}")
            logger.info(f"  è€—æ—¶: {epoch_time:.2f}ç§’")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_acc > best_accuracy:
                best_accuracy = val_acc
                logger.info(f"æ–°çš„æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_accuracy:.4f}")
                
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                os.makedirs("./output", exist_ok=True)
                best_model_path = "./output/cecsl_best_model.ckpt"
                save_checkpoint(self.model, best_model_path)
                logger.info(f"æœ€ä½³æ¨¡å‹å·²ä¿å­˜: {best_model_path}")
        
        logger.info(f"è®­ç»ƒå®Œæˆ! æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_accuracy:.4f}")
        return self.model
    
    def save_model(self, model_path):
        """ä¿å­˜æ¨¡å‹"""
        logger.info(f"ä¿å­˜æ¨¡å‹åˆ°: {model_path}")
        save_checkpoint(self.model, model_path)
        
        # åŒæ—¶ä¿å­˜è¯æ±‡è¡¨
        vocab_path = model_path.replace('.ckpt', '_vocab.json')
        vocab_info = {
            'word2idx': self.word2idx,
            'idx2word': self.idx2word,
            'vocab_size': self.vocab_size
        }
        with open(vocab_path, 'w', encoding='utf-8') as f:
            json.dump(vocab_info, f, ensure_ascii=False, indent=2)
        
        logger.info(f"âœ… æ¨¡å‹å’Œè¯æ±‡è¡¨ä¿å­˜å®Œæˆ")

def main():
    """æµ‹è¯•CE-CSLè®­ç»ƒå™¨"""
    # æ£€æŸ¥æ•°æ®æ˜¯å¦å­˜åœ¨
    data_root = Path("../data/CE-CSL")  # ä»trainingç›®å½•åˆ°dataç›®å½•
    if not data_root.exists():
        logger.error(f"æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_root}")
        return False
    
    # æ£€æŸ¥å¿…è¦çš„æ–‡ä»¶
    required_files = [
        "train.corpus.csv",
        "dev.corpus.csv", 
        "processed/train/train_metadata.json"
    ]
    
    for file_path in required_files:
        if not (data_root / file_path).exists():
            logger.error(f"å¿…è¦æ–‡ä»¶ä¸å­˜åœ¨: {data_root / file_path}")
            return False
    
    # åˆ›å»ºé…ç½®
    config = CECSLTrainingConfig(
        vocab_size=1000,
        d_model=128,  # è¾ƒå°çš„æ¨¡å‹ç”¨äºæµ‹è¯•
        n_heads=4,
        n_layers=2,
        dropout=0.1,
        batch_size=4,  # è¾ƒå°çš„æ‰¹æ¬¡å¤§å°
        learning_rate=1e-3,
        epochs=10,  # è¾ƒå°‘çš„epochç”¨äºæµ‹è¯•
        max_sequence_length=100,  # è¾ƒçŸ­çš„åºåˆ—
        image_size=(224, 224),  # å®é™…çš„å›¾åƒå°ºå¯¸ï¼ŒåŒ¹é…æ•°æ®
        device_target="CPU"
    )
    
    try:
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = CECSLTrainer(config)
        
        # åŠ è½½æ•°æ®
        trainer.load_data()
        
        # æ„å»ºæ¨¡å‹
        trainer.build_model()
        
        # å¼€å§‹è®­ç»ƒ
        model = trainer.train()
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        trainer.save_model("./output/cecsl_final_model.ckpt")
        
        logger.info("âœ… CE-CSLè®­ç»ƒæˆåŠŸå®Œæˆ!")
        return True
        
    except Exception as e:
        logger.error(f"è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = main()
    if success:
        print("ğŸ‰ CE-CSLçœŸå®æ•°æ®è®­ç»ƒæˆåŠŸ!")
    else:
        print("âŒ CE-CSLè®­ç»ƒå¤±è´¥!")
        sys.exit(1)
