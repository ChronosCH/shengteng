#!/usr/bin/env python3
"""
è®­ç»ƒå¼‚å¸¸è¯Šæ–­è„šæœ¬ - åˆ†æWERä¿æŒ0çš„åŸå› 
"""

import json
import os
from collections import defaultdict

def analyze_training_logs(log_dir):
    """åˆ†æè®­ç»ƒæ—¥å¿—æ–‡ä»¶"""
    
    print("=== è®­ç»ƒå¼‚å¸¸è¯Šæ–­ ===")
    
    # æŸ¥æ‰¾æ—¥å¿—æ–‡ä»¶
    log_files = []
    if os.path.exists(log_dir):
        for file in os.listdir(log_dir):
            if file.endswith('.log') or 'train' in file.lower():
                log_files.append(os.path.join(log_dir, file))
    
    if not log_files:
        print(f"âŒ åœ¨ {log_dir} ä¸­æœªæ‰¾åˆ°æ—¥å¿—æ–‡ä»¶")
        print(f"ğŸ’¡ è¿™å¯èƒ½æ˜¯å› ä¸º:")
        print(f"  1. è®­ç»ƒæ—¥å¿—æ²¡æœ‰ä¿å­˜åˆ°è¯¥ç›®å½•")
        print(f"  2. æ—¥å¿—æ–‡ä»¶è¢«ç§»åŠ¨æˆ–åˆ é™¤")
        print(f"  3. è®­ç»ƒä½¿ç”¨äº†ä¸åŒçš„è¾“å‡ºç›®å½•")
        return
    
    print(f"å‘ç° {len(log_files)} ä¸ªæ—¥å¿—æ–‡ä»¶:")
    for log_file in log_files:
        print(f"  - {log_file}")
    
    # åˆ†ææ¯ä¸ªæ—¥å¿—æ–‡ä»¶
    for log_file in log_files:
        print(f"\n=== åˆ†æ {os.path.basename(log_file)} ===")
        analyze_single_log(log_file)

def analyze_single_log(log_file):
    """åˆ†æå•ä¸ªè®­ç»ƒæ—¥å¿—"""
    
    try:
        with open(log_file, 'r', encoding='utf-8') as f:
            lines = f.readlines()
    except Exception as e:
        print(f"âŒ æ— æ³•è¯»å–æ—¥å¿—æ–‡ä»¶: {e}")
        return
    
    print(f"æ—¥å¿—æ€»è¡Œæ•°: {len(lines)}")
    
    # åˆ†æå…³é”®æŒ‡æ ‡
    epochs = []
    wer_values = []
    loss_values = []
    best_model_updates = []
    
    current_epoch = None
    
    for i, line in enumerate(lines):
        line = line.strip()
        
        # æ£€æµ‹epochä¿¡æ¯
        if 'epoch' in line.lower() or 'Epoch' in line:
            if 'Epoch:' in line or 'epoch:' in line:
                try:
                    epoch_part = line.split('Epoch:')[-1] if 'Epoch:' in line else line.split('epoch:')[-1]
                    epoch_num = int(epoch_part.split()[0].split('/')[0])
                    current_epoch = epoch_num
                    if epoch_num not in epochs:
                        epochs.append(epoch_num)
                except:
                    pass
        
        # æ£€æµ‹WERå€¼
        if 'WER' in line:
            try:
                wer_part = line.split('WER')[-1]
                # æŸ¥æ‰¾æ•°å­—ï¼ˆå¯èƒ½æ˜¯ç™¾åˆ†æ¯”æˆ–å°æ•°ï¼‰
                import re
                wer_match = re.search(r'[:\s=]+([\d.]+)', wer_part)
                if wer_match:
                    wer_val = float(wer_match.group(1))
                    wer_values.append((current_epoch or len(wer_values), wer_val, line))
            except:
                pass
        
        # æ£€æµ‹losså€¼
        if 'loss' in line.lower() and ('=' in line or ':' in line):
            try:
                import re
                loss_match = re.search(r'loss[:\s=]+([\d.]+)', line.lower())
                if loss_match:
                    loss_val = float(loss_match.group(1))
                    loss_values.append((current_epoch or len(loss_values), loss_val, line))
            except:
                pass
        
        # æ£€æµ‹best modelæ›´æ–°
        if 'best' in line.lower() and ('model' in line.lower() or 'checkpoint' in line.lower()):
            best_model_updates.append((current_epoch or i, line))
    
    # æŠ¥å‘Šåˆ†æç»“æœ
    print(f"\nğŸ“Š è®­ç»ƒæŒ‡æ ‡ç»Ÿè®¡:")
    print(f"  æ£€æµ‹åˆ°çš„epochæ•°: {len(epochs)} {epochs if epochs else ''}")
    print(f"  WERè®°å½•æ•°: {len(wer_values)}")
    print(f"  Lossè®°å½•æ•°: {len(loss_values)}")
    print(f"  Best modelæ›´æ–°æ•°: {len(best_model_updates)}")
    
    if wer_values:
        print(f"\nğŸ“ˆ WERå˜åŒ–è¶‹åŠ¿:")
        for epoch, wer, line in wer_values[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
            print(f"  Epoch {epoch}: WER={wer:.6f}")
            if wer == 0.0:
                print(f"    âš ï¸  å‘ç°WER=0 - è¿™å¯èƒ½å¯¼è‡´best modelä¸æ›´æ–°")
        
        # æ£€æŸ¥WERæ˜¯å¦ä¸€ç›´ä¸º0
        zero_wer_count = sum(1 for _, wer, _ in wer_values if wer == 0.0)
        if zero_wer_count > 1:
            print(f"    ğŸš¨ è­¦å‘Š: {zero_wer_count}/{len(wer_values)} æ¬¡WERä¸º0")
            print(f"    ğŸ” è¿™è§£é‡Šäº†ä¸ºä»€ä¹ˆè®­ç»ƒåœ¨epoch 1ååœæ­¢æ›´æ–°best model")
    
    if loss_values:
        print(f"\nğŸ“‰ Losså˜åŒ–è¶‹åŠ¿:")
        for epoch, loss, line in loss_values[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
            print(f"  Epoch {epoch}: Loss={loss:.6f}")
    
    if best_model_updates:
        print(f"\nğŸ’¾ Best Modelæ›´æ–°è®°å½•:")
        for epoch, line in best_model_updates:
            print(f"  Epoch {epoch}: {line.strip()}")
    else:
        print(f"\nâŒ æœªå‘ç°best modelæ›´æ–°è®°å½•")
        print(f"  è¿™ç¡®è®¤äº†è®­ç»ƒè¿‡ç¨‹ä¸­best modelæ²¡æœ‰è¢«æ›´æ–°")

def analyze_model_checkpoints(checkpoint_dir):
    """åˆ†ææ¨¡å‹æ£€æŸ¥ç‚¹æ–‡ä»¶"""
    
    print(f"\n=== æ£€æŸ¥ç‚¹æ–‡ä»¶åˆ†æ ===")
    
    if not os.path.exists(checkpoint_dir):
        print(f"âŒ æ£€æŸ¥ç‚¹ç›®å½•ä¸å­˜åœ¨: {checkpoint_dir}")
        return
    
    # åˆ—å‡ºæ‰€æœ‰æ£€æŸ¥ç‚¹æ–‡ä»¶
    checkpoint_files = []
    for file in os.listdir(checkpoint_dir):
        if file.endswith('.ckpt') or 'epoch' in file.lower():
            checkpoint_files.append(file)
    
    print(f"å‘ç° {len(checkpoint_files)} ä¸ªæ£€æŸ¥ç‚¹æ–‡ä»¶:")
    
    # æŒ‰ä¿®æ”¹æ—¶é—´æ’åº
    checkpoint_info = []
    for file in checkpoint_files:
        file_path = os.path.join(checkpoint_dir, file)
        stat = os.stat(file_path)
        checkpoint_info.append((file, stat.st_mtime, stat.st_size))
    
    checkpoint_info.sort(key=lambda x: x[1])  # æŒ‰æ—¶é—´æ’åº
    
    for file, mtime, size in checkpoint_info:
        import datetime
        time_str = datetime.datetime.fromtimestamp(mtime).strftime('%Y-%m-%d %H:%M:%S')
        size_mb = size / (1024 * 1024)
        print(f"  {file}: {time_str}, {size_mb:.1f}MB")
    
    # æ£€æŸ¥best modelæ–‡ä»¶
    best_files = [f for f in checkpoint_files if 'best' in f.lower()]
    if best_files:
        print(f"\nğŸ† Best modelæ–‡ä»¶:")
        for file in best_files:
            file_path = os.path.join(checkpoint_dir, file)
            stat = os.stat(file_path)
            time_str = datetime.datetime.fromtimestamp(stat.st_mtime).strftime('%Y-%m-%d %H:%M:%S')
            print(f"  {file}: {time_str}")
    else:
        print(f"\nâŒ æœªæ‰¾åˆ°best modelæ–‡ä»¶")
        print(f"  è¿™ç¡®è®¤äº†WER=0å¯¼è‡´best modelä»æœªè¢«ä¿å­˜")

def analyze_config_file(config_path):
    """åˆ†æè®­ç»ƒé…ç½®æ–‡ä»¶"""
    
    print(f"\n=== é…ç½®æ–‡ä»¶åˆ†æ ===")
    
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)
        
        print(f"âœ“ æˆåŠŸè¯»å–é…ç½®æ–‡ä»¶: {config_path}")
        
        # æ£€æŸ¥å…³é”®é…ç½® - éœ€è¦åœ¨åµŒå¥—ç»“æ„ä¸­æŸ¥æ‰¾
        training_config = config.get('training', {})
        
        key_configs = {
            'num_epochs': training_config.get('num_epochs'),
            'early_stopping_patience': training_config.get('early_stopping_patience'),
            'learning_rate': training_config.get('learning_rate'),
            'batch_size': training_config.get('batch_size'),
            'save_interval': training_config.get('save_interval'),
            'eval_interval': training_config.get('eval_interval')
        }
        
        print(f"\nğŸ”§ å…³é”®è®­ç»ƒé…ç½®:")
        for key, value in key_configs.items():
            if value is not None:
                print(f"  {key}: {value}")
            else:
                print(f"  {key}: æœªè®¾ç½®")
        
        # æ£€æŸ¥æ¨¡å‹é…ç½®
        model_config = config.get('model', {})
        print(f"\nğŸ¤– æ¨¡å‹é…ç½®:")
        print(f"  hidden_size: {model_config.get('hidden_size', 'æœªè®¾ç½®')}")
        print(f"  device_target: {model_config.get('device_target', 'æœªè®¾ç½®')}")
        
        # åˆ†æå¯èƒ½å¯¼è‡´è®­ç»ƒå¼‚å¸¸çš„é…ç½®
        print(f"\nğŸ” è®­ç»ƒå¼‚å¸¸åŸå› åˆ†æ:")
        
        num_epochs = training_config.get('num_epochs', 5)
        patience = training_config.get('early_stopping_patience', 3)
        
        print(f"  é…ç½®çš„æ€»epochæ•°: {num_epochs}")
        print(f"  æ—©åœè€å¿ƒå€¼: {patience}")
        
        if num_epochs == 5 and patience == 3:
            print(f"\nâš ï¸  æ½œåœ¨é—®é¢˜:")
            print(f"  1. å¦‚æœWERä»epoch 1å¼€å§‹å°±æ˜¯0å¹¶ä¿æŒä¸å˜")
            print(f"  2. è®­ç»ƒå¯èƒ½ä¼šå› ä¸ºæ²¡æœ‰æ”¹å–„è€Œæ—©åœ")
            print(f"  3. æˆ–è€…best modelé€‰æ‹©é€»è¾‘å¯èƒ½æœ‰é—®é¢˜")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å…³äºæ¨¡å‹ä¿å­˜çš„é…ç½®
        save_interval = training_config.get('save_interval', 1)
        eval_interval = training_config.get('eval_interval', 1)
        
        print(f"\nğŸ’¾ æ¨¡å‹ä¿å­˜é…ç½®:")
        print(f"  ä¿å­˜é—´éš”: æ¯ {save_interval} epoch")
        print(f"  è¯„ä¼°é—´éš”: æ¯ {eval_interval} epoch")
        
        if save_interval == 1 and eval_interval == 1:
            print(f"  âœ“ åº”è¯¥æ¯ä¸ªepochéƒ½ä¿å­˜å’Œè¯„ä¼°")
            print(f"  â“ ä½†å®é™…åªä¿ç•™äº†epoch 1çš„æƒé‡ï¼Œè¯´æ˜best modelé€»è¾‘æœ‰é—®é¢˜")
    
    except Exception as e:
        print(f"âŒ æ— æ³•è¯»å–é…ç½®æ–‡ä»¶: {e}")

def main():
    """ä¸»å‡½æ•°"""
    
    config_path = "/data/shengteng/training/configs/safe_gpu_config.json"
    
    print("=== è®­ç»ƒå¼‚å¸¸è¯Šæ–­å·¥å…· ===\n")
    
    # é¦–å…ˆè¯»å–é…ç½®æ–‡ä»¶è·å–å®é™…è·¯å¾„
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)
        
        checkpoint_dir = config.get('paths', {}).get('checkpoint_dir', "/data/shengteng/training/checkpoints_gpu")
        log_dir = config.get('paths', {}).get('log_dir', "/data/shengteng/training/logs_gpu")
        output_dir = config.get('paths', {}).get('output_dir', "/data/shengteng/training/output_gpu")
        
        print(f"ğŸ“‚ é…ç½®æ–‡ä»¶æŒ‡å®šçš„è·¯å¾„:")
        print(f"  æ£€æŸ¥ç‚¹ç›®å½•: {checkpoint_dir}")
        print(f"  æ—¥å¿—ç›®å½•: {log_dir}")
        print(f"  è¾“å‡ºç›®å½•: {output_dir}")
        
    except Exception as e:
        print(f"âŒ æ— æ³•è¯»å–é…ç½®æ–‡ä»¶: {e}")
        # ä½¿ç”¨é»˜è®¤è·¯å¾„
        checkpoint_dir = "/data/shengteng/training/checkpoints_gpu"
        log_dir = "/data/shengteng/training/logs_gpu"
        output_dir = "/data/shengteng/training/output_gpu"
    
    # 1. åˆ†æè®­ç»ƒæ—¥å¿—
    print("\n1ï¸âƒ£ æ£€æŸ¥è®­ç»ƒæ—¥å¿—...")
    analyze_training_logs(log_dir)
    if log_dir != output_dir:
        print("  åŒæ—¶æ£€æŸ¥è¾“å‡ºç›®å½•...")
        analyze_training_logs(output_dir)
    
    # 2. åˆ†ææ¨¡å‹æ£€æŸ¥ç‚¹  
    print("\n2ï¸âƒ£ æ£€æŸ¥æ¨¡å‹æ£€æŸ¥ç‚¹...")
    analyze_model_checkpoints(checkpoint_dir)
    if checkpoint_dir != output_dir:
        print("  åŒæ—¶æ£€æŸ¥è¾“å‡ºç›®å½•...")
        analyze_model_checkpoints(output_dir)
    
    # 3. åˆ†æé…ç½®æ–‡ä»¶
    print("\n3ï¸âƒ£ æ£€æŸ¥é…ç½®æ–‡ä»¶...")
    analyze_config_file(config_path)
    
    print(f"\n=== è¯Šæ–­æ€»ç»“ ===")
    print(f"ğŸ” æ ¹æ®ç”¨æˆ·åé¦ˆå’Œåˆ†æç»“æœ:")
    print(f"  1. WERåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¿æŒä¸º0")
    print(f"  2. è¿™å¯¼è‡´best modelæ£€æŸ¥ç‚¹ä»æœªæ›´æ–°")
    print(f"  3. è®­ç»ƒå®é™…ä¸Šåªä¿ç•™äº†epoch 1çš„æƒé‡")
    print(f"  4. è¯æ±‡è¡¨åŒ…å«å¤§é‡é‡å¤çš„å¥å·æ ‡è®°")
    
    print(f"\nğŸ’¡ å»ºè®®çš„ä¿®å¤æ–¹æ¡ˆ:")
    print(f"  1. ä½¿ç”¨æ¸…ç†åçš„è¯æ±‡è¡¨ (å·²ç”Ÿæˆ)")
    print(f"  2. ä¿®æ”¹è®­ç»ƒé…ç½®ï¼Œä½¿ç”¨lossä½œä¸ºç›‘æ§æŒ‡æ ‡")
    print(f"  3. è®¾ç½®save_best_only=Falseä»¥ä¿å­˜æ‰€æœ‰epoch")
    print(f"  4. é‡æ–°å¼€å§‹å®Œæ•´çš„è®­ç»ƒè¿‡ç¨‹")

if __name__ == "__main__":
    main()
