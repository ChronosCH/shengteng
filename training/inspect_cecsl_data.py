#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ£€æŸ¥CE-CSLæ•°æ®çš„å®é™…å°ºå¯¸
"""

import numpy as np
from pathlib import Path
import json
import csv

def inspect_data():
    """æ£€æŸ¥æ•°æ®å°ºå¯¸"""
    data_root = Path("../data/CE-CSL")
    
    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨æ¸…ç†åçš„è¯æ±‡è¡¨
    cleaned_vocab_file = data_root / "cleaned_vocab.json"
    if cleaned_vocab_file.exists():
        print("=== æ¸…ç†åçš„è¯æ±‡è¡¨ ===")
        with open(cleaned_vocab_file, 'r', encoding='utf-8') as f:
            vocab_data = json.load(f)
        print(f"æ¸…ç†åè¯æ±‡è¡¨å¤§å°: {vocab_data['vocab_size']}")
        print(f"æœ€å¸¸è§æ ‡ç­¾: {[item[0] for item in vocab_data['most_common'][:10]]}")
    
    # é¦–å…ˆè¿›è¡Œå…¨é¢ç»Ÿè®¡
    print("ğŸ” CE-CSLæ•°æ®é›†å®Œæ•´æ€§æ£€æŸ¥")
    print("=" * 60)
    
    # 1. æ£€æŸ¥åŸå§‹è§†é¢‘vsé¢„å¤„ç†æ•°æ®çš„æ•°é‡å¯¹æ¯”
    print("\n1. æ•°æ®é‡å¯¹æ¯”:")
    for split in ["train", "dev", "test"]:
        # åŸå§‹è§†é¢‘è®¡æ•°
        video_dir = data_root / "video" / split
        if video_dir.exists():
            total_videos = 0
            for translator_dir in video_dir.iterdir():
                if translator_dir.is_dir():
                    videos = list(translator_dir.glob("*.mp4"))
                    total_videos += len(videos)
        else:
            total_videos = 0
        
        # é¢„å¤„ç†æ•°æ®è®¡æ•°
        processed_dir = data_root / "processed" / split
        if processed_dir.exists():
            processed_count = len(list(processed_dir.glob("*_frames.npy")))
        else:
            processed_count = 0
        
        # Corpusè®°å½•è®¡æ•°
        corpus_file = data_root / f"{split}.corpus.csv"
        if corpus_file.exists():
            with open(corpus_file, 'r', encoding='utf-8') as f:
                corpus_count = len(f.readlines()) - 1  # å‡å»æ ‡é¢˜è¡Œ
        else:
            corpus_count = 0
        
        print(f"  {split:5}: åŸå§‹è§†é¢‘={total_videos:3d}, é¢„å¤„ç†={processed_count:3d}, Corpus={corpus_count:3d}")
        
        if total_videos > processed_count:
            print(f"    âš ï¸  {split} å­˜åœ¨æœªå¤„ç†çš„è§†é¢‘ ({total_videos - processed_count} ä¸ª)")
    
    # 2. æ£€æŸ¥corpusæ–‡ä»¶
    print("\n2. æ£€æŸ¥ Corpus æ–‡ä»¶")
    for split in ["train", "dev", "test"]:
        corpus_file = data_root / f"{split}.corpus.csv"
        if corpus_file.exists():
            with open(corpus_file, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                rows = list(reader)
                print(f"{split}.corpus.csv: {len(rows)} æ¡è®°å½•")
                if rows:
                    print(f"  ç¤ºä¾‹: {rows[0]}")
    
    # 3. æ£€æŸ¥å…ƒæ•°æ®
    print("\n3. æ£€æŸ¥å…ƒæ•°æ®æ–‡ä»¶")
    for split in ["train", "dev"]:
        metadata_file = data_root / "processed" / split / f"{split}_metadata.json"
        print(f"æ£€æŸ¥å…ƒæ•°æ®æ–‡ä»¶: {metadata_file}")
        
        if metadata_file.exists():
            with open(metadata_file, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
            
            print(f"å…ƒæ•°æ®æ¡ç›®æ•°: {len(metadata)}")
            if metadata:
                print(f"ç¬¬ä¸€ä¸ªæ¡ç›®: {metadata[0]}")
        
        # æ£€æŸ¥å®é™…çš„å¸§æ•°æ®
        train_dir = data_root / "processed" / split
        print(f"\næ£€æŸ¥{split}æ•°æ®ç›®å½•: {train_dir}")
        
        # æ‰¾åˆ°æ‰€æœ‰.npyæ–‡ä»¶
        npy_files = list(train_dir.glob("*_frames.npy"))
        print(f"æ‰¾åˆ° {len(npy_files)} ä¸ªå¸§æ•°æ®æ–‡ä»¶")
        
        if npy_files:
            # æ£€æŸ¥å‰å‡ ä¸ªæ–‡ä»¶çš„å°ºå¯¸
            for i, npy_file in enumerate(npy_files[:3]):
                print(f"\næ–‡ä»¶ {i+1}: {npy_file.name}")
                try:
                    frames = np.load(npy_file)
                    print(f"  å½¢çŠ¶: {frames.shape}")
                    print(f"  æ•°æ®ç±»å‹: {frames.dtype}")
                    print(f"  å€¼èŒƒå›´: [{frames.min():.3f}, {frames.max():.3f}]")
                    
                    # åˆ†ææ•°æ®ç»“æ„
                    if len(frames.shape) == 4:  # (T, H, W, C)
                        T, H, W, C = frames.shape
                        print(f"  è§†é¢‘å¸§: T={T}, H={H}, W={W}, C={C}")
                        # å±•å¹³åçš„ç‰¹å¾ç»´åº¦
                        feature_dim = H * W * C
                        print(f"  å±•å¹³ç‰¹å¾ç»´åº¦: {feature_dim}")
                    elif len(frames.shape) == 3:  # (T, H, W)
                        T, H, W = frames.shape
                        print(f"  ç°åº¦è§†é¢‘å¸§: T={T}, H={H}, W={W}")
                    elif len(frames.shape) == 2:  # (T, F)
                        T, F = frames.shape
                        print(f"  ç‰¹å¾åºåˆ—: T={T}, F={F}")
                    
                except Exception as e:
                    print(f"  åŠ è½½å¤±è´¥: {e}")
    
    # æ£€æŸ¥å¼€å‘é›†
    dev_dir = data_root / "processed" / "dev"
    if dev_dir.exists():
        dev_npy_files = list(dev_dir.glob("*_frames.npy"))
        print(f"\nå¼€å‘é›†æ‰¾åˆ° {len(dev_npy_files)} ä¸ªå¸§æ•°æ®æ–‡ä»¶")

if __name__ == "__main__":
    inspect_data()
