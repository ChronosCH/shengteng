#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
CE-CSLæ ‡ç­¾æ¸…ç†å·¥å…·
å¤„ç†å¼‚å¸¸æ ‡ç­¾ã€ç‰¹æ®Šå­—ç¬¦å’Œé•¿æ ‡ç­¾é—®é¢˜
"""

import re
import json
import csv
from pathlib import Path
from collections import defaultdict, Counter
from typing import Dict, List, Tuple, Set
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class LabelCleaner:
    """æ ‡ç­¾æ¸…ç†å™¨"""
    
    def __init__(self):
        # ç‰¹æ®Šå­—ç¬¦æ¸…ç†è§„åˆ™
        self.special_char_patterns = [
            (r'\{[^}]*\}', ''),  # åˆ é™¤ {xxx} æ³¨é‡Š
            (r'\([^)]*\)', ''),  # åˆ é™¤ (xxx) æ³¨é‡Š
            (r'\[[^\]]*\]', ''), # åˆ é™¤ [xxx] æ³¨é‡Š
            (r'ï¼ˆ[^ï¼‰]*ï¼‰', ''),  # åˆ é™¤ ï¼ˆxxxï¼‰ æ³¨é‡Š
            (r'[{}()\[\]ï¼ˆï¼‰]', ''),  # åˆ é™¤å‰©ä½™æ‹¬å·
        ]
        
        # å¸¸è§æ‰‹è¯­è¯æ±‡æ˜ å°„
        self.common_mappings = {
            'æ–‡æ–‡ç« çš„ç¬¬ä¸€ä¸ªæ‰‹åŠ¿åŠ¨ä½œ': 'æ–‡ç« ',
            'ä½ ç…§é¡¾': 'ç…§é¡¾',
            'é¾™è™¾': 'é¾™è™¾',
            # å¯ä»¥æ ¹æ®éœ€è¦æ·»åŠ æ›´å¤šæ˜ å°„
        }
        
        # åœç”¨è¯åˆ—è¡¨
        self.stop_words = {'çš„', 'äº†', 'æ˜¯', 'åœ¨', 'å’Œ', 'ä¸', 'æˆ–'}
        
        # æœ€å¤§æ ‡ç­¾é•¿åº¦
        self.max_label_length = 6
    
    def clean_single_label(self, label: str) -> str:
        """æ¸…ç†å•ä¸ªæ ‡ç­¾"""
        if not label or not label.strip():
            return ""
        
        original_label = label
        cleaned = label.strip()
        
        # 1. åº”ç”¨é¢„å®šä¹‰æ˜ å°„
        if cleaned in self.common_mappings:
            cleaned = self.common_mappings[cleaned]
            logger.debug(f"æ˜ å°„: '{original_label}' -> '{cleaned}'")
            return cleaned
        
        # 2. æ¸…ç†ç‰¹æ®Šå­—ç¬¦å’Œæ³¨é‡Š
        for pattern, replacement in self.special_char_patterns:
            cleaned = re.sub(pattern, replacement, cleaned)
        
        # 3. æ¸…ç†å¤šä½™ç©ºæ ¼
        cleaned = re.sub(r'\s+', '', cleaned)
        
        # 4. å¤„ç†è¿‡é•¿æ ‡ç­¾
        if len(cleaned) > self.max_label_length:
            # å°è¯•æå–ä¸»è¦è¯æ±‡
            cleaned = self._extract_main_word(cleaned)
        
        # 5. å¤„ç†ç©ºç»“æœ
        if not cleaned:
            # å°è¯•ä»åŸå§‹æ ‡ç­¾æå–æœ‰ç”¨ä¿¡æ¯
            cleaned = self._fallback_extraction(original_label)
        
        logger.debug(f"æ¸…ç†: '{original_label}' -> '{cleaned}'")
        return cleaned
    
    def _extract_main_word(self, long_label: str) -> str:
        """ä»é•¿æ ‡ç­¾ä¸­æå–ä¸»è¦è¯æ±‡"""
        # å¸¸è§çš„è¯æ±‡ä¼˜å…ˆçº§
        priority_words = ['ä½ ', 'æˆ‘', 'ä»–', 'å¥¹', 'è°¢è°¢', 'è¯·', 'å¥½', 'ä¸', 'æ˜¯', 'æœ‰', 'æ²¡æœ‰']
        
        for word in priority_words:
            if word in long_label:
                return word
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä¼˜å…ˆè¯æ±‡ï¼Œå–å‰é¢çš„å­—ç¬¦
        if len(long_label) > 2:
            return long_label[:2]
        
        return long_label
    
    def _fallback_extraction(self, original_label: str) -> str:
        """åå¤‡æå–æ–¹æ³•"""
        # ç§»é™¤æ‰€æœ‰ç‰¹æ®Šå­—ç¬¦ï¼Œä¿ç•™ä¸­æ–‡å­—ç¬¦
        chinese_only = re.sub(r'[^\u4e00-\u9fff]', '', original_label)
        
        if chinese_only:
            # è¿”å›å‰ä¸¤ä¸ªä¸­æ–‡å­—ç¬¦
            return chinese_only[:2] if len(chinese_only) >= 2 else chinese_only
        
        # å¦‚æœæ²¡æœ‰ä¸­æ–‡å­—ç¬¦ï¼Œè¿”å›é»˜è®¤æ ‡ç­¾
        return "æœªçŸ¥"
    
    def analyze_labels(self, corpus_files: List[str]) -> Dict:
        """åˆ†ææ‰€æœ‰æ ‡ç­¾"""
        all_labels = []
        label_counts = Counter()
        problematic_labels = []
        
        for corpus_file in corpus_files:
            if not Path(corpus_file).exists():
                continue
                
            with open(corpus_file, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    label = row['label'].strip()
                    all_labels.append(label)
                    label_counts[label] += 1
                    
                    # æ£€æŸ¥é—®é¢˜æ ‡ç­¾
                    if len(label) > self.max_label_length or any(char in label for char in '{}()[]ï¼ˆï¼‰'):
                        problematic_labels.append(label)
        
        return {
            'total_labels': len(all_labels),
            'unique_labels': len(label_counts),
            'label_distribution': dict(label_counts.most_common(20)),
            'problematic_labels': list(set(problematic_labels)),
            'max_length': max(len(label) for label in all_labels) if all_labels else 0,
            'avg_length': sum(len(label) for label in all_labels) / len(all_labels) if all_labels else 0
        }
    
    def clean_corpus_files(self, data_root: str) -> Dict:
        """æ¸…ç†æ‰€æœ‰corpusæ–‡ä»¶"""
        data_path = Path(data_root)
        results = {}
        
        for split in ['train', 'dev', 'test']:
            corpus_file = data_path / f"{split}.corpus.csv"
            if not corpus_file.exists():
                continue
            
            logger.info(f"æ¸…ç† {split}.corpus.csv...")
            
            # å¤‡ä»½åŸæ–‡ä»¶
            backup_file = data_path / f"{split}.corpus.csv.backup"
            if not backup_file.exists():
                corpus_file.rename(backup_file)
                logger.info(f"å¤‡ä»½åŸæ–‡ä»¶: {backup_file}")
            
            # è¯»å–å¹¶æ¸…ç†æ•°æ®
            cleaned_records = []
            original_records = []
            
            with open(backup_file, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    original_label = row['label']
                    cleaned_label = self.clean_single_label(original_label)
                    
                    # è·³è¿‡ç©ºæ ‡ç­¾
                    if not cleaned_label:
                        logger.warning(f"è·³è¿‡ç©ºæ ‡ç­¾è®°å½•: {row['video_id']}")
                        continue
                    
                    cleaned_record = dict(row)
                    cleaned_record['label'] = cleaned_label
                    cleaned_records.append(cleaned_record)
                    original_records.append(original_label)
            
            # å†™å…¥æ¸…ç†åçš„æ–‡ä»¶
            with open(corpus_file, 'w', encoding='utf-8', newline='') as f:
                if cleaned_records:
                    writer = csv.DictWriter(f, fieldnames=cleaned_records[0].keys())
                    writer.writeheader()
                    writer.writerows(cleaned_records)
            
            # ç»Ÿè®¡ç»“æœ
            results[split] = {
                'original_count': len(original_records),
                'cleaned_count': len(cleaned_records),
                'removed_count': len(original_records) - len(cleaned_records),
                'unique_labels': len(set(record['label'] for record in cleaned_records))
            }
            
            logger.info(f"{split} æ¸…ç†å®Œæˆ: {results[split]}")
        
        return results
    
    def create_cleaned_vocabulary(self, data_root: str) -> Dict:
        """åˆ›å»ºæ¸…ç†åçš„è¯æ±‡è¡¨"""
        data_path = Path(data_root)
        
        # æ”¶é›†æ‰€æœ‰æ¸…ç†åçš„æ ‡ç­¾
        all_labels = set()
        label_counts = Counter()
        
        for split in ['train', 'dev', 'test']:
            corpus_file = data_path / f"{split}.corpus.csv"
            if corpus_file.exists():
                with open(corpus_file, 'r', encoding='utf-8') as f:
                    reader = csv.DictReader(f)
                    for row in reader:
                        label = row['label']
                        all_labels.add(label)
                        label_counts[label] += 1
        
        # åˆ›å»ºè¯æ±‡è¡¨
        vocab_list = ['<PAD>', '<UNK>'] + sorted(list(all_labels))
        word2idx = {word: i for i, word in enumerate(vocab_list)}
        
        vocab_info = {
            'vocab_size': len(vocab_list),
            'word2idx': word2idx,
            'idx2word': vocab_list,
            'label_distribution': dict(label_counts),
            'most_common': label_counts.most_common(10)
        }
        
        # ä¿å­˜è¯æ±‡è¡¨
        vocab_file = data_path / "cleaned_vocab.json"
        with open(vocab_file, 'w', encoding='utf-8') as f:
            json.dump(vocab_info, f, indent=2, ensure_ascii=False)
        
        logger.info(f"æ¸…ç†åè¯æ±‡è¡¨å·²ä¿å­˜: {vocab_file}")
        logger.info(f"è¯æ±‡è¡¨å¤§å°: {len(vocab_list)}")
        
        return vocab_info

def main():
    print("ğŸ§¹ CE-CSLæ ‡ç­¾æ¸…ç†å·¥å…·")
    print("=" * 50)
    
    data_root = "../data/CE-CSL"
    cleaner = LabelCleaner()
    
    # 1. åˆ†æåŸå§‹æ ‡ç­¾
    print("\n1. åˆ†æåŸå§‹æ ‡ç­¾...")
    corpus_files = [
        f"{data_root}/train.corpus.csv",
        f"{data_root}/dev.corpus.csv", 
        f"{data_root}/test.corpus.csv"
    ]
    
    analysis = cleaner.analyze_labels(corpus_files)
    print(f"æ€»æ ‡ç­¾æ•°: {analysis['total_labels']}")
    print(f"å”¯ä¸€æ ‡ç­¾æ•°: {analysis['unique_labels']}")
    print(f"æœ€å¤§é•¿åº¦: {analysis['max_length']}")
    print(f"å¹³å‡é•¿åº¦: {analysis['avg_length']:.1f}")
    print(f"é—®é¢˜æ ‡ç­¾æ•°: {len(analysis['problematic_labels'])}")
    
    # 2. æ¸…ç†corpusæ–‡ä»¶
    print("\n2. æ¸…ç†corpusæ–‡ä»¶...")
    results = cleaner.clean_corpus_files(data_root)
    
    # 3. åˆ›å»ºæ¸…ç†åçš„è¯æ±‡è¡¨
    print("\n3. åˆ›å»ºæ¸…ç†åçš„è¯æ±‡è¡¨...")
    vocab_info = cleaner.create_cleaned_vocabulary(data_root)
    
    # 4. æ€»ç»“
    print("\nğŸ‰ æ¸…ç†å®Œæˆ!")
    print("=" * 50)
    for split, result in results.items():
        print(f"{split}: {result['original_count']} -> {result['cleaned_count']} "
              f"(ç§»é™¤ {result['removed_count']}, å”¯ä¸€æ ‡ç­¾ {result['unique_labels']})")
    
    print(f"\næœ€ç»ˆè¯æ±‡è¡¨å¤§å°: {vocab_info['vocab_size']}")
    print("æœ€å¸¸è§æ ‡ç­¾:", [item[0] for item in vocab_info['most_common']])
    
    print("\nå»ºè®®è¿è¡Œä»¥ä¸‹å‘½ä»¤éªŒè¯ç»“æœ:")
    print("python inspect_cecsl_data.py")

if __name__ == "__main__":
    main()
