#!/usr/bin/env python3
"""
GPUå†…å­˜ä¼˜åŒ–å’Œæ¸…ç†è„šæœ¬
åœ¨è®­ç»ƒå‰è¿è¡Œä»¥ç¡®ä¿æœ€ä½³çš„å†…å­˜ä½¿ç”¨
"""

import gc
import os
import sys
import subprocess

def check_gpu_memory():
    """æ£€æŸ¥GPUå†…å­˜ä½¿ç”¨æƒ…å†µ"""
    try:
        result = subprocess.run(['nvidia-smi', '--query-gpu=memory.used,memory.total', '--format=csv,nounits,noheader'], 
                              capture_output=True, text=True)
        if result.returncode == 0:
            lines = result.stdout.strip().split('\n')
            for i, line in enumerate(lines):
                used, total = map(int, line.split(','))
                used_gb = used / 1024
                total_gb = total / 1024
                free_gb = total_gb - used_gb
                print(f"GPU {i}: {used_gb:.1f}GB / {total_gb:.1f}GB used, {free_gb:.1f}GB free")
                
                if free_gb < 2.0:
                    print(f"âš ï¸  Warning: GPU {i} has less than 2GB free memory")
                    return False
                elif free_gb < 4.0:
                    print(f"âš ï¸  Warning: GPU {i} has less than 4GB free memory")
                    
            return True
    except Exception as e:
        print(f"Error checking GPU memory: {e}")
        return False

def clear_gpu_cache():
    """æ¸…ç†GPUç¼“å­˜"""
    try:
        # æ¸…ç†Pythonåƒåœ¾å›æ”¶
        gc.collect()
        
        # å°è¯•æ¸…ç†MindSporeç¼“å­˜
        try:
            import mindspore as ms
            # å¦‚æœæœ‰ä»»ä½•å¼ é‡æ“ä½œç¼“å­˜ï¼Œæ¸…ç†å®ƒä»¬
            print("âœ“ MindSpore memory cleared")
        except ImportError:
            print("MindSpore not available for cache clearing")
            
        # æ¸…ç†CUDAç¼“å­˜ï¼ˆå¦‚æœä½¿ç”¨PyTorchï¼‰
        try:
            import torch
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                print("âœ“ PyTorch CUDA cache cleared")
        except ImportError:
            pass
            
        print("âœ“ System garbage collection completed")
        return True
        
    except Exception as e:
        print(f"Error during cache clearing: {e}")
        return False

def optimize_environment():
    """ä¼˜åŒ–ç¯å¢ƒå˜é‡"""
    # è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡
    os.environ['CUDA_LAUNCH_BLOCKING'] = '1'  # åŒæ­¥CUDAæ“ä½œï¼Œæœ‰åŠ©äºè°ƒè¯•
    os.environ['CUDA_CACHE_DISABLE'] = '1'    # ç¦ç”¨CUDAç¼“å­˜
    
    # è®¾ç½®MindSporeç›¸å…³ç¯å¢ƒå˜é‡
    os.environ['MS_DEV_ENABLE_FALLBACK'] = '0'  # ç¦ç”¨fallbackä»¥é¿å…å†…å­˜ç¢ç‰‡
    
    print("âœ“ Environment variables optimized")

def check_system_memory():
    """æ£€æŸ¥ç³»ç»Ÿå†…å­˜"""
    try:
        with open('/proc/meminfo', 'r') as f:
            meminfo = f.read()
        
        for line in meminfo.split('\n'):
            if 'MemAvailable:' in line:
                mem_available_kb = int(line.split()[1])
                mem_available_gb = mem_available_kb / (1024 * 1024)
                print(f"System RAM available: {mem_available_gb:.1f}GB")
                
                if mem_available_gb < 4.0:
                    print("âš ï¸  Warning: Less than 4GB system RAM available")
                    return False
                break
                
        return True
    except Exception as e:
        print(f"Error checking system memory: {e}")
        return False

def main():
    print("ğŸš€ GPU Memory Optimization and Cleanup")
    print("=" * 50)
    
    # ä¼˜åŒ–ç¯å¢ƒ
    optimize_environment()
    
    # æ¸…ç†ç¼“å­˜
    print("\nğŸ“‹ Cleaning caches...")
    clear_gpu_cache()
    
    # æ£€æŸ¥ç³»ç»Ÿå†…å­˜
    print("\nğŸ’¾ Checking system memory...")
    check_system_memory()
    
    # æ£€æŸ¥GPUå†…å­˜
    print("\nğŸ® Checking GPU memory...")
    gpu_ok = check_gpu_memory()
    
    print("\n" + "=" * 50)
    if gpu_ok:
        print("âœ… Memory optimization completed successfully!")
        print("ğŸ’¡ Recommendations:")
        print("   - Use batch_size=1 for initial testing")
        print("   - Monitor GPU memory during training")
        print("   - Consider reducing model size if issues persist")
    else:
        print("âŒ GPU memory issues detected!")
        print("ğŸ’¡ Try these solutions:")
        print("   1. Restart the system or kill GPU processes")
        print("   2. Use smaller batch sizes")
        print("   3. Reduce model parameters")
        print("   4. Use gradient checkpointing")
        
    return gpu_ok

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
