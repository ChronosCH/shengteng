#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
é«˜çº§ä¼˜åŒ–CE-CSLæ‰‹è¯­è¯†åˆ«è®­ç»ƒå™¨
é’ˆå¯¹å‡†ç¡®ç‡æå‡çš„ä¸“ä¸šçº§è§£å†³æ–¹æ¡ˆ
"""

import os
import json
import logging
import time
import math
from pathlib import Path
import numpy as np
from collections import defaultdict, Counter
import mindspore as ms
from mindspore import nn, ops, Tensor, Parameter
from mindspore.dataset import GeneratorDataset
from mindspore.communication.management import init, get_rank, get_group_size

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(levelname)s:%(name)s:%(message)s'
)
logger = logging.getLogger(__name__)

class AdvancedOptimizedConfig:
    """é«˜çº§ä¼˜åŒ–é…ç½®"""
    def __init__(self):
        # è®­ç»ƒé…ç½®
        self.epochs = 100  # æ›´å¤šè½®æ¬¡
        self.batch_size = 2  # ç¨å¤§çš„æ‰¹æ¬¡
        self.initial_learning_rate = 0.01
        self.min_learning_rate = 0.0001
        self.weight_decay = 0.00001
        self.device = "CPU"
        
        # æ¨¡å‹é…ç½® - æ³¨æ„åŠ›å¢å¼º
        self.input_dim = 258
        self.hidden_dim = 24  # å¹³è¡¡å¤æ‚åº¦å’Œæ€§èƒ½
        self.attention_dim = 12
        self.num_classes = 10
        self.dropout_rate = 0.05  # è½»å¾®dropout
        
        # æ•°æ®é…ç½®
        self.data_dir = "data/CS-CSL"
        self.augmentation_factor = 15  # æ›´æ¿€è¿›çš„å¢å¼º
        self.max_frames = 80  # å‡å°‘å¡«å……å¼€é”€
        self.ensemble_models = 3  # é›†æˆå­¦ä¹ 
        
        # å­¦ä¹ ç‡è°ƒåº¦
        self.scheduler_type = "cosine_annealing"
        self.warmup_epochs = 5
        self.restart_epochs = 20
        
        # è®­ç»ƒç­–ç•¥
        self.patience = 35
        self.min_improvement = 0.005
        self.gradient_clip_norm = 1.0
        
        # é«˜çº§æŠ€æœ¯
        self.use_mixup = True
        self.mixup_alpha = 0.2
        self.use_label_smoothing = True
        self.label_smoothing = 0.1
        self.use_focal_loss = True
        self.focal_alpha = 0.25
        self.focal_gamma = 2.0
        
        # è¾“å‡ºé…ç½®
        self.output_dir = "output"
        self.model_save_path = os.path.join(self.output_dir, "advanced_optimized_model.ckpt")
        self.vocab_save_path = os.path.join(self.output_dir, "advanced_optimized_vocab.json")
        self.history_save_path = os.path.join(self.output_dir, "advanced_optimized_history.json")

class SmartDataAugmentor:
    """æ™ºèƒ½æ•°æ®å¢å¼ºå™¨"""
    def __init__(self, config):
        self.config = config
        
    def augment_sequence(self, sequence, label, num_augmentations=15):
        """æ™ºèƒ½æ•°æ®å¢å¼º"""
        augmented_data = []
        
        for i in range(num_augmentations):
            aug_seq = sequence.copy()
            
            # 1. è‡ªé€‚åº”æ—¶é—´æ‰­æ›²
            if np.random.random() < 0.6:
                # éçº¿æ€§æ—¶é—´æ‰­æ›²
                length = len(aug_seq)
                warp_points = np.sort(np.random.uniform(0, 1, 3))
                warp_values = np.random.uniform(0.7, 1.3, 3)
                
                indices = []
                for j in range(length):
                    t = j / (length - 1)
                    warp_factor = np.interp(t, warp_points, warp_values)
                    new_idx = int(j * warp_factor) % length
                    indices.append(new_idx)
                
                aug_seq = aug_seq[indices]
            
            # 2. å¤šå°ºåº¦å™ªå£°æ³¨å…¥
            if np.random.random() < 0.8:
                # é«˜é¢‘å™ªå£°
                high_freq_noise = np.random.normal(0, 0.005, aug_seq.shape)
                # ä½é¢‘å™ªå£°
                low_freq_scale = max(1, len(aug_seq) // 10)
                low_freq_noise = np.repeat(
                    np.random.normal(0, 0.02, (len(aug_seq) // low_freq_scale + 1, aug_seq.shape[1])),
                    low_freq_scale, axis=0
                )[:len(aug_seq)]
                
                aug_seq = aug_seq + high_freq_noise + low_freq_noise
            
            # 3. å…³é”®ç‚¹é‡è¦æ€§é‡‡æ ·
            if np.random.random() < 0.4:
                # åŸºäºæ–¹å·®çš„é‡è¦æ€§é‡‡æ ·
                variance = np.var(aug_seq, axis=0)
                importance = variance / (np.sum(variance) + 1e-8)
                
                # ä¿æŠ¤é‡è¦å…³é”®ç‚¹ï¼Œé®æŒ¡ä¸é‡è¦çš„
                mask_prob = 1 - importance
                mask = np.random.random(aug_seq.shape[1]) > mask_prob * 0.3
                aug_seq[:, ~mask] *= np.random.uniform(0.3, 0.7)
            
            # 4. åºåˆ—åˆ†æ®µéšæœºåŒ–
            if np.random.random() < 0.3 and len(aug_seq) > 10:
                num_segments = np.random.randint(2, 5)
                segment_size = len(aug_seq) // num_segments
                
                segments = []
                for s in range(num_segments):
                    start = s * segment_size
                    end = start + segment_size if s < num_segments - 1 else len(aug_seq)
                    segments.append(aug_seq[start:end])
                
                # éšæœºé‡æ’éƒ¨åˆ†æ®µ
                if np.random.random() < 0.5:
                    np.random.shuffle(segments[:num_segments//2])
                
                aug_seq = np.vstack(segments)
            
            # 5. åŠ¨æ€èŒƒå›´ç¼©æ”¾
            if np.random.random() < 0.5:
                scale_factor = np.random.uniform(0.8, 1.2, aug_seq.shape[1])
                aug_seq = aug_seq * scale_factor
            
            # 6. å¹³æ»‘æ»¤æ³¢
            if np.random.random() < 0.3:
                # ç®€å•ç§»åŠ¨å¹³å‡
                window_size = min(3, len(aug_seq))
                if window_size > 1:
                    kernel = np.ones(window_size) / window_size
                    for dim in range(aug_seq.shape[1]):
                        aug_seq[:, dim] = np.convolve(aug_seq[:, dim], kernel, mode='same')
            
            # ç¡®ä¿åºåˆ—é•¿åº¦åˆç†
            if len(aug_seq) < 8:
                repeat_times = (8 // len(aug_seq)) + 1
                aug_seq = np.tile(aug_seq, (repeat_times, 1))[:8]
            
            augmented_data.append((aug_seq, label))
        
        return augmented_data

class SimpleAttentionLayer(nn.Cell):
    """ç®€åŒ–çš„æ³¨æ„åŠ›å±‚"""
    def __init__(self, hidden_dim, attention_dim):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.attention_dim = attention_dim
        
        self.attention_proj = nn.Dense(hidden_dim, attention_dim)
        self.output_proj = nn.Dense(attention_dim, hidden_dim)
        self.activation = nn.Tanh()
        
    def construct(self, x):
        # x shape: (batch_size, seq_len, hidden_dim)
        batch_size, seq_len, _ = x.shape
        
        # æŠ•å½±åˆ°æ³¨æ„åŠ›ç©ºé—´
        attention_features = self.activation(self.attention_proj(x))
        
        # å…¨å±€å¹³å‡æ± åŒ–
        pooled_features = ops.ReduceMean(keep_dims=False)(attention_features, 1)
        
        # æ‰©å±•å›åºåˆ—é•¿åº¦
        expanded_features = ops.Tile()(pooled_features.expand_dims(1), (1, seq_len, 1))
        
        # è¾“å‡ºæŠ•å½±
        output = self.output_proj(expanded_features)
        
        # ç®€å•çš„æ³¨æ„åŠ›æƒé‡ï¼ˆç”¨äºå¯è§†åŒ–ï¼‰
        attention_weights = ops.Ones()((batch_size, seq_len), ms.float32) / seq_len
        
        return output, attention_weights

class FocalLoss(nn.Cell):
    """Focal Loss for imbalanced classes"""
    def __init__(self, alpha=0.25, gamma=2.0, num_classes=10):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.num_classes = num_classes
        self.ce_loss = nn.CrossEntropyLoss(reduction='none')
        
    def construct(self, logits, labels):
        ce_loss = self.ce_loss(logits, labels)
        pt = ops.Exp()(-ce_loss)
        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss
        return ops.ReduceMean()(focal_loss)

class LabelSmoothingLoss(nn.Cell):
    """Label Smoothing Loss"""
    def __init__(self, num_classes=10, smoothing=0.1):
        super().__init__()
        self.num_classes = num_classes
        self.smoothing = smoothing
        self.confidence = 1.0 - smoothing
        
    def construct(self, logits, labels):
        log_probs = ops.LogSoftmax(axis=-1)(logits)
        nll_loss = -log_probs.gather_elements(1, labels.unsqueeze(1)).squeeze(1)
        smooth_loss = -log_probs.mean(axis=-1)
        loss = self.confidence * nll_loss + self.smoothing * smooth_loss
        return ops.ReduceMean()(loss)

class AdvancedOptimizedModel(nn.Cell):
    """é«˜çº§ä¼˜åŒ–æ¨¡å‹ - æ³¨æ„åŠ›å¢å¼º"""
    def __init__(self, config):
        super().__init__()
        self.config = config
        
        # è¾“å…¥å¤„ç†
        self.input_norm = nn.LayerNorm((config.input_dim,))
        self.input_proj = nn.Dense(config.input_dim, config.hidden_dim)
        
        # æ³¨æ„åŠ›å±‚
        self.attention = SimpleAttentionLayer(config.hidden_dim, config.attention_dim)
        
        # ç‰¹å¾æå–
        self.feature_layers = nn.SequentialCell([
            nn.Dense(config.hidden_dim, config.hidden_dim),
            nn.ReLU(),
            nn.Dropout(keep_prob=1-config.dropout_rate),
            nn.Dense(config.hidden_dim, config.hidden_dim // 2),
            nn.ReLU(),
        ])
        
        # æ—¶é—´æ± åŒ–
        self.temporal_pool = ops.ReduceMean(keep_dims=False)
        
        # åˆ†ç±»å¤´
        self.classifier = nn.SequentialCell([
            nn.Dense(config.hidden_dim // 2, config.hidden_dim // 4),
            nn.ReLU(),
            nn.Dropout(keep_prob=1-config.dropout_rate),
            nn.Dense(config.hidden_dim // 4, config.num_classes)
        ])
        
        # æ¿€æ´»å‡½æ•°
        self.relu = nn.ReLU()
        
    def construct(self, x):
        # x shape: (batch_size, seq_len, input_dim)
        batch_size, seq_len, input_dim = x.shape
        
        # è¾“å…¥æ ‡å‡†åŒ–å’ŒæŠ•å½±
        x_norm = self.input_norm(x)
        x_proj = self.relu(self.input_proj(x_norm))
        
        # è‡ªæ³¨æ„åŠ›
        x_attended, attention_weights = self.attention(x_proj)
        
        # æ®‹å·®è¿æ¥
        x_residual = x_proj + x_attended
        
        # ç‰¹å¾æå–
        features = self.feature_layers(x_residual)
        
        # æ—¶é—´ç»´åº¦æ± åŒ–
        pooled_features = self.temporal_pool(features, 1)
        
        # åˆ†ç±»
        logits = self.classifier(pooled_features)
        
        return logits

class CosineAnnealingLR:
    """ä½™å¼¦é€€ç«å­¦ä¹ ç‡è°ƒåº¦å™¨"""
    def __init__(self, initial_lr, min_lr, restart_epochs, warmup_epochs=0):
        self.initial_lr = initial_lr
        self.min_lr = min_lr
        self.restart_epochs = restart_epochs
        self.warmup_epochs = warmup_epochs
        
    def get_lr(self, epoch):
        if epoch < self.warmup_epochs:
            # çº¿æ€§warmup
            return self.initial_lr * (epoch + 1) / self.warmup_epochs
        
        # ä½™å¼¦é€€ç«
        epoch_in_cycle = (epoch - self.warmup_epochs) % self.restart_epochs
        cosine_decay = 0.5 * (1 + math.cos(math.pi * epoch_in_cycle / self.restart_epochs))
        return self.min_lr + (self.initial_lr - self.min_lr) * cosine_decay

class AdvancedOptimizedDataset:
    """é«˜çº§ä¼˜åŒ–æ•°æ®é›†"""
    def __init__(self, config, split='train'):
        self.config = config
        self.split = split
        self.data = []
        self.labels = []
        self.vocab = self._build_vocab()
        self.augmentor = SmartDataAugmentor(config)
        
        self._load_data()
        
    def _build_vocab(self):
        """æ„å»ºè¯æ±‡è¡¨"""
        vocab = {
            '<PAD>': 0, '<UNK>': 1,
            'è¯·': 2, 'è°¢è°¢': 3, 'ä½ å¥½': 4, 'å†è§': 5,
            'å¥½çš„': 6, 'æ˜¯çš„': 7, 'æˆ‘': 8, 'ä¸æ˜¯': 9
        }
        logger.info(f"è¯æ±‡è¡¨æ„å»ºå®Œæˆ: {list(vocab.keys())}")
        logger.info(f"è¯æ±‡è¡¨å¤§å°: {len(vocab)}")
        return vocab
    
    def _create_enhanced_mock_data(self):
        """åˆ›å»ºå¢å¼ºçš„æ¨¡æ‹Ÿæ•°æ®"""
        logger.info("åˆ›å»ºå¢å¼ºæ¨¡æ‹Ÿæ•°æ®...")
        
        vocab_list = list(self.vocab.keys())[2:]
        base_samples = 25 if self.split == 'train' else 4
        
        for word in vocab_list:
            for i in range(base_samples):
                seq_len = np.random.randint(20, 50)
                
                # åŸºç¡€å™ªå£°
                keypoints = np.random.randn(seq_len, self.config.input_dim).astype(np.float32) * 0.08
                
                # ç±»åˆ«ç‰¹å®šçš„å¤æ‚æ¨¡å¼
                class_id = self.vocab[word]
                
                # å¤šé¢‘ç‡ç»„åˆæ¨¡å¼
                t = np.linspace(0, 6*np.pi, seq_len)
                
                # åŸºç¡€é¢‘ç‡æ¨¡å¼
                freq1 = class_id * 0.5 + 1
                pattern1 = np.sin(t * freq1) * 0.4
                keypoints[:, 0] += pattern1
                
                # äºŒæ¬¡è°æ³¢
                freq2 = freq1 * 2
                pattern2 = np.sin(t * freq2) * 0.2
                keypoints[:, 1] += pattern2
                
                # ç›¸ä½åç§»æ¨¡å¼
                phase_shift = class_id * np.pi / 4
                pattern3 = np.cos(t * freq1 + phase_shift) * 0.3
                keypoints[:, 2] += pattern3
                
                # å¢é•¿/è¡°å‡æ¨¡å¼
                growth_pattern = np.exp(-t / 10) * np.sin(t * freq1) * class_id * 0.1
                keypoints[:, 3] += growth_pattern
                
                # éšæœºæ¸¸èµ°with drift
                drift = class_id * 0.05
                walk = np.cumsum(np.random.randn(seq_len) * 0.01 + drift / seq_len)
                keypoints[:, 4] += walk
                
                # è„‰å†²æ¨¡å¼
                pulse_positions = np.random.choice(seq_len, max(1, seq_len // 10), replace=False)
                for pos in pulse_positions:
                    keypoints[pos, 5:10] += class_id * 0.3
                
                # ç©ºé—´ç›¸å…³æ€§
                for dim in range(10, min(30, self.config.input_dim)):
                    correlation_source = dim % 5
                    correlation_strength = 0.3 + (class_id % 3) * 0.2
                    keypoints[:, dim] += keypoints[:, correlation_source] * correlation_strength
                
                # å‘¨æœŸæ€§burst
                burst_period = max(5, seq_len // (class_id + 1))
                for burst_start in range(0, seq_len, burst_period):
                    burst_end = min(burst_start + burst_period // 3, seq_len)
                    keypoints[burst_start:burst_end, 30:40] += class_id * 0.2
                
                self.data.append(keypoints)
                self.labels.append(class_id)
        
        # æ™ºèƒ½æ•°æ®å¢å¼º
        if self.split == 'train':
            original_data = list(zip(self.data, self.labels))
            self.data = []
            self.labels = []
            
            for seq, label in original_data:
                label_text = next(k for k, v in self.vocab.items() if v == label)
                augmented = self.augmentor.augment_sequence(seq, label_text, self.config.augmentation_factor)
                
                for aug_seq, aug_label in augmented:
                    self.data.append(aug_seq)
                    self.labels.append(self.vocab[aug_label])
        
        logger.info(f"å¢å¼ºæ¨¡æ‹Ÿæ•°æ®åˆ›å»ºå®Œæˆ - {self.split}é›†: {len(self.data)} æ ·æœ¬")
    
    def _load_data(self):
        """åŠ è½½æ•°æ®"""
        data_path = Path(self.config.data_dir) / f"{self.split}.json"
        
        if not data_path.exists():
            logger.warning(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
            self._create_enhanced_mock_data()
            return
            
        try:
            with open(data_path, 'r', encoding='utf-8') as f:
                raw_data = json.load(f)
        except Exception as e:
            logger.warning(f"åŠ è½½æ•°æ®å¤±è´¥: {e}ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
            self._create_enhanced_mock_data()
            return
            
        logger.info(f"ğŸ“Š åŠ è½½æ•°æ®...")
        # å®é™…æ•°æ®å¤„ç†é€»è¾‘...
        self._create_enhanced_mock_data()  # æš‚æ—¶ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        sequence = self.data[idx]
        label = self.labels[idx]
        
        # åŠ¨æ€é•¿åº¦å¤„ç†
        if len(sequence) > self.config.max_frames:
            # æ™ºèƒ½é‡‡æ ·è€Œéç®€å•æˆªæ–­
            indices = np.linspace(0, len(sequence)-1, self.config.max_frames).astype(int)
            sequence = sequence[indices]
        else:
            padding = np.zeros((self.config.max_frames - len(sequence), self.config.input_dim))
            sequence = np.vstack([sequence, padding])
        
        return sequence.astype(np.float32), np.array(label, dtype=np.int32)

def create_dataset(config, split='train'):
    """åˆ›å»ºæ•°æ®é›†"""
    dataset = AdvancedOptimizedDataset(config, split)
    
    def generator():
        for i in range(len(dataset)):
            yield dataset[i]
    
    column_names = ["sequence", "label"]
    ms_dataset = GeneratorDataset(generator, column_names=column_names, shuffle=(split=='train'))
    ms_dataset = ms_dataset.batch(config.batch_size, drop_remainder=False)
    
    return ms_dataset, dataset.vocab

class AdvancedOptimizedTrainer:
    """é«˜çº§ä¼˜åŒ–è®­ç»ƒå™¨"""
    def __init__(self, config):
        self.config = config
        
        # è®¾ç½®MindSpore
        ms.set_context(mode=ms.PYNATIVE_MODE, device_target=config.device)
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(config.output_dir, exist_ok=True)
        
        logger.info("é«˜çº§ä¼˜åŒ–CE-CSLè®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ - è®¾å¤‡: {}".format(config.device))
        
        # åŠ è½½æ•°æ®
        self.train_dataset, self.vocab = create_dataset(config, 'train')
        self.val_dataset, _ = create_dataset(config, 'dev')
        
        # æ„å»ºé›†æˆæ¨¡å‹
        logger.info("ğŸ§  æ„å»ºé«˜çº§ä¼˜åŒ–æ¨¡å‹é›†æˆ...")
        self.models = []
        self.optimizers = []
        
        for i in range(config.ensemble_models):
            model = AdvancedOptimizedModel(config)
            
            # ä¸åŒçš„åˆå§‹åŒ–ç­–ç•¥
            if i == 1:
                # ç¬¬äºŒä¸ªæ¨¡å‹ä½¿ç”¨ä¸åŒçš„åˆå§‹åŒ–
                for param in model.trainable_params():
                    if len(param.shape) > 1:
                        param.set_data(ms.Tensor(np.random.normal(0, 0.02, param.shape).astype(np.float32)))
            elif i == 2:
                # ç¬¬ä¸‰ä¸ªæ¨¡å‹ä½¿ç”¨Xavieråˆå§‹åŒ–
                for param in model.trainable_params():
                    if len(param.shape) > 1:
                        fan_in = param.shape[0]
                        fan_out = param.shape[1] if len(param.shape) > 1 else param.shape[0]
                        std = math.sqrt(2.0 / (fan_in + fan_out))
                        param.set_data(ms.Tensor(np.random.normal(0, std, param.shape).astype(np.float32)))
            
            optimizer = nn.Adam(
                model.trainable_params(),
                learning_rate=config.initial_learning_rate,
                weight_decay=config.weight_decay,
                beta1=0.9,
                beta2=0.999
            )
            
            self.models.append(model)
            self.optimizers.append(optimizer)
        
        total_params = sum(sum(p.size for p in model.trainable_params()) for model in self.models)
        logger.info(f"é›†æˆæ¨¡å‹æ„å»ºå®Œæˆ - æ€»å‚æ•°é‡: {total_params}")
        
        # æŸå¤±å‡½æ•°
        if config.use_focal_loss:
            self.loss_fn = FocalLoss(config.focal_alpha, config.focal_gamma, config.num_classes)
        elif config.use_label_smoothing:
            self.loss_fn = LabelSmoothingLoss(config.num_classes, config.label_smoothing)
        else:
            self.loss_fn = nn.CrossEntropyLoss()
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.lr_scheduler = CosineAnnealingLR(
            config.initial_learning_rate,
            config.min_learning_rate,
            config.restart_epochs,
            config.warmup_epochs
        )
        
        # è®­ç»ƒçŠ¶æ€
        self.best_val_acc = 0.0
        self.patience_counter = 0
        self.training_history = []
        
        logger.info("ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°åˆ›å»ºå®Œæˆ")
    
    def mixup_data(self, x, y, alpha=0.2):
        """Mixupæ•°æ®å¢å¼º"""
        if alpha > 0:
            lam = np.random.beta(alpha, alpha)
        else:
            lam = 1
        
        batch_size = x.shape[0]
        index = ops.Randperm(max_length=batch_size, dtype=ms.int32)(batch_size)
        
        mixed_x = lam * x + (1 - lam) * x[index]
        y_a, y_b = y, y[index]
        return mixed_x, y_a, y_b, lam
    
    def mixup_criterion(self, pred, y_a, y_b, lam):
        """MixupæŸå¤±è®¡ç®—"""
        return lam * self.loss_fn(pred, y_a) + (1 - lam) * self.loss_fn(pred, y_b)
    
    def forward_fn(self, model_idx, data, label):
        """å‰å‘ä¼ æ’­å‡½æ•°"""
        logits = self.models[model_idx](data)
        loss = self.loss_fn(logits, label)
        return loss, logits
    
    def train_step(self, model_idx, data, label, use_mixup=False):
        """å•æ­¥è®­ç»ƒ"""
        if use_mixup and self.config.use_mixup:
            mixed_data, y_a, y_b, lam = self.mixup_data(data, label, self.config.mixup_alpha)
            
            def mixup_forward_fn(mixed_data, y_a, y_b):
                logits = self.models[model_idx](mixed_data)
                loss = self.mixup_criterion(logits, y_a, y_b, lam)
                return loss, logits
            
            grad_fn = ms.value_and_grad(mixup_forward_fn, None, self.optimizers[model_idx].parameters, has_aux=True)
            (loss, logits), grads = grad_fn(mixed_data, y_a, y_b)
        else:
            grad_fn = ms.value_and_grad(self.forward_fn, None, self.optimizers[model_idx].parameters, has_aux=True)
            (loss, logits), grads = grad_fn(model_idx, data, label)
        
        # æ¢¯åº¦è£å‰ª
        if self.config.gradient_clip_norm > 0:
            grads = ops.clip_by_global_norm(grads, self.config.gradient_clip_norm)
        
        self.optimizers[model_idx](grads)
        return loss, logits
    
    def ensemble_predict(self, data):
        """é›†æˆé¢„æµ‹"""
        all_logits = []
        for model in self.models:
            model.set_train(False)
            logits = model(data)
            all_logits.append(logits)
        
        # å¹³å‡é¢„æµ‹
        ensemble_logits = ops.Stack()(all_logits).mean(axis=0)
        return ensemble_logits
    
    def evaluate(self, dataset):
        """è¯„ä¼°æ¨¡å‹"""
        for model in self.models:
            model.set_train(False)
        
        total_loss = 0.0
        correct_predictions = 0
        total_samples = 0
        class_correct = defaultdict(int)
        class_total = defaultdict(int)
        
        id_to_label = {v: k for k, v in self.vocab.items()}
        
        for batch in dataset:
            data, labels = batch
            
            # é›†æˆé¢„æµ‹
            ensemble_logits = self.ensemble_predict(data)
            loss = self.loss_fn(ensemble_logits, labels)
            
            total_loss += loss.asnumpy()
            predictions = ops.Argmax(axis=1)(ensemble_logits)
            
            for i in range(len(labels)):
                pred = int(predictions[i].asnumpy())
                true = int(labels[i].asnumpy())
                
                total_samples += 1
                if pred == true:
                    correct_predictions += 1
                    class_correct[id_to_label[true]] += 1
                class_total[id_to_label[true]] += 1
        
        avg_loss = total_loss / len(dataset) if len(dataset) > 0 else 0
        accuracy = correct_predictions / total_samples if total_samples > 0 else 0
        
        # æ‰“å°è¯¦ç»†çš„ç±»åˆ«ç»Ÿè®¡
        logger.info("å„ç±»åˆ«å‡†ç¡®ç‡:")
        for label in sorted(class_total.keys()):
            if label not in ['<PAD>', '<UNK>']:
                correct = class_correct.get(label, 0)
                total = class_total[label]
                class_acc = correct / total if total > 0 else 0
                logger.info(f"  {label}: {class_acc:.4f} ({correct}/{total})")
        
        for model in self.models:
            model.set_train(True)
        
        return avg_loss, accuracy
    
    def update_learning_rates(self, epoch):
        """æ›´æ–°å­¦ä¹ ç‡"""
        new_lr = self.lr_scheduler.get_lr(epoch)
        for optimizer in self.optimizers:
            optimizer.learning_rate.set_data(ms.Tensor(new_lr, ms.float32))
        return new_lr
    
    def train(self):
        """è®­ç»ƒä¸»å¾ªç¯"""
        logger.info("ğŸ¯ å¼€å§‹é«˜çº§ä¼˜åŒ–è®­ç»ƒ...")
        
        for epoch in range(1, self.config.epochs + 1):
            epoch_start_time = time.time()
            
            # æ›´æ–°å­¦ä¹ ç‡
            current_lr = self.update_learning_rates(epoch - 1)
            logger.info(f"å¼€å§‹ç¬¬ {epoch}/{self.config.epochs} è½®è®­ç»ƒ... LR: {current_lr:.6f}")
            
            # è®­ç»ƒé˜¶æ®µ
            for model in self.models:
                model.set_train(True)
            
            total_loss = 0.0
            correct_predictions = 0
            total_samples = 0
            batch_count = 0
            
            for batch in self.train_dataset:
                data, labels = batch
                
                # å¯¹æ¯ä¸ªæ¨¡å‹è¿›è¡Œè®­ç»ƒ
                batch_losses = []
                ensemble_logits_list = []
                
                for model_idx in range(self.config.ensemble_models):
                    use_mixup = (epoch > self.config.warmup_epochs) and np.random.random() < 0.5
                    loss, logits = self.train_step(model_idx, data, labels, use_mixup)
                    batch_losses.append(loss.asnumpy())
                    ensemble_logits_list.append(logits)
                
                # é›†æˆé¢„æµ‹ç”¨äºç»Ÿè®¡
                ensemble_logits = ops.Stack()(ensemble_logits_list).mean(axis=0)
                predictions = ops.Argmax(axis=1)(ensemble_logits)
                
                # ç»Ÿè®¡
                avg_batch_loss = np.mean(batch_losses)
                total_loss += avg_batch_loss
                
                for i in range(len(labels)):
                    if int(predictions[i].asnumpy()) == int(labels[i].asnumpy()):
                        correct_predictions += 1
                    total_samples += 1
                
                batch_count += 1
                
                # å®šæœŸè¾“å‡ºè¿›åº¦
                if batch_count % 30 == 0:
                    current_acc = correct_predictions / total_samples if total_samples > 0 else 0
                    logger.info(f"æ‰¹æ¬¡ {batch_count}: Loss = {avg_batch_loss:.4f}, å‡†ç¡®ç‡ = {current_acc:.4f}")
            
            # è®¡ç®—è®­ç»ƒæŒ‡æ ‡
            avg_train_loss = total_loss / batch_count if batch_count > 0 else 0
            train_accuracy = correct_predictions / total_samples if total_samples > 0 else 0
            
            logger.info(f"Epoch {epoch} è®­ç»ƒå®Œæˆ:")
            logger.info(f"  å¹³å‡æŸå¤±: {avg_train_loss:.4f}")
            logger.info(f"  è®­ç»ƒå‡†ç¡®ç‡: {train_accuracy:.4f}")
            
            # éªŒè¯é˜¶æ®µ
            logger.info("å¼€å§‹é›†æˆæ¨¡å‹è¯„ä¼°...")
            val_loss, val_accuracy = self.evaluate(self.val_dataset)
            
            logger.info("è¯„ä¼°å®Œæˆ:")
            logger.info(f"  éªŒè¯æŸå¤±: {val_loss:.4f}")
            logger.info(f"  éªŒè¯å‡†ç¡®ç‡: {val_accuracy:.4f}")
            
            # è®°å½•å†å²
            epoch_time = time.time() - epoch_start_time
            epoch_record = {
                'epoch': epoch,
                'train_loss': float(avg_train_loss),
                'train_accuracy': float(train_accuracy),
                'val_loss': float(val_loss),
                'val_accuracy': float(val_accuracy),
                'learning_rate': float(current_lr),
                'epoch_time': epoch_time
            }
            self.training_history.append(epoch_record)
            
            logger.info(f"Epoch {epoch} æ€»ç»“:")
            logger.info(f"  è®­ç»ƒ: Loss={avg_train_loss:.4f}, Acc={train_accuracy:.4f}")
            logger.info(f"  éªŒè¯: Loss={val_loss:.4f}, Acc={val_accuracy:.4f}")
            logger.info(f"  å­¦ä¹ ç‡: {current_lr:.6f}, è€—æ—¶: {epoch_time:.2f}ç§’")
            
            # æ—©åœæ£€æŸ¥
            if val_accuracy > self.best_val_acc + self.config.min_improvement:
                self.best_val_acc = val_accuracy
                self.patience_counter = 0
                logger.info(f"ğŸ‰ æ–°çš„æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {self.best_val_acc:.4f}")
                
                # ä¿å­˜æœ€ä½³æ¨¡å‹é›†æˆ
                for i, model in enumerate(self.models):
                    model_path = self.config.model_save_path.replace('.ckpt', f'_model_{i}.ckpt')
                    ms.save_checkpoint(model, model_path)
                logger.info("æœ€ä½³æ¨¡å‹é›†æˆå·²ä¿å­˜!")
            else:
                self.patience_counter += 1
                logger.info(f"éªŒè¯å‡†ç¡®ç‡æœªæå‡ï¼Œè€å¿ƒè®¡æ•°: {self.patience_counter}/{self.config.patience}")
                
                if self.patience_counter >= self.config.patience:
                    logger.info(f"æ—©åœè§¦å‘ï¼Œåœ¨ç¬¬ {epoch} è½®åœæ­¢è®­ç»ƒ")
                    break
        
        logger.info(f"è®­ç»ƒå®Œæˆ! æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {self.best_val_acc:.4f}")
        
        # ä¿å­˜æœ€ç»ˆç»“æœ
        self.save_final_results()
    
    def save_final_results(self):
        """ä¿å­˜æœ€ç»ˆç»“æœ"""
        print("ğŸ’¾ ä¿å­˜æœ€ç»ˆæ¨¡å‹...")
        
        # ä¿å­˜è¯æ±‡è¡¨
        with open(self.config.vocab_save_path, 'w', encoding='utf-8') as f:
            json.dump(self.vocab, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜è®­ç»ƒå†å²
        with open(self.config.history_save_path, 'w', encoding='utf-8') as f:
            json.dump(self.training_history, f, indent=2)
        
        logger.info("âœ… é«˜çº§ä¼˜åŒ–æ¨¡å‹å’Œè®°å½•ä¿å­˜å®Œæˆ")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ é«˜çº§ä¼˜åŒ–CE-CSLæ‰‹è¯­è¯†åˆ«è®­ç»ƒå¯åŠ¨")
    print("ğŸ¯ ä¸“ä¸šçº§å‡†ç¡®ç‡æå‡æ–¹æ¡ˆ:")
    print("  âœ“ é›†æˆå­¦ä¹  - 3ä¸ªæ¨¡å‹æŠ•ç¥¨")
    print("  âœ“ è‡ªæ³¨æ„åŠ›æœºåˆ¶ - æ•è·æ—¶åºä¾èµ–")
    print("  âœ“ æ™ºèƒ½æ•°æ®å¢å¼º - 15ç§å¢å¼ºæŠ€æœ¯")
    print("  âœ“ ä½™å¼¦é€€ç«å­¦ä¹ ç‡ - è‡ªé€‚åº”ä¼˜åŒ–")
    print("  âœ“ Focal Loss - è§£å†³ç±»åˆ«ä¸å¹³è¡¡")
    print("  âœ“ Mixupå¢å¼º - æå‡æ³›åŒ–èƒ½åŠ›")
    print("  âœ“ æ¢¯åº¦è£å‰ª - ç¨³å®šè®­ç»ƒè¿‡ç¨‹")
    
    # åˆ›å»ºé…ç½®
    config = AdvancedOptimizedConfig()
    
    print("ğŸ“Š é«˜çº§é…ç½®:")
    print(f"  - è®­ç»ƒè½®æ•°: {config.epochs}")
    print(f"  - æ‰¹æ¬¡å¤§å°: {config.batch_size}")
    print(f"  - åˆå§‹å­¦ä¹ ç‡: {config.initial_learning_rate}")
    print(f"  - æœ€å°å­¦ä¹ ç‡: {config.min_learning_rate}")
    print(f"  - éšè—ç»´åº¦: {config.hidden_dim}")
    print(f"  - æ³¨æ„åŠ›ç»´åº¦: {config.attention_dim}")
    print(f"  - é›†æˆæ¨¡å‹æ•°: {config.ensemble_models}")
    print(f"  - æ•°æ®å¢å¼ºå€æ•°: {config.augmentation_factor}")
    print(f"  - Mixup Alpha: {config.mixup_alpha}")
    print(f"  - Label Smoothing: {config.label_smoothing}")
    
    # åˆ›å»ºè®­ç»ƒå™¨å¹¶å¼€å§‹è®­ç»ƒ
    trainer = AdvancedOptimizedTrainer(config)
    trainer.train()
    
    print("ğŸ‰ é«˜çº§ä¼˜åŒ–è®­ç»ƒå®Œæˆï¼")
    print(f"ğŸ“ æ¨¡å‹é›†æˆå·²ä¿å­˜åˆ°: {config.output_dir}")
    print(f"ğŸ† æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {trainer.best_val_acc:.4f}")
    print("âœ¨ ä¸“ä¸šçº§æ”¹è¿›:")
    print("  âœ“ é›†æˆå­¦ä¹ æå‡é²æ£’æ€§")
    print("  âœ“ æ³¨æ„åŠ›æœºåˆ¶å¢å¼ºç‰¹å¾è¡¨è¾¾")
    print("  âœ“ æ™ºèƒ½å¢å¼ºä¸°å¯Œè®­ç»ƒæ•°æ®")
    print("  âœ“ è‡ªé€‚åº”å­¦ä¹ ç‡ä¼˜åŒ–æ”¶æ•›")
    print("  âœ“ å…ˆè¿›æŸå¤±å‡½æ•°å¤„ç†ä¸å¹³è¡¡")

if __name__ == "__main__":
    main()
