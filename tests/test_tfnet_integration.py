"""
TFNeté›†æˆæµ‹è¯•è„šæœ¬
éªŒè¯MindSpore TFNetå®ç°çš„åŸºæœ¬åŠŸèƒ½
"""

import os
import sys
import json
import numpy as np
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))
sys.path.append(str(project_root / "training"))

def test_imports():
    """æµ‹è¯•æ¨¡å—å¯¼å…¥"""
    print("=" * 50)
    print("æµ‹è¯•æ¨¡å—å¯¼å…¥...")
    
    try:
        import mindspore as ms
        print(f"âœ“ MindSpore {ms.__version__} å¯¼å…¥æˆåŠŸ")
    except ImportError as e:
        print(f"âœ— MindSpore å¯¼å…¥å¤±è´¥: {e}")
        return False
    
    try:
        from tfnet_mindspore import TFNetMindSpore, SeqKD
        print("âœ“ TFNet MindSpore æ¨¡å‹å¯¼å…¥æˆåŠŸ")
    except ImportError as e:
        print(f"âœ— TFNet æ¨¡å‹å¯¼å…¥å¤±è´¥: {e}")
        return False
    
    try:
        from cecsl_data_processor import CECSLLabelProcessor, CECSLVideoProcessor
        print("âœ“ CE-CSL æ•°æ®å¤„ç†å™¨å¯¼å…¥æˆåŠŸ")
    except ImportError as e:
        print(f"âœ— æ•°æ®å¤„ç†å™¨å¯¼å…¥å¤±è´¥: {e}")
        return False
    
    try:
        from tfnet_decoder import CTCDecoder, WERCalculator
        print("âœ“ TFNet è§£ç å™¨å¯¼å…¥æˆåŠŸ")
    except ImportError as e:
        print(f"âœ— è§£ç å™¨å¯¼å…¥å¤±è´¥: {e}")
        # è¿™ä¸ªå¯èƒ½ä¼šå› ä¸ºeditdistanceåŒ…è€Œå¤±è´¥ï¼Œä½†ä¸æ˜¯è‡´å‘½é”™è¯¯
        print("  æ³¨æ„: å¯èƒ½éœ€è¦å®‰è£… editdistance åŒ…: pip install editdistance")
    
    return True

def test_model_creation():
    """æµ‹è¯•æ¨¡å‹åˆ›å»º"""
    print("=" * 50)
    print("æµ‹è¯•æ¨¡å‹åˆ›å»º...")
    
    try:
        from tfnet_mindspore import TFNetMindSpore
        import mindspore as ms
        
        # è®¾ç½®ä¸Šä¸‹æ–‡
        ms.context.set_context(mode=ms.context.GRAPH_MODE, device_target="CPU")
        
        # åˆ›å»ºæ¨¡å‹
        model = TFNetMindSpore(
            hidden_size=512,
            vocab_size=1000,
            module_choice="TFNet",
            dataset_name="CE-CSL"
        )
        
        print("âœ“ TFNet æ¨¡å‹åˆ›å»ºæˆåŠŸ")
        
        # æµ‹è¯•å‰å‘ä¼ æ’­
        batch_size, seq_len, height, width, channels = 2, 10, 224, 224, 3
        dummy_input = ms.Tensor(np.random.randn(batch_size, seq_len, height, width, channels), ms.float32)
        dummy_lengths = ms.Tensor([8, 6], ms.int32)
        
        outputs = model(dummy_input, dummy_lengths, is_train=False)
        print(f"âœ“ æ¨¡å‹å‰å‘ä¼ æ’­æˆåŠŸï¼Œè¾“å‡ºæ•°é‡: {len(outputs)}")
        
        return True
        
    except Exception as e:
        print(f"âœ— æ¨¡å‹åˆ›å»ºå¤±è´¥: {e}")
        return False

def test_data_processor():
    """æµ‹è¯•æ•°æ®å¤„ç†å™¨"""
    print("=" * 50)
    print("æµ‹è¯•æ•°æ®å¤„ç†å™¨...")
    
    try:
        from cecsl_data_processor import CECSLLabelProcessor
        
        # åˆ›å»ºç¤ºä¾‹è¯æ±‡è¡¨
        processor = CECSLLabelProcessor()
        
        # æµ‹è¯•è¯æ±‡é¢„å¤„ç†
        test_words = ["ä½ å¥½(1)", "ä¸–ç•Œ", "æ‰‹è¯­è¯†åˆ«", "æµ‹è¯•123"]
        processed = processor.preprocess_words(test_words)
        print(f"âœ“ è¯æ±‡é¢„å¤„ç†æˆåŠŸ: {test_words} -> {processed}")
        
        # åˆ›å»ºæµ‹è¯•è¯æ±‡è¡¨
        processor.word2idx = {' ': 0, 'ä½ å¥½': 1, 'ä¸–ç•Œ': 2, 'æ‰‹è¯­': 3, 'è¯†åˆ«': 4}
        processor.idx2word = [' ', 'ä½ å¥½', 'ä¸–ç•Œ', 'æ‰‹è¯­', 'è¯†åˆ«']
        
        # ä¿å­˜è¯æ±‡è¡¨
        test_vocab_dir = project_root / "temp"
        test_vocab_dir.mkdir(exist_ok=True)
        test_vocab_file = test_vocab_dir / "test_vocab.json"
        
        processor.save_vocabulary(str(test_vocab_file))
        print(f"âœ“ è¯æ±‡è¡¨ä¿å­˜æˆåŠŸ: {test_vocab_file}")
        
        # åŠ è½½è¯æ±‡è¡¨
        new_processor = CECSLLabelProcessor()
        new_processor.load_vocabulary(str(test_vocab_file))
        print("âœ“ è¯æ±‡è¡¨åŠ è½½æˆåŠŸ")
        
        # æ¸…ç†æµ‹è¯•æ–‡ä»¶
        test_vocab_file.unlink()
        
        return True
        
    except Exception as e:
        print(f"âœ— æ•°æ®å¤„ç†å™¨æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_decoder():
    """æµ‹è¯•è§£ç å™¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰"""
    print("=" * 50)
    print("æµ‹è¯•CTCè§£ç å™¨...")
    
    try:
        from tfnet_decoder import CTCDecoder, WERCalculator
        
        # åˆ›å»ºæµ‹è¯•è¯æ±‡è¡¨æ–‡ä»¶
        test_vocab_dir = project_root / "temp"
        test_vocab_dir.mkdir(exist_ok=True)
        test_vocab_file = test_vocab_dir / "test_vocab.json"
        
        test_vocab = {
            'word2idx': {' ': 0, 'ä½ å¥½': 1, 'ä¸–ç•Œ': 2, 'æ‰‹è¯­': 3, 'è¯†åˆ«': 4},
            'idx2word': [' ', 'ä½ å¥½', 'ä¸–ç•Œ', 'æ‰‹è¯­', 'è¯†åˆ«'],
            'vocab_size': 5
        }
        
        with open(test_vocab_file, 'w', encoding='utf-8') as f:
            json.dump(test_vocab, f, ensure_ascii=False)
        
        # åˆ›å»ºè§£ç å™¨
        decoder = CTCDecoder(str(test_vocab_file), blank_id=0)
        print("âœ“ CTCè§£ç å™¨åˆ›å»ºæˆåŠŸ")
        
        # æµ‹è¯•è´ªå©ªè§£ç 
        log_probs = np.random.randn(10, 1, 5)  # (seq_len, batch_size, vocab_size)
        input_lengths = np.array([8])
        
        greedy_results = decoder.greedy_decode(log_probs, input_lengths)
        print(f"âœ“ è´ªå©ªè§£ç æˆåŠŸ: {greedy_results}")
        
        # æµ‹è¯•WERè®¡ç®—å™¨
        wer_calc = WERCalculator()
        references = [['ä½ å¥½', 'ä¸–ç•Œ'], ['æ‰‹è¯­', 'è¯†åˆ«']]
        hypotheses = [['ä½ å¥½', 'ä¸–ç•Œ'], ['æ‰‹è¯­']]
        
        wer_results = wer_calc.compute_wer(references, hypotheses)
        print(f"âœ“ WERè®¡ç®—æˆåŠŸ: {wer_results['wer']:.2f}%")
        
        # æ¸…ç†æµ‹è¯•æ–‡ä»¶
        test_vocab_file.unlink()
        
        return True
        
    except ImportError:
        print("âš  è§£ç å™¨æµ‹è¯•è·³è¿‡ï¼ˆeditdistance åŒ…æœªå®‰è£…ï¼‰")
        return True
    except Exception as e:
        print(f"âœ— è§£ç å™¨æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_service_integration():
    """æµ‹è¯•æœåŠ¡é›†æˆ"""
    print("=" * 50)
    print("æµ‹è¯•æœåŠ¡é›†æˆ...")
    
    try:
        # æ£€æŸ¥æœåŠ¡æ–‡ä»¶
        service_file = project_root / "backend" / "services" / "diffusion_slp_service.py"
        if service_file.exists():
            print("âœ“ æœåŠ¡æ–‡ä»¶å­˜åœ¨")
            
            # å°è¯•å¯¼å…¥æœåŠ¡
            sys.path.append(str(project_root / "backend"))
            from services.diffusion_slp_service import DiffusionSLPService
            print("âœ“ æœåŠ¡ç±»å¯¼å…¥æˆåŠŸ")
            
            return True
        else:
            print("âœ— æœåŠ¡æ–‡ä»¶ä¸å­˜åœ¨")
            return False
            
    except Exception as e:
        print(f"âœ— æœåŠ¡é›†æˆæµ‹è¯•å¤±è´¥: {e}")
        return False

def test_config_files():
    """æµ‹è¯•é…ç½®æ–‡ä»¶"""
    print("=" * 50)
    print("æµ‹è¯•é…ç½®æ–‡ä»¶...")
    
    config_file = project_root / "training" / "configs" / "tfnet_cecsl_config.json"
    
    if not config_file.exists():
        print(f"âœ— é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_file}")
        return False
    
    try:
        with open(config_file, 'r', encoding='utf-8') as f:
            config = json.load(f)
        
        print("âœ“ é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ")
        
        # æ£€æŸ¥å¿…è¦çš„é…ç½®é¡¹
        required_sections = ['model_config', 'training_config', 'hardware_config', 'paths']
        for section in required_sections:
            if section in config:
                print(f"âœ“ é…ç½®æ®µ '{section}' å­˜åœ¨")
            else:
                print(f"âœ— é…ç½®æ®µ '{section}' ç¼ºå¤±")
                return False
        
        return True
        
    except Exception as e:
        print(f"âœ— é…ç½®æ–‡ä»¶æµ‹è¯•å¤±è´¥: {e}")
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("TFNet MindSpore é›†æˆæµ‹è¯•")
    print("=" * 50)
    
    tests = [
        ("æ¨¡å—å¯¼å…¥", test_imports),
        ("æ¨¡å‹åˆ›å»º", test_model_creation),
        ("æ•°æ®å¤„ç†å™¨", test_data_processor),
        ("è§£ç å™¨", test_decoder),
        ("æœåŠ¡é›†æˆ", test_service_integration),
        ("é…ç½®æ–‡ä»¶", test_config_files),
    ]
    
    results = []
    
    for test_name, test_func in tests:
        try:
            result = test_func()
            results.append((test_name, result))
        except Exception as e:
            print(f"âœ— æµ‹è¯• '{test_name}' å¼‚å¸¸: {e}")
            results.append((test_name, False))
    
    # æ€»ç»“
    print("=" * 50)
    print("æµ‹è¯•æ€»ç»“:")
    print("=" * 50)
    
    passed = 0
    total = len(results)
    
    for test_name, result in results:
        status = "âœ“ é€šè¿‡" if result else "âœ— å¤±è´¥"
        print(f"{test_name:20s} {status}")
        if result:
            passed += 1
    
    print("=" * 50)
    print(f"æ€»è®¡: {passed}/{total} ä¸ªæµ‹è¯•é€šè¿‡")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼TFNeté›†æˆæˆåŠŸã€‚")
        return True
    else:
        print("âš  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç›¸å…³é—®é¢˜ã€‚")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
