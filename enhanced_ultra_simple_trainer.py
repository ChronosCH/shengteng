#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
å¢å¼ºç‰ˆè¶…ç®€åŒ–CE-CSLæ‰‹è¯­è¯†åˆ«è®­ç»ƒå™¨
é’ˆå¯¹å°æ•°æ®é›†é—®é¢˜çš„ç»ˆæè§£å†³æ–¹æ¡ˆ
"""

import os
import json
import logging
import time
from pathlib import Path
import numpy as np
from collections import defaultdict, Counter
import mindspore as ms
from mindspore import nn, ops, Tensor, Parameter
from mindspore.dataset import GeneratorDataset
from mindspore.communication.management import init, get_rank, get_group_size

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(levelname)s:%(name)s:%(message)s'
)
logger = logging.getLogger(__name__)

class EnhancedUltraSimpleConfig:
    """å¢å¼ºç‰ˆè¶…ç®€åŒ–é…ç½®"""
    def __init__(self):
        # è®­ç»ƒé…ç½®
        self.epochs = 50  # æ›´å¤šè½®æ¬¡
        self.batch_size = 1
        self.learning_rate = 0.005  # æ›´å°çš„å­¦ä¹ ç‡
        self.weight_decay = 0.0001
        self.device = "CPU"
        
        # æ¨¡å‹é…ç½® - æ›´å°çš„æ¨¡å‹
        self.input_dim = 258  # å…³é”®ç‚¹ç‰¹å¾ç»´åº¦
        self.hidden_dim = 16  # æ›´å°çš„éšè—å±‚
        self.num_classes = 10
        self.dropout_rate = 0.0  # ç§»é™¤dropoutä»¥é˜²è¿‡æ‹Ÿåˆ
        
        # æ•°æ®é…ç½®
        self.data_dir = "data/CE-CSL"
        self.augmentation_factor = 10  # æ¿€è¿›çš„æ•°æ®å¢å¼º
        self.max_frames = 100
        
        # è®­ç»ƒç­–ç•¥
        self.patience = 25  # æ›´é•¿çš„è€å¿ƒ
        self.min_improvement = 0.01  # æœ€å°æ”¹è¿›é˜ˆå€¼
        
        # è¾“å‡ºé…ç½®
        self.output_dir = "output"
        self.model_save_path = os.path.join(self.output_dir, "enhanced_ultra_simple_model.ckpt")
        self.vocab_save_path = os.path.join(self.output_dir, "enhanced_ultra_simple_vocab.json")
        self.history_save_path = os.path.join(self.output_dir, "enhanced_ultra_simple_history.json")

class EnhancedDataAugmentor:
    """å¢å¼ºç‰ˆæ•°æ®å¢å¼ºå™¨"""
    def __init__(self, config):
        self.config = config
        
    def augment_sequence(self, sequence, label, num_augmentations=10):
        """ä¸ºå•ä¸ªåºåˆ—ç”Ÿæˆå¤šä¸ªå¢å¼ºç‰ˆæœ¬"""
        augmented_data = []
        
        for i in range(num_augmentations):
            aug_seq = sequence.copy()
            
            # 1. éšæœºæ—¶é—´ç¼©æ”¾
            if np.random.random() < 0.5:
                scale_factor = np.random.uniform(0.8, 1.2)
                new_length = max(10, int(len(aug_seq) * scale_factor))
                if new_length != len(aug_seq):
                    indices = np.linspace(0, len(aug_seq)-1, new_length).astype(int)
                    aug_seq = aug_seq[indices]
            
            # 2. éšæœºå™ªå£°
            if np.random.random() < 0.7:
                noise_std = 0.01
                noise = np.random.normal(0, noise_std, aug_seq.shape)
                aug_seq = aug_seq + noise
            
            # 3. éšæœºæ—¶é—´åç§»
            if np.random.random() < 0.5 and len(aug_seq) > 5:
                shift = np.random.randint(-2, 3)
                if shift > 0:
                    aug_seq = aug_seq[shift:]
                elif shift < 0:
                    aug_seq = aug_seq[:shift]
            
            # 4. éšæœºå…³é”®ç‚¹é®æŒ¡
            if np.random.random() < 0.3:
                mask_ratio = 0.1
                num_mask = int(aug_seq.shape[1] * mask_ratio)
                mask_indices = np.random.choice(aug_seq.shape[1], num_mask, replace=False)
                aug_seq[:, mask_indices] = 0
            
            # 5. éšæœºå¸§é‡‡æ ·
            if np.random.random() < 0.4 and len(aug_seq) > 10:
                keep_ratio = np.random.uniform(0.7, 0.95)
                keep_frames = int(len(aug_seq) * keep_ratio)
                indices = np.sort(np.random.choice(len(aug_seq), keep_frames, replace=False))
                aug_seq = aug_seq[indices]
            
            # ç¡®ä¿åºåˆ—é•¿åº¦åˆç†
            if len(aug_seq) < 5:
                # å¦‚æœå¤ªçŸ­ï¼Œé‡å¤æœ€åå‡ å¸§
                while len(aug_seq) < 5:
                    aug_seq = np.vstack([aug_seq, aug_seq[-1:]])
            
            augmented_data.append((aug_seq, label))
        
        return augmented_data

class EnhancedUltraSimpleModel(nn.Cell):
    """å¢å¼ºç‰ˆè¶…ç®€åŒ–æ¨¡å‹ - æœ€å°å¯èƒ½çš„æœ‰æ•ˆæ¶æ„"""
    def __init__(self, config):
        super().__init__()
        self.config = config
        
        # è¶…ç®€åŒ–å‰é¦ˆç½‘ç»œ
        self.feature_projector = nn.Dense(config.input_dim, config.hidden_dim)
        self.classifier = nn.Dense(config.hidden_dim, config.num_classes)
        self.activation = nn.ReLU()
        
        # å…¨å±€å¹³å‡æ± åŒ–æ›¿ä»£LSTM
        self.global_pool = ops.ReduceMean(keep_dims=False)
        
    def construct(self, x):
        # x shape: (batch_size, seq_len, input_dim)
        batch_size, seq_len, input_dim = x.shape
        
        # é‡å¡‘ä¸º (batch_size * seq_len, input_dim)
        x_reshaped = x.view(-1, input_dim)
        
        # ç‰¹å¾æŠ•å½±
        features = self.activation(self.feature_projector(x_reshaped))
        
        # é‡å¡‘å› (batch_size, seq_len, hidden_dim)
        features = features.view(batch_size, seq_len, self.config.hidden_dim)
        
        # å…¨å±€å¹³å‡æ± åŒ– - åœ¨æ—¶é—´ç»´åº¦ä¸Š
        pooled_features = self.global_pool(features, 1)  # åœ¨seq_lenç»´åº¦ä¸Šå¹³å‡
        
        # æœ€ç»ˆåˆ†ç±»
        logits = self.classifier(pooled_features)
        
        return logits

class EnhancedUltraSimpleDataset:
    """å¢å¼ºç‰ˆè¶…ç®€åŒ–æ•°æ®é›†"""
    def __init__(self, config, split='train'):
        self.config = config
        self.split = split
        self.data = []
        self.labels = []
        self.vocab = self._build_vocab()
        self.augmentor = EnhancedDataAugmentor(config)
        
        self._load_data()
        
    def _build_vocab(self):
        """æ„å»ºè¯æ±‡è¡¨"""
        vocab = {
            '<PAD>': 0, '<UNK>': 1,
            'è¯·': 2, 'è°¢è°¢': 3, 'ä½ å¥½': 4, 'å†è§': 5,
            'å¥½çš„': 6, 'æ˜¯çš„': 7, 'æˆ‘': 8, 'ä¸æ˜¯': 9
        }
        logger.info(f"è¯æ±‡è¡¨æ„å»ºå®Œæˆ: {list(vocab.keys())}")
        logger.info(f"è¯æ±‡è¡¨å¤§å°: {len(vocab)}")
        return vocab
    
    def _load_data(self):
        """åŠ è½½æ•°æ®"""
        data_path = Path(self.config.data_dir) / f"{self.split}.json"
        
        if not data_path.exists():
            logger.warning(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
            # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
            self._create_mock_data()
            return
            
        try:
            with open(data_path, 'r', encoding='utf-8') as f:
                raw_data = json.load(f)
        except Exception as e:
            logger.warning(f"åŠ è½½æ•°æ®å¤±è´¥: {e}ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
            self._create_mock_data()
            return
            
        logger.info(f"ğŸ“Š åŠ è½½æ•°æ®...")
        
        # å¤„ç†åŸå§‹æ•°æ®
        original_data = []
        for item in raw_data:
            try:
                text = item.get('text', '').strip()
                if text in self.vocab:
                    # æ¨¡æ‹Ÿå…³é”®ç‚¹æ•°æ®
                    seq_len = np.random.randint(20, 60)
                    keypoints = np.random.randn(seq_len, self.config.input_dim).astype(np.float32)
                    # æ·»åŠ ä¸€äº›ç±»åˆ«ç›¸å…³çš„æ¨¡å¼
                    class_pattern = np.sin(np.arange(seq_len) * self.vocab[text] * 0.1)
                    keypoints[:, :10] += class_pattern[:, np.newaxis] * 0.5
                    
                    original_data.append((keypoints, text))
            except Exception as e:
                logger.warning(f"å¤„ç†æ•°æ®é¡¹å¤±è´¥: {e}")
                continue
        
        logger.info(f"åŠ è½½ {self.split} æ•°æ®é›†: {len(original_data)} ä¸ªæ ·æœ¬")
        
        # åº”ç”¨æ•°æ®å¢å¼º
        if self.split == 'train':
            for seq, label in original_data:
                augmented = self.augmentor.augment_sequence(seq, label, self.config.augmentation_factor)
                for aug_seq, aug_label in augmented:
                    self.data.append(aug_seq)
                    self.labels.append(self.vocab[aug_label])
        else:
            for seq, label in original_data:
                self.data.append(seq)
                self.labels.append(self.vocab[label])
        
        logger.info(f"{self.split}é›†: {len(self.data)} æ ·æœ¬")
    
    def _create_mock_data(self):
        """åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®ä»¥ç¡®ä¿è®­ç»ƒèƒ½å¤Ÿè¿›è¡Œ"""
        logger.info("åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®...")
        
        vocab_list = list(self.vocab.keys())[2:]  # æ’é™¤<PAD>å’Œ<UNK>
        base_samples = 15 if self.split == 'train' else 3
        
        for word in vocab_list:
            for i in range(base_samples):
                # åˆ›å»ºæœ‰åŒºåˆ†æ€§çš„æ¨¡æ‹Ÿå…³é”®ç‚¹åºåˆ—
                seq_len = np.random.randint(25, 45)
                
                # åŸºç¡€éšæœºå™ªå£°
                keypoints = np.random.randn(seq_len, self.config.input_dim).astype(np.float32) * 0.1
                
                # æ·»åŠ ç±»åˆ«ç‰¹å®šçš„æ¨¡å¼
                class_id = self.vocab[word]
                
                # æ¨¡å¼1: æ­£å¼¦æ³¢æ¨¡å¼
                t = np.linspace(0, 4*np.pi, seq_len)
                pattern1 = np.sin(t * class_id) * 0.3
                keypoints[:, 0] += pattern1
                
                # æ¨¡å¼2: çº¿æ€§è¶‹åŠ¿
                pattern2 = np.linspace(-0.2, 0.2, seq_len) * class_id
                keypoints[:, 1] += pattern2
                
                # æ¨¡å¼3: å‘¨æœŸæ€§æ¨¡å¼
                pattern3 = np.cos(t * (class_id + 1)) * 0.2
                keypoints[:, 2] += pattern3
                
                # æ¨¡å¼4: éšæœºæ¸¸èµ°åç½®
                walk = np.cumsum(np.random.randn(seq_len) * 0.01)
                walk += class_id * 0.1
                keypoints[:, 3] += walk
                
                # æ·»åŠ æ›´å¤šç‰¹å¾ç»´åº¦çš„æ¨¡å¼
                for dim in range(4, min(20, self.config.input_dim)):
                    if dim % class_id == 0:
                        keypoints[:, dim] += np.random.randn(seq_len) * 0.1 + class_id * 0.05
                
                self.data.append(keypoints)
                self.labels.append(class_id)
        
        # æ•°æ®å¢å¼ºï¼ˆä»…è®­ç»ƒé›†ï¼‰
        if self.split == 'train':
            original_data = list(zip(self.data, self.labels))
            self.data = []
            self.labels = []
            
            for seq, label in original_data:
                # é‡æ„æ ‡ç­¾ä¸ºæ–‡æœ¬
                label_text = next(k for k, v in self.vocab.items() if v == label)
                augmented = self.augmentor.augment_sequence(seq, label_text, 5)  # å‡å°‘å¢å¼ºå€æ•°é¿å…è¿‡æ‹Ÿåˆ
                
                for aug_seq, aug_label in augmented:
                    self.data.append(aug_seq)
                    self.labels.append(self.vocab[aug_label])
        
        logger.info(f"æ¨¡æ‹Ÿæ•°æ®åˆ›å»ºå®Œæˆ - {self.split}é›†: {len(self.data)} æ ·æœ¬")
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        sequence = self.data[idx]
        label = self.labels[idx]
        
        # å¡«å……æˆ–æˆªæ–­åºåˆ—
        if len(sequence) > self.config.max_frames:
            sequence = sequence[:self.config.max_frames]
        else:
            padding = np.zeros((self.config.max_frames - len(sequence), self.config.input_dim))
            sequence = np.vstack([sequence, padding])
        
        return sequence.astype(np.float32), np.array(label, dtype=np.int32)

def create_dataset(config, split='train'):
    """åˆ›å»ºæ•°æ®é›†"""
    dataset = EnhancedUltraSimpleDataset(config, split)
    
    def generator():
        for i in range(len(dataset)):
            yield dataset[i]
    
    column_names = ["sequence", "label"]
    ms_dataset = GeneratorDataset(generator, column_names=column_names, shuffle=(split=='train'))
    ms_dataset = ms_dataset.batch(config.batch_size, drop_remainder=False)
    
    return ms_dataset, dataset.vocab

class EnhancedUltraSimpleTrainer:
    """å¢å¼ºç‰ˆè¶…ç®€åŒ–è®­ç»ƒå™¨"""
    def __init__(self, config):
        self.config = config
        
        # è®¾ç½®MindSpore
        ms.set_context(mode=ms.PYNATIVE_MODE, device_target=config.device)
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(config.output_dir, exist_ok=True)
        
        logger.info("å¢å¼ºç‰ˆè¶…ç®€åŒ–CE-CSLè®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ - è®¾å¤‡: {}".format(config.device))
        
        # åŠ è½½æ•°æ®
        self.train_dataset, self.vocab = create_dataset(config, 'train')
        self.val_dataset, _ = create_dataset(config, 'dev')
        
        # æ„å»ºæ¨¡å‹
        logger.info("ğŸ§  æ„å»ºå¢å¼ºç‰ˆè¶…ç®€åŒ–æ¨¡å‹...")
        self.model = EnhancedUltraSimpleModel(config)
        
        # è®¡ç®—å‚æ•°é‡
        total_params = sum(p.size for p in self.model.trainable_params())
        logger.info(f"æ¨¡å‹æ„å»ºå®Œæˆ - å‚æ•°é‡: {total_params}")
        
        # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
        self.loss_fn = nn.CrossEntropyLoss()
        self.optimizer = nn.SGD(
            self.model.trainable_params(),
            learning_rate=config.learning_rate,
            weight_decay=config.weight_decay,
            momentum=0.9  # æ·»åŠ åŠ¨é‡
        )
        
        # è®­ç»ƒçŠ¶æ€
        self.best_val_acc = 0.0
        self.patience_counter = 0
        self.training_history = []
        
        logger.info("ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°åˆ›å»ºå®Œæˆ")
    
    def forward_fn(self, data, label):
        """å‰å‘ä¼ æ’­"""
        logits = self.model(data)
        loss = self.loss_fn(logits, label)
        return loss, logits
    
    def train_step(self, data, label):
        """å•æ­¥è®­ç»ƒ"""
        grad_fn = ms.value_and_grad(self.forward_fn, None, self.optimizer.parameters, has_aux=True)
        (loss, logits), grads = grad_fn(data, label)
        self.optimizer(grads)
        return loss, logits
    
    def evaluate(self, dataset):
        """è¯„ä¼°æ¨¡å‹"""
        self.model.set_train(False)
        
        total_loss = 0.0
        correct_predictions = 0
        total_samples = 0
        class_correct = defaultdict(int)
        class_total = defaultdict(int)
        
        # åå‘è¯æ±‡è¡¨
        id_to_label = {v: k for k, v in self.vocab.items()}
        
        for batch in dataset:
            data, labels = batch
            
            # å‰å‘ä¼ æ’­
            logits = self.model(data)
            loss = self.loss_fn(logits, labels)
            
            # ç»Ÿè®¡
            total_loss += loss.asnumpy()
            
            predictions = ops.Argmax(axis=1)(logits)
            
            # æ‰¹æ¬¡å†…ç»Ÿè®¡
            for i in range(len(labels)):
                pred = int(predictions[i].asnumpy())
                true = int(labels[i].asnumpy())
                
                total_samples += 1
                if pred == true:
                    correct_predictions += 1
                    class_correct[id_to_label[true]] += 1
                class_total[id_to_label[true]] += 1
        
        avg_loss = total_loss / len(dataset) if len(dataset) > 0 else 0
        accuracy = correct_predictions / total_samples if total_samples > 0 else 0
        
        # æ‰“å°å„ç±»åˆ«å‡†ç¡®ç‡
        logger.info("å„ç±»åˆ«å‡†ç¡®ç‡:")
        for label in sorted(class_total.keys()):
            if label not in ['<PAD>', '<UNK>']:
                correct = class_correct.get(label, 0)
                total = class_total[label]
                class_acc = correct / total if total > 0 else 0
                logger.info(f"  {label}: {class_acc:.4f} ({correct}/{total})")
        
        self.model.set_train(True)
        return avg_loss, accuracy
    
    def train(self):
        """è®­ç»ƒä¸»å¾ªç¯"""
        logger.info("ğŸ¯ å¼€å§‹å¢å¼ºç‰ˆè¶…ç®€åŒ–é«˜å‡†ç¡®ç‡è®­ç»ƒ...")
        
        for epoch in range(1, self.config.epochs + 1):
            epoch_start_time = time.time()
            logger.info(f"å¼€å§‹ç¬¬ {epoch}/{self.config.epochs} è½®è®­ç»ƒ...")
            
            # è®­ç»ƒé˜¶æ®µ
            self.model.set_train(True)
            total_loss = 0.0
            correct_predictions = 0
            total_samples = 0
            
            batch_count = 0
            for batch in self.train_dataset:
                data, labels = batch
                
                # è®­ç»ƒæ­¥éª¤
                loss, logits = self.train_step(data, labels)
                
                # ç»Ÿè®¡
                total_loss += loss.asnumpy()
                predictions = ops.Argmax(axis=1)(logits)
                
                batch_correct = 0
                for i in range(len(labels)):
                    if predictions[i].asnumpy() == labels[i].asnumpy():
                        batch_correct += 1
                        correct_predictions += 1
                    total_samples += 1
                
                batch_count += 1
                
                # å®šæœŸè¾“å‡ºè¿›åº¦
                if batch_count % 20 == 0:
                    current_acc = correct_predictions / total_samples if total_samples > 0 else 0
                    logger.info(f"æ ·æœ¬ {batch_count * self.config.batch_size}: Loss = {loss.asnumpy():.4f}, å½“å‰å‡†ç¡®ç‡ = {current_acc:.4f}")
            
            # è®¡ç®—è®­ç»ƒæŒ‡æ ‡
            avg_train_loss = total_loss / batch_count if batch_count > 0 else 0
            train_accuracy = correct_predictions / total_samples if total_samples > 0 else 0
            
            logger.info(f"Epoch {epoch} è®­ç»ƒå®Œæˆ:")
            logger.info(f"  å¹³å‡æŸå¤±: {avg_train_loss:.4f}")
            logger.info(f"  è®­ç»ƒå‡†ç¡®ç‡: {train_accuracy:.4f}")
            
            # éªŒè¯é˜¶æ®µ
            logger.info("å¼€å§‹æ¨¡å‹è¯„ä¼°...")
            val_loss, val_accuracy = self.evaluate(self.val_dataset)
            
            logger.info("è¯„ä¼°å®Œæˆ:")
            logger.info(f"  éªŒè¯æŸå¤±: {val_loss:.4f}")
            logger.info(f"  éªŒè¯å‡†ç¡®ç‡: {val_accuracy:.4f}")
            
            # è®°å½•å†å²
            epoch_time = time.time() - epoch_start_time
            epoch_record = {
                'epoch': epoch,
                'train_loss': float(avg_train_loss),
                'train_accuracy': float(train_accuracy),
                'val_loss': float(val_loss),
                'val_accuracy': float(val_accuracy),
                'epoch_time': epoch_time
            }
            self.training_history.append(epoch_record)
            
            logger.info(f"Epoch {epoch} æ€»ç»“:")
            logger.info(f"  è®­ç»ƒæŸå¤±: {avg_train_loss:.4f}, è®­ç»ƒå‡†ç¡®ç‡: {train_accuracy:.4f}")
            logger.info(f"  éªŒè¯æŸå¤±: {val_loss:.4f}, éªŒè¯å‡†ç¡®ç‡: {val_accuracy:.4f}")
            logger.info(f"  è€—æ—¶: {epoch_time:.2f}ç§’")
            
            # æ—©åœæ£€æŸ¥
            if val_accuracy > self.best_val_acc + self.config.min_improvement:
                self.best_val_acc = val_accuracy
                self.patience_counter = 0
                logger.info(f"ğŸ‰ æ–°çš„æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {self.best_val_acc:.4f}")
                
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                ms.save_checkpoint(self.model, self.config.model_save_path)
                logger.info("æœ€ä½³æ¨¡å‹å·²ä¿å­˜!")
            else:
                self.patience_counter += 1
                logger.info(f"éªŒè¯å‡†ç¡®ç‡æœªæå‡ï¼Œè€å¿ƒè®¡æ•°: {self.patience_counter}/{self.config.patience}")
                
                if self.patience_counter >= self.config.patience:
                    logger.info(f"æ—©åœè§¦å‘ï¼Œåœ¨ç¬¬ {epoch} è½®åœæ­¢è®­ç»ƒ")
                    break
        
        logger.info(f"è®­ç»ƒå®Œæˆ! æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {self.best_val_acc:.4f}")
        
        # ä¿å­˜æœ€ç»ˆç»“æœ
        self.save_final_results()
    
    def save_final_results(self):
        """ä¿å­˜æœ€ç»ˆç»“æœ"""
        print("ğŸ’¾ ä¿å­˜æœ€ç»ˆæ¨¡å‹...")
        
        # ä¿å­˜è¯æ±‡è¡¨
        with open(self.config.vocab_save_path, 'w', encoding='utf-8') as f:
            json.dump(self.vocab, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜è®­ç»ƒå†å²
        with open(self.config.history_save_path, 'w', encoding='utf-8') as f:
            json.dump(self.training_history, f, indent=2)
        
        logger.info("âœ… å¢å¼ºç‰ˆè¶…ç®€åŒ–æ¨¡å‹å’Œè¯æ±‡è¡¨ä¿å­˜å®Œæˆ")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¢å¼ºç‰ˆè¶…ç®€åŒ–ç¨³å®šç‰ˆCE-CSLæ‰‹è¯­è¯†åˆ«è®­ç»ƒå¯åŠ¨")
    print("ğŸ”§ è®¾è®¡ç†å¿µ:")
    print("  âœ“ æœ€å°æœ‰æ•ˆæ¶æ„ - åªä¿ç•™å¿…è¦ç»„ä»¶")
    print("  âœ“ æ¿€è¿›æ•°æ®å¢å¼º - 10å€æ•°æ®æ‰©å……")
    print("  âœ“ æ›´é•¿è®­ç»ƒæ—¶é—´ - 50è½®è®­ç»ƒ")
    print("  âœ“ æ™ºèƒ½æ—©åœç­–ç•¥ - é˜²æ­¢è¿‡æ‹Ÿåˆ")
    print("  âœ“ ç±»åˆ«ç‰¹å®šæ¨¡å¼ - å¢å¼ºæ•°æ®åŒºåˆ†æ€§")
    
    # åˆ›å»ºé…ç½®
    config = EnhancedUltraSimpleConfig()
    
    print("ğŸ“Š è¯¦ç»†é…ç½®:")
    print(f"  - è®­ç»ƒè½®æ•°: {config.epochs}")
    print(f"  - æ‰¹æ¬¡å¤§å°: {config.batch_size}")
    print(f"  - å­¦ä¹ ç‡: {config.learning_rate}")
    print(f"  - æƒé‡è¡°å‡: {config.weight_decay}")
    print(f"  - è®¾å¤‡: {config.device}")
    print(f"  - éšè—ç»´åº¦: {config.hidden_dim}")
    print(f"  - Dropoutç‡: {config.dropout_rate}")
    print(f"  - æ•°æ®ç›®å½•: {config.data_dir}")
    print(f"  - æ•°æ®å¢å¼ºå€æ•°: {config.augmentation_factor}")
    print(f"  - æ—©åœè€å¿ƒå€¼: {config.patience}")
    
    # åˆ›å»ºè®­ç»ƒå™¨å¹¶å¼€å§‹è®­ç»ƒ
    trainer = EnhancedUltraSimpleTrainer(config)
    trainer.train()
    
    print("ğŸ‰ å¢å¼ºç‰ˆè¶…ç®€åŒ–è®­ç»ƒå®Œæˆï¼")
    print(f"ğŸ“ æ¨¡å‹å·²ä¿å­˜åˆ°: {config.model_save_path}")
    print(f"ğŸ“Š è®­ç»ƒå†å²å·²ä¿å­˜åˆ°: {config.history_save_path}")
    print(f"ğŸ† æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {trainer.best_val_acc:.4f}")
    print("âœ¨ ä¸»è¦æ”¹è¿›:")
    print("  âœ“ æ¿€è¿›æ•°æ®å¢å¼ºç­–ç•¥")
    print("  âœ“ æ›´å°æ›´ç¨³å®šçš„æ¨¡å‹")
    print("  âœ“ ç±»åˆ«ç‰¹å®šç‰¹å¾æ¨¡å¼")
    print("  âœ“ æ™ºèƒ½æ—©åœé˜²è¿‡æ‹Ÿåˆ")
    print("  âœ“ åŠ¨é‡ä¼˜åŒ–å™¨")

if __name__ == "__main__":
    main()
